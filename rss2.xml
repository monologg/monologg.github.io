<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Monologg Blog</title>
    <link>https://monologg.kr/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Monologg Blog</description>
    <pubDate>Mon, 07 Aug 2023 16:26:50 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>2주 간의 KoELECTRA 개발기 - 2부</title>
      <link>https://monologg.kr/2020/05/02/koelectra-part2/</link>
      <guid>https://monologg.kr/2020/05/02/koelectra-part2/</guid>
      <pubDate>Fri, 01 May 2020 18:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;strong&gt;2주 간의 KoELECTRA 개발&lt;/strong&gt;을 마치고, 그 과정을 글로 남기려고 한다.&lt;/p&gt;
&lt;p&gt;이 글을 읽으신 분들은 내가 했던 &lt;strong&gt;삽질(?)&lt;/strong&gt;을 최대한 덜 하길 바라는 마음이다:)&lt;/p&gt;
&lt;p&gt;2부에는 &lt;strong&gt;Pretraining, Finetuning&lt;/strong&gt; 등을 다룰 예정이다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Github Repo:&lt;/strong&gt; &lt;a href=&quot;https://github.com/monologg/KoELECTRA&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://github.com/monologg/KoELECTRA&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><strong>2주 간의 KoELECTRA 개발</strong>을 마치고, 그 과정을 글로 남기려고 한다.</p><p>이 글을 읽으신 분들은 내가 했던 <strong>삽질(?)</strong>을 최대한 덜 하길 바라는 마음이다:)</p><p>2부에는 <strong>Pretraining, Finetuning</strong> 등을 다룰 예정이다.</p><blockquote><p><strong>Github Repo:</strong> <a href="https://github.com/monologg/KoELECTRA" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/monologg/KoELECTRA</a></p></blockquote><a id="more"></a><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>😀 드디어 모든 삽질들을 끝내고, Pretraining을 시작하였다 😀</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><div class="justified-gallery"><p><img src="/images/2020-05-02-koelectra-part2/base_loss.png" alt="base loss"><br><img src="/images/2020-05-02-koelectra-part2/small_loss.png" alt="small loss"></p></div><p>약 300k step 까지의 <code>base</code>와 <code>small</code>의 loss 추이이다. 첫 100k까지는 매우 빠르게 줄어들다가, 그 이후에는 조금씩 줄어드는 모습을 보인다.</p><h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><p>학습 중간중간 성능 체크는 <code>nsmc</code> 데이터셋을 가지고 간단하게 평가하였다 (사실 GPU가 1개 밖에 없어 <code>nsmc</code>만 테스트한 건 비밀😢)</p><table><thead><tr><th>Acc(%)</th><th>25K</th><th>75K</th><th>90K</th><th>125K</th><th>150K</th><th>250K</th><th>300K</th><th>450K</th></tr></thead><tbody><tr><td><code>Base</code></td><td>88.10</td><td>88.48</td><td>88.67</td><td>88.92</td><td>88.97</td><td>89.51</td><td>89.65</td><td>90.16</td></tr></tbody></table><p>Step이 증가할수록 accuracy가 오르는 것이 눈에 띄게 보이니 신기하긴 했다. (이것이 Pretraining의 힘인가….)</p><h3 id="Training-Time"><a href="#Training-Time" class="headerlink" title="Training Time"></a>Training Time</h3><p>데이터의 경우 <strong>14GB</strong>로 총 <code>2.6B Token</code>이다. BERT와 ELECTRA에서는 <code>3.3B Token</code>을 사용한 것에 비하면 데이터의 양이 조금 모자란 게 아쉽긴 하지만, 이것이 개인 단위에서 모을 수 있었던 최선의 데이터양이었다😵</p><p><strong>TPU v3-8</strong> 기준으로 <code>Base</code> 모델은 <strong>약 7일</strong>, <code>Small</code> 모델은 <strong>약 3일</strong>이 소요되었다. 그래서 이 기간 동안 <strong>Finetuning</strong> 코드를 짜는 것으로 시간을 절약하였다.</p><h2 id="Finetuning-코드-제작"><a href="#Finetuning-코드-제작" class="headerlink" title="Finetuning 코드 제작"></a>Finetuning 코드 제작</h2><p>코드의 경우는 <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="external nofollow noopener noreferrer" target="_blank">Transformers의 Example 코드</a>를 참고하여 제작하였다.</p><p><strong>Finetuning의 경우 총 7개의 task에 대해 진행하였다.</strong> (때마침 얼마 전 카카오브레인에서 <code>KorNLI</code>와 <code>KorSTS</code> 데이터셋을 공개해주었다👍 1년 전과 비교했을 때 벤치마크를 평가할 수 있는 한국어 데이터셋이 많아진 것은 정말 좋은 일이라 할 수 있다.)</p><table><thead><tr><th></th><th align="center">NSMC</th><th align="center">PAWS</th><th align="center">QuestionPair</th><th align="center">KorNLI</th><th align="center">KorSTS</th><th align="center">NaverNER</th><th align="center">KorQuad</th></tr></thead><tbody><tr><td><strong>Task</strong></td><td align="center">감정분석</td><td align="center">유사문장</td><td align="center">유사문장</td><td align="center">추론</td><td align="center">유사문장</td><td align="center">개체명인식</td><td align="center">기계독해</td></tr><tr><td><strong>Metric</strong></td><td align="center">Acc</td><td align="center">Acc</td><td align="center">Acc</td><td align="center">Acc</td><td align="center">Spearman</td><td align="center">F1</td><td align="center">EM/F1</td></tr></tbody></table><p>기존의 연구들에서는 <code>Bert-Multilingual-Cased</code>를 가지고 많이 비교하였는데, 이번 연구에서는 <code>XLM-Roberta-Base</code> 모델로 평가를 시도하였다. 확실히 xlm-roberta가 bert보다는 성능이 좋았기에, 적어도 <code>KoELECTRA</code>가 xlm-roberta는 뛰어넘어야 유의미하지 않을까라고 생각해서 였다.</p><p><a href="https://deview.kr/2019/schedule/291" rel="external nofollow noopener noreferrer" target="_blank">Deview에서 발표한 Larva</a>의 경우 Benchmark pipeline을 만들어서 ckpt가 들어올 때마다 계속 evaluation을 해주었다고 하는데, 앞에서도 말했듯이 나에게는 GPU가 1개 밖에 없고, GCP에서 GPU를 많이 빌릴 수도 없기에 <strong>가장 최근 5개의 ckpt를 가지고 평가</strong>하였고, 그 중에서 평균값이 가장 좋았던 것을 최종 모델로 선정하였다.</p><p><strong>Finetuning용 코드 및 사용법</strong>은 <a href="https://github.com/monologg/KoELECTRA/tree/master/finetune" rel="external nofollow noopener noreferrer" target="_blank">여기</a>에서 직접 확인해볼 수 있다.</p><h2 id="Convert-from-Tensorflow-to-Pytorch"><a href="#Convert-from-Tensorflow-to-Pytorch" class="headerlink" title="Convert from Tensorflow to Pytorch"></a>Convert from Tensorflow to Pytorch</h2><p>Huggingface에서 <code>ElectraModel</code>을 구현하면서 tensorflow를 pytorch로 변환하는 코드도 함께 만들어줬다😍</p><p>관련 내용은 [<a href="/2020/05/01/transformers-porting/" title="내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기">내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기</a>]를 읽어보길 바란다. (<strong>여기서도 굉장히 삽질을 많이 해본 입장으로서 꼭 읽어보길 권한다</strong>)</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><div class="justified-gallery"><p><img src="/images/2020-05-02-koelectra-part2/base_result.png" alt="Base result"><br><img src="/images/2020-05-02-koelectra-part2/small_result.png" alt="Small result"></p></div><p>처음에 이 프로젝트를 계획할 때 가장 걱정되었던 점이 <strong>“성능이 안 나오면 어쩌나”</strong>였다. (성능이 너무 안 좋으면 사실 2주를 제대로 날린 셈이기에….)</p><p><strong>결과는 예상했던 것보다 훨씬 좋았다.</strong> 애초에 데이터의 양이나 Tokenizer 등을 고려했을 때 <code>HanBERT</code>를 완벽히 따라잡는 것은 무리라고 생각했지만, <code>KoBERT</code>보다 전반적으로 성능이 많이 좋을 줄은 몰랐다. <code>HanBERT</code>와도 실질적인 결과는 비슷하거나 오히려 더 좋은 케이스도 있어서 이 정도면 🎉<strong>대성공</strong>🎉이라고 봐도 될 꺼 같다.</p><p><code>KoELECTRA-Small</code>의 성능이 가장 인상적이었는데, 모델의 사이즈가 <code>DistilKoBERT</code>의 절반임에도 불구하고 우수한 성능을 보였다. <strong>경량화 모델에서 충분히 사용할 만한 가치가 있을 것 같다.</strong></p><h2 id="How-to-Use"><a href="#How-to-Use" class="headerlink" title="How to Use"></a>How to Use</h2><p>이 프로젝트를 처음 계획했을 때의 고려사항 중 아래 사항들이 가장 중요했었다.</p><blockquote><p>“<strong>모든 OS</strong>에서 사용 가능”<br>“<strong>tokenization 파일을 만들 필요 없이</strong> 곧바로 사용 가능”</p></blockquote><p>그리고 이제 <code>transformers</code> 라이브러리만 있으면 어떠한 환경에서도 <code>한국어 PLM</code>을 사용할 수 있게된 것이다🤗</p><figure class="highlight bash"><figcaption><span>Installation</span></figcaption><table><tr><td class="code"><pre><span class="line">$ pip3 install -U transformers</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>How to Use</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ElectraModel, ElectraTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># KoELECTRA-Base</span></span><br><span class="line">model = ElectraModel.from_pretrained(<span class="string">"monologg/koelectra-base-discriminator"</span>)</span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-base-discriminator"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KoELECTRA-Small</span></span><br><span class="line">model = ElectraModel.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br></pre></td></tr></table></figure><h2 id="맺으며"><a href="#맺으며" class="headerlink" title="맺으며"></a>맺으며</h2><center><p><img src="/images/2020-05-02-koelectra-part2/facebook-reaction.png" alt="와우!"></p></center><p>솔직히 이렇게까지 반응이 좋을 줄은 몰랐다🙄 개발자로서 이럴 때가 가장 보람있지 않은가 싶다.</p><p>처음 계획할 때부터 <strong>“이거 만들면 다른 분들에게도 큰 도움이 되겠다”</strong>라 생각했는데, 실제로도 그런 것 같아 기분이 (굉장히) 좋다 😀</p><blockquote><p>“내가 불편해하는 것은 분명 다른 누군가도 불편해한다. 그런 부분을 해결해주는 것이 개발자의 역할이라고 생각한다.”</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA Official Code</a></li><li><a href="https://github.com/huggingface/transformers" rel="external nofollow noopener noreferrer" target="_blank">Huggingface Transformers</a></li><li><a href="https://deview.kr/data/deview/2019/presentation/[111]+%E1%84%8B%E1%85%A5%E1%86%B7_%E1%84%8E%E1%85%A5%E1%86%BC+%E1%84%8F%E1%85%B3%E1%86%AB+%E1%84%8B%E1%85%A5%E1%86%AB%E1%84%8B%E1%85%A5+%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF+%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%A1%E1%86%BC+%E1%84%80%E1%85%A1%E1%84%83%E1%85%A9%E1%86%BC%E1%84%80%E1%85%B5.pdf" rel="external nofollow noopener noreferrer" target="_blank">Deview 2019 Larva Presentation</a></li><li><a href="https://github.com/kakaobrain/KorNLUDatasets" rel="external nofollow noopener noreferrer" target="_blank">KorNLI, KorSTS</a></li></ul><h2 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h2><ul><li><a href="/2020/05/01/transformers-porting/" title="내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기">내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기</a></li><li><a href="/2020/05/02/koelectra-part1/" title="2주 간의 KoELECTRA 개발기 - 1부">2주 간의 KoELECTRA 개발기 - 1부</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/05/02/koelectra-part2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2주 간의 KoELECTRA 개발기 - 1부</title>
      <link>https://monologg.kr/2020/05/02/koelectra-part1/</link>
      <guid>https://monologg.kr/2020/05/02/koelectra-part1/</guid>
      <pubDate>Fri, 01 May 2020 16:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;strong&gt;2주 간의 KoELECTRA 개발&lt;/strong&gt;을 마치고, 그 과정을 글로 남기려고 한다.&lt;/p&gt;
&lt;p&gt;이 글을 읽으신 분들은 내가 했던 &lt;strong&gt;삽질(?)&lt;/strong&gt;을 최대한 덜 하길 바라는 마음이다:)&lt;/p&gt;
&lt;p&gt;1부에는 &lt;strong&gt;실제 학습을 돌리기 전까지의 과정&lt;/strong&gt;을 다룰 예정이다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Github Repo:&lt;/strong&gt; &lt;a href=&quot;https://github.com/monologg/KoELECTRA&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://github.com/monologg/KoELECTRA&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><strong>2주 간의 KoELECTRA 개발</strong>을 마치고, 그 과정을 글로 남기려고 한다.</p><p>이 글을 읽으신 분들은 내가 했던 <strong>삽질(?)</strong>을 최대한 덜 하길 바라는 마음이다:)</p><p>1부에는 <strong>실제 학습을 돌리기 전까지의 과정</strong>을 다룰 예정이다.</p><blockquote><p><strong>Github Repo:</strong> <a href="https://github.com/monologg/KoELECTRA" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/monologg/KoELECTRA</a></p></blockquote><a id="more"></a><h2 id="문제-의식"><a href="#문제-의식" class="headerlink" title="문제 의식"></a>문제 의식</h2><p>한국에 Public하게 공개되어 있는 <code>한국어 PLM(Pretrained Language Model)</code>에는 크게 3가지가 있다.</p><ol><li>SKT의 <code>KoBERT</code></li><li>TwoBlock AI의 <code>HanBERT</code></li><li>ETRI의 <code>KorBERT</code></li></ol><p>3가지 모두 좋은 성능을 보이지만, 3가지 모두 단점이 존재한다. 각각의 단점을 정리하면 아래와 같다.</p><table><thead><tr><th align="left"></th><th align="left">단점</th></tr></thead><tbody><tr><td align="left"><strong>KoBERT</strong></td><td align="left"><strong>Vocab size (8002개)</strong>가 상대적으로 작음</td></tr><tr><td align="left"><strong>HanBERT</strong></td><td align="left">Tokenizer로 인해 <strong>Ubuntu 환경</strong>에서만 사용 가능</td></tr><tr><td align="left"><strong>KorBERT</strong></td><td align="left"><strong>API 신청</strong> 등의 과정 필요</td></tr></tbody></table><p>특히 3가지 모두 공통적으로 <code>Huggingface Transformers</code> 라이브러리에서 사용하려면 <strong>tokenization 파일을 따로 만들어야 하는 단점</strong>이 있다.</p><p><a href="https://github.com/monologg/DistilKoBERT" rel="external nofollow noopener noreferrer" target="_blank">KoBERT</a>와 <a href="https://github.com/monologg/HanBert-Transformers" rel="external nofollow noopener noreferrer" target="_blank">HanBERT</a>의 경우 내가 직접 tokenization 파일을 만들어서 github에 배포했지만, 일부 사용자들이 불편함을 토로하기도 했다.</p><p>그래서 이번 기회에 위의 단점들을 모두 해결한 <code>한국어 PLM</code>을 개발하고 싶었다.</p><blockquote><ol><li><strong>Vocab size</strong>가 어느 정도 커야 함 (30000개 정도)</li><li><strong>모든 OS</strong>에서 사용 가능</li><li><strong>tokenization 파일을 만들 필요 없이</strong> 곧바로 사용 가능</li><li>어느 정도 <strong>성능</strong>까지 보장되어야 함</li></ol></blockquote><p>위의 4가지를 모두 만족시키기 위하여 시작한 프로젝트가 바로 <code>KoELECTRA</code> 이다.</p><h2 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h2><p>실제 현업에서도 좋은 성능을 위해 <code>Mecab + Sentencepiece</code>를 많이 사용하는 것으로 알고 있다. 그러나 공식 BERT, ELECTRA 등은 <code>Wordpiece</code>를 사용하고 있으며, <code>transformers</code>에서도 공식적으로 Wordpiece만 지원하고 있다.</p><p>즉, ELECTRA에서 Mecab이나 Sentencepiece를 사용하려면 추가적으로 <code>tokenization</code> 파일을 만들어야 하며, <code>transformers</code> 라이브러리의 api가 바뀌면 내가 직접 그에 맞게 tokenization 파일을 바꿔줘야 한다는 것이다. (KoBERT의 경우 실제로도 그렇게 하고 있다ㅠ)</p><p><strong>이러한 문제들 때문에 이번 프로젝트에서는 무조건으로 <code>Wordpiece</code>를 사용하는 방안으로 진행하였다.</strong></p><h3 id="Wordpiece"><a href="#Wordpiece" class="headerlink" title="Wordpiece"></a>Wordpiece</h3><blockquote><p>“Wordpiece vocab을 만들 때 Huggingface의 <a href="https://github.com/huggingface/tokenizers" rel="external nofollow noopener noreferrer" target="_blank"><code>Tokenizers</code></a> 라이브러리를 쓰는 것이 가장 좋다.”</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--corpus_file"</span>, type=str)</span><br><span class="line">parser.add_argument(<span class="string">"--vocab_size"</span>, type=int, default=<span class="number">32000</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--limit_alphabet"</span>, type=int, default=<span class="number">6000</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">tokenizer = BertWordPieceTokenizer(</span><br><span class="line">    vocab_file=<span class="literal">None</span>,</span><br><span class="line">    clean_text=<span class="literal">True</span>,</span><br><span class="line">    handle_chinese_chars=<span class="literal">True</span>,</span><br><span class="line">    strip_accents=<span class="literal">False</span>, <span class="comment"># Must be False if cased model</span></span><br><span class="line">    lowercase=<span class="literal">False</span>,</span><br><span class="line">    wordpieces_prefix=<span class="string">"##"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.train(</span><br><span class="line">    files=[args.corpus_file],</span><br><span class="line">    limit_alphabet=args.limit_alphabet,</span><br><span class="line">    vocab_size=args.vocab_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.save(<span class="string">"./"</span>, <span class="string">"bert-wordpiece"</span>)</span><br></pre></td></tr></table></figure><blockquote><ul><li>Vocab size의 경우 관례적으로 많이 쓰이는 <strong>약 3만개</strong>로 세팅하였다.</li><li><strong>전처리 없이</strong> 원본 Corpus만 가지고 vocab 만들면 <strong>성능이 굉장히 안 좋음</strong></li><li>character coverage를 최대한 높게 잡는 것이 좋다고 판단 (<strong>즉, corpus에서 등장했던 모든 character를 vocab에 포함시킴</strong>)<ul><li>예를 들어 <code>퀭메일</code>이란 단어가 있다고 가정하자.</li><li>만일 <code>퀭</code>이 vocab에 없다면 <code>퀭메일</code> 전체를 <code>[UNK]</code>로 처리하게 된다.</li><li>만일 <code>퀭</code>이 vocab 안에 있으면 <code>퀭 + ##메일</code>로 tokenize가 될 수 있어 <code>##메일</code>이란 단어의 의미를 가져갈 수 있다.</li></ul></li></ul></blockquote><p>(자세한 내용은 이전에 포스팅한 <a href="https://monologg.kr/2020/04/27/wordpiece-vocab/">[나만의 BERT Wordpiece Vocab 만들기]</a>을 참고)</p><h2 id="전처리-Preprocessing"><a href="#전처리-Preprocessing" class="headerlink" title="전처리 (Preprocessing)"></a>전처리 (Preprocessing)</h2><blockquote><p>“첫째도 <strong>전처리</strong>! 둘째도 <strong>전처리</strong>! 셋째도 <strong>전처리</strong>!”<br>“PLM의 성능에 가장 큰 영향을 주는 것은 <code>corpus quality</code>이다!”</p></blockquote><p><strong>크롤링한 뉴스의 문장 하나</strong>를 살펴보자</p><blockquote><p>[주요기사] ☞ [포토 스토리] 무허가 도시광산 을 아시나요? ☞ [따뜻한 사진 한 장] 사랑, 하나가 되어 가는 길 &lt;찰나의 기록, 순간의 진실 / KPPA 바로가기&gt; Copyrightsⓒ 한국사진기자협회(<a href="http://www.kppa.or.kr" rel="external nofollow noopener noreferrer" target="_blank">www.kppa.or.kr</a>), powered by castnet. 무단 전재 및 재배포 금지 보내기</p></blockquote><p>문제는 이런 문장이 매우 많은데다가, 이걸 Pretrain에 넣을 시 성능이 나빠질 게 뻔하다….</p><p>이렇게 noise가 많은 Corpus로 vocab을 만들고 pretrain까지 하면 성능이 좋을 리가 없다.</p><p>아래는 내가 적용한 대표적인 전처리 기준이다.</p><blockquote><ul><li>한자, 일부 특수문자 제거</li><li><strong>한국어 문장 분리기</strong> (<a href="https://github.com/likejazz/korean-sentence-splitter" rel="external nofollow noopener noreferrer" target="_blank">kss</a>) 사용</li><li>뉴스 관련 문장은 제거 (<code>무단전재</code>, <code>(서울=뉴스1)</code> 등 포함되면 무조건 제외)</li></ul></blockquote><p>사실 전처리의 기준에 정답은 없다. 가장 중요한 것은 <strong>자신의 Task에 맞게 전처리 기준을 세우는 것이다</strong>. 내가 생각했던 Task 들에는 한자는 중요하지 않다고 판단해서 지운 것이지, 한자가 꼭 필요한 Task의 경우에는 지우면 안 될 것이다.</p><h2 id="TPU-사용-관련-Tip"><a href="#TPU-사용-관련-Tip" class="headerlink" title="TPU 사용 관련 Tip"></a>TPU 사용 관련 Tip</h2><p>(자세한 내용은 이전에 포스팅한 <a href="https://monologg.kr/2020/04/20/tpu-electra/">[TPU를 이용하여 Electra Pretraining하기]</a>을 참고)</p><h3 id="1-Tensorflow-Research-Cloud-TFRC-를-쓰면-TPU가-무료"><a href="#1-Tensorflow-Research-Cloud-TFRC-를-쓰면-TPU가-무료" class="headerlink" title="1. Tensorflow Research Cloud(TFRC)를 쓰면 TPU가 무료"></a>1. Tensorflow Research Cloud(TFRC)를 쓰면 TPU가 무료</h3><p>→ 이미 <a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">공식 코드</a>가 TPU를 완벽히 지원하기에, 직접 ELECTRA를 만들고 싶다면 <strong>GPU보다는 TPU를 쓰는 것을 강력히 권장한다.</strong></p><h3 id="2-VM-Instance는-작은-것-n1-standard-1-을-써도-상관-없다"><a href="#2-VM-Instance는-작은-것-n1-standard-1-을-써도-상관-없다" class="headerlink" title="2. VM Instance는 작은 것(n1-standard-1)을 써도 상관 없다"></a>2. VM Instance는 작은 것(<code>n1-standard-1</code>)을 써도 상관 없다</h3><p>→ ELECTRA를 GCP에서 학습하려면 <code>TPU</code>, <code>Bucket</code>, <code>VM Instance</code> 이렇게 3개가 필요하다. 그런데 <strong>저장소는 Bucket이, 연산은 TPU가 처리</strong>하기 때문에 VM Instance는 가벼운 것을 써도 된다.<br>→ <code>n1-standard-1</code>는 시간당 약 $0.037, <code>n1-standard-4</code>는 시간당 약 $0.14이다. (<strong>비용이 무려 2배 차이!!</strong>)</p><h3 id="3-TPU를-쓰는-경우-모든-input-file은-Cloud-storage-bucket을-통해야만-한다"><a href="#3-TPU를-쓰는-경우-모든-input-file은-Cloud-storage-bucket을-통해야만-한다" class="headerlink" title="3. TPU를 쓰는 경우 모든 input file은 Cloud storage bucket을 통해야만 한다"></a>3. TPU를 쓰는 경우 모든 input file은 Cloud storage bucket을 통해야만 한다</h3><p>→ 이것도 처음에 몰랐다가 고생했던 삽질 중 하나다. <code>tf.estimator.tpu</code> 쪽 코드를 쓰는 경우 로컬 데이터가 아닌 <code>Bucket</code>을 통해야만 한다. (<a href="https://cloud.google.com/tpu/docs/troubleshooting?hl=ko#common-errors" rel="external nofollow noopener noreferrer" target="_blank">관련 FAQ</a>)<br>→ 다행히도 Bucket의 비용이 비싸지가 않다 (특정 리전에 만들어 놓으면 1GB당 월 약 $0.03)</p><h3 id="4-VM-Instance와-TPU를-만들-때-ctpu-up-명령어를-사용해라"><a href="#4-VM-Instance와-TPU를-만들-때-ctpu-up-명령어를-사용해라" class="headerlink" title="4. VM Instance와 TPU를 만들 때 ctpu up 명령어를 사용해라"></a>4. VM Instance와 TPU를 만들 때 <code>ctpu up</code> 명령어를 사용해라</h3><p>→ VM Instance와 TPU를 따로 따로 생성하면 처음에 정상적으로 작동하지 않는 경우가 있었다. 아래와 같이 <code>cloud shell</code>로 명령어를 한 번만 치면 VM과 TPU가 동시에 생성된다😃</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ctpu up --zone=europe-west4<span class="_">-a</span> --tf-version=1.15 \</span><br><span class="line">          --tpu-size=v3-8 --machine-type=n1-standard-2 \</span><br><span class="line">          --disk-size-gb=20 --name=&#123;<span class="variable">$VM_NAME</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Configuration의-함정"><a href="#Configuration의-함정" class="headerlink" title="Configuration의 함정"></a>Configuration의 함정</h2><p>앞에서도 언급했듯이 ELECTRA Pretraining은 <a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">공식 코드</a>를 그대로 가져다 쓰는 것이 좋다. 공식 코드를 가져다쓰기 전에 <code>논문</code>과 <code>코드 분석</code>을 어느 정도 하고 진행하는 것을 권장한다.</p><p>그럼에도 좀 헷갈렸던 부분이 있어 여기서 언급하려 한다.</p><h3 id="1-공식-레포에서-제공하는-small-모델은-정확히는-small-모델이다"><a href="#1-공식-레포에서-제공하는-small-모델은-정확히는-small-모델이다" class="headerlink" title="1. 공식 레포에서 제공하는 small 모델은 정확히는 small++ 모델이다"></a>1. 공식 레포에서 제공하는 <code>small</code> 모델은 정확히는 <code>small++</code> 모델이다</h3><p><img src="/images/2020-05-02-koelectra-part1/small_notice.png" alt><br><img src="/images/2020-05-02-koelectra-part1/small_gen_size.png" alt></p><p>공식 코드에서도 알 수 있듯이 <code>small</code>로 공개된 모델은 정확히 말하면 <code>small++</code> 모델이다. 둘의 차이점은 아래와 같다.</p><table><thead><tr><th align="left"></th><th>max_seq_len</th><th>generator_hidden_size</th></tr></thead><tbody><tr><td align="left"><strong>small</strong></td><td>128</td><td>0.25</td></tr><tr><td align="left"><strong>small++</strong></td><td>512</td><td>1.0</td></tr></tbody></table><p>(<code>generator_hidden_size=1.0</code>이란 것은 <code>discriminator</code>와 <code>generator</code>의 hidden_size가 같다는 것이다)</p><p>이러한 부분이 논문에 자세히 나와 있지 않아 처음에 small 모델을 만들 때 진짜 small로 만들었다가 다시 small++로 만드는 수고를 거쳤다…. (이 글을 읽은 분들은 이 삽질을 안 하길 빈다.)</p><figure class="highlight json"><figcaption><span>hparams.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"tpu_name"</span>: <span class="string">"electra-small"</span>,</span><br><span class="line">  <span class="attr">"tpu_zone"</span>: <span class="string">"europe-west4-a"</span>,</span><br><span class="line">  <span class="attr">"num_train_steps"</span>: <span class="number">1000000</span>,</span><br><span class="line">  <span class="attr">"save_checkpoints_steps"</span>: <span class="number">50000</span>,</span><br><span class="line">  <span class="attr">"train_batch_size"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"learning_rate"</span>: <span class="number">5e-4</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">32200</span>,</span><br><span class="line">  <span class="attr">"max_seq_length"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"generator_hidden_size"</span>: <span class="number">1.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-max-seq-length를-128로-줄인다면-max-position-embeddings도-128로-줄여야-한다"><a href="#2-max-seq-length를-128로-줄인다면-max-position-embeddings도-128로-줄여야-한다" class="headerlink" title="2. max_seq_length를 128로 줄인다면 max_position_embeddings도 128로 줄여야 한다"></a>2. max_seq_length를 128로 줄인다면 max_position_embeddings도 128로 줄여야 한다</h3><p>간혹 커스터마이즈된 모델을 만들 때 <code>max_seq_length</code>를 줄이고자 하는 경우가 있다.</p><p>그럴 시 <code>max_seq_length</code>만 줄이면 해결된다고 오해할 수 있는데, <code>max_position_embeddings</code>도 줄여줘야 transformers 라이브러리에 맞게 변환할 때 문제가 생기지 않는다. (transformers가 max_position_embeddings으로 최대 길이를 알아내기 때문!)</p><p>더 큰 함정은 아래와 같이 <code>model_hparam_overrides</code>라는 attribute 안에 <code>max_position_embeddings</code>를 넣어줘야 하는 것이다!</p><figure class="highlight json"><figcaption><span>hparams.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"max_seq_length"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"model_hparam_overrides"</span>: &#123;</span><br><span class="line">    <span class="attr">"max_position_embeddings"</span>: <span class="number">128</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="마치며"><a href="#마치며" class="headerlink" title="마치며"></a>마치며</h2><p>1부에서는 <strong>실제 Pretraining을 시작하기 전의 준비 과정</strong>을 다뤘다.</p><a href="/2020/05/02/koelectra-part2/" title="2주 간의 KoELECTRA 개발기 - 2부">2주 간의 KoELECTRA 개발기 - 2부</a> 에서는 실제 Pretraining, Transformers 포팅, Finetuning 등을 다룰 예정이다:)<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA Official Code</a></li><li><a href="https://github.com/huggingface/tokenizers" rel="external nofollow noopener noreferrer" target="_blank">Huggingface Tokenizers</a></li><li><a href="https://cloud.google.com/tpu/docs?hl=ko" rel="external nofollow noopener noreferrer" target="_blank">Cloud TPU Documentation</a></li></ul><h2 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h2><ul><li><a href="/2020/04/20/tpu-electra/" title="TPU를 이용하여 Electra Pretraining하기">TPU를 이용하여 Electra Pretraining하기</a></li><li><a href="/2020/04/27/wordpiece-vocab/" title="나만의 BERT Wordpiece Vocab 만들기">나만의 BERT Wordpiece Vocab 만들기</a></li><li><a href="/2020/05/02/koelectra-part2/" title="2주 간의 KoELECTRA 개발기 - 2부">2주 간의 KoELECTRA 개발기 - 2부</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/05/02/koelectra-part1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기</title>
      <link>https://monologg.kr/2020/05/01/transformers-porting/</link>
      <guid>https://monologg.kr/2020/05/01/transformers-porting/</guid>
      <pubDate>Thu, 30 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;BERT&lt;/code&gt;, &lt;code&gt;ALBERT&lt;/code&gt;, &lt;code&gt;ELECTRA&lt;/code&gt; 등을 직접 Pretrain하게 되면 모델이 Tensorflow의 ckpt 형태로 저장이 된다.&lt;/p&gt;
&lt;p&gt;이번 글에서는 &lt;code&gt;tensorflow ckpt&lt;/code&gt;를 transformers의 &lt;code&gt;pytorch ckpt&lt;/code&gt;로 변환하는 법을 알아보겠다🤗&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>BERT</code>, <code>ALBERT</code>, <code>ELECTRA</code> 등을 직접 Pretrain하게 되면 모델이 Tensorflow의 ckpt 형태로 저장이 된다.</p><p>이번 글에서는 <code>tensorflow ckpt</code>를 transformers의 <code>pytorch ckpt</code>로 변환하는 법을 알아보겠다🤗</p><a id="more"></a><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><ul><li>이번 글에서는 <code>ELECTRA-Small</code>을 기준으로 실습을 해본다. (<code>BERT</code> 등도 방법은 크게 다르지 않다)</li><li><code>transformers v2.8.0</code>을 기준으로 작성하였다. 이후 버전에서 호환되지 않는 경우가 있을 수 있다.</li><li><code>Transformers</code>의 <code>ELECTRA</code>는 <code>discriminator</code>와 <code>generator</code>를 <strong>각각 따로 만들어줘야 한다!</strong></li></ul><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><h3 id="1-Original-Tensorflow-Checkpoint"><a href="#1-Original-Tensorflow-Checkpoint" class="headerlink" title="1. Original Tensorflow Checkpoint"></a>1. Original Tensorflow Checkpoint</h3><p>당연히 <strong>Tensorflow로 학습한 결과물</strong>을 가지고 있어야 한다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── koelectra-small-tf</span><br><span class="line">│   ├── checkpoint</span><br><span class="line">│   ├── events.out.tfevents.1586942968.koelectra-small</span><br><span class="line">│   ├── graph.pbtxt</span><br><span class="line">│   ├── ...</span><br><span class="line">│   ├── model.ckpt-700000.data-00000-of-00001</span><br><span class="line">│   ├── model.ckpt-700000.index</span><br><span class="line">│   └── model.ckpt-700000.meta</span><br><span class="line">└── ...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>checkpoint</span></figcaption><table><tr><td class="code"><pre><span class="line">model_checkpoint_path: &quot;model.ckpt-700000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-600000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-625000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-650000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-675000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-700000&quot;</span><br></pre></td></tr></table></figure><p>주의할 점은 <code>checkpoint</code> 파일에서 <code>model_checkpoint_path</code> 값을 <strong>“원하는 step의 ckpt”</strong>로 바꿔줘야 한다는 것이다.</p><h3 id="2-config-json"><a href="#2-config-json" class="headerlink" title="2. config.json"></a>2. config.json</h3><blockquote><p><strong>(주의!) <code>transformers</code> 라이브러리가 업데이트되면서 API가 변경되는 경우가 있고, 이에 따라 <code>config.json</code>의 attribute가 추가/변경되는 경우가 있다.</strong></p></blockquote><blockquote><p><a href="https://huggingface.co/models" rel="external nofollow noopener noreferrer" target="_blank">https://huggingface.co/models</a>로 가서 대표 모델의 <code>config.json</code>을 보면서 직접 만들어야 한다.</p></blockquote><ul><li><code>vocab_size</code> 변경에만 주의하면 충분함</li><li>만일 <code>max_seq_length</code>를 바꿨다면 <code>max_position_embeddings</code>도 바꿔야 함</li></ul><figure class="highlight json"><figcaption><span>discriminator</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"architectures"</span>: [<span class="string">"ElectraForPreTraining"</span>],</span><br><span class="line">  <span class="attr">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"embedding_size"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="attr">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"hidden_size"</span>: <span class="number">256</span>,</span><br><span class="line">  <span class="attr">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="attr">"intermediate_size"</span>: <span class="number">1024</span>,</span><br><span class="line">  <span class="attr">"layer_norm_eps"</span>: <span class="number">1e-12</span>,</span><br><span class="line">  <span class="attr">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"model_type"</span>: <span class="string">"electra"</span>,</span><br><span class="line">  <span class="attr">"num_attention_heads"</span>: <span class="number">4</span>,</span><br><span class="line">  <span class="attr">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="attr">"pad_token_id"</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">32200</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><figcaption><span>generator</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"architectures"</span>: [<span class="string">"ElectraForMaskedLM"</span>],</span><br><span class="line">  <span class="attr">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"embedding_size"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="attr">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"hidden_size"</span>: <span class="number">256</span>,</span><br><span class="line">  <span class="attr">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="attr">"intermediate_size"</span>: <span class="number">1024</span>,</span><br><span class="line">  <span class="attr">"layer_norm_eps"</span>: <span class="number">1e-12</span>,</span><br><span class="line">  <span class="attr">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"model_type"</span>: <span class="string">"electra"</span>,</span><br><span class="line">  <span class="attr">"num_attention_heads"</span>: <span class="number">4</span>,</span><br><span class="line">  <span class="attr">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="attr">"pad_token_id"</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">32200</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-tokenizer-config-json"><a href="#3-tokenizer-config-json" class="headerlink" title="3. tokenizer_config.json"></a>3. tokenizer_config.json</h3><p><code>cased</code> 모델의 경우 그냥 tokenizer를 load하면 매번 <code>do_lower_case=False</code>를 직접 추가해줘야 한다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ElectraTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>,</span><br><span class="line">                                             do_lower_case=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p><code>tokenizer_config.json</code>을 만들어주면 이러한 번거로움을 없앨 수 있다.<br>(만일 <code>max_seq_length</code>가 128이면 <code>model_max_length</code>도 128로 바꿔주면 된다.)</p><figure class="highlight json"><figcaption><span>tokenizer_config.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"do_lower_case"</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">"model_max_length"</span>: <span class="number">512</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-vocab-txt"><a href="#4-vocab-txt" class="headerlink" title="4. vocab.txt"></a>4. vocab.txt</h3><p>tensorflow에서 학습했을 때 쓴 <code>vocab.txt</code>를 그대로 쓰면 된다.</p><h3 id="5-최종적인-디렉토리-형태"><a href="#5-최종적인-디렉토리-형태" class="headerlink" title="5. 최종적인 디렉토리 형태"></a>5. 최종적인 디렉토리 형태</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── koelectra-small-tf</span><br><span class="line">│   ├── checkpoint</span><br><span class="line">│   ├── events.out.tfevents.1586942968.koelectra-small</span><br><span class="line">│   ├── graph.pbtxt</span><br><span class="line">│   ├── ...</span><br><span class="line">│   ├── model.ckpt-700000.data-00000-of-00001</span><br><span class="line">│   ├── model.ckpt-700000.index</span><br><span class="line">│   └── model.ckpt-700000.meta</span><br><span class="line">│</span><br><span class="line">├── electra-small-discriminator</span><br><span class="line">│   ├── config.json</span><br><span class="line">│   ├── tokenizer_config.json</span><br><span class="line">│   └── vocab.txt</span><br><span class="line">│</span><br><span class="line">├── electra-small-generator</span><br><span class="line">│   ├── config.json</span><br><span class="line">│   ├── tokenizer_config.json</span><br><span class="line">│   └── vocab.txt</span><br><span class="line">└── ...</span><br></pre></td></tr></table></figure><ul><li><code>electra-small-discriminator</code>와 <code>electra-small-generator</code> 폴더를 각각 만든다.</li><li><code>config.json</code>은 discriminator용과 generator용을 <strong>따로</strong> 만들어서 폴더 안에 넣는다.</li><li><code>tokenizer_config.json</code>과 <code>vocab.txt</code>는 discriminator와 generator 둘 다 <strong>동일한 파일</strong>을 넣으면 된다.</li></ul><h2 id="Convert"><a href="#Convert" class="headerlink" title="Convert"></a>Convert</h2><figure class="highlight python"><figcaption><span>convert.py</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> transformers.convert_electra_original_tf_checkpoint_to_pytorch <span class="keyword">import</span> convert_tf_checkpoint_to_pytorch</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--tf_ckpt_path"</span>, type=str, default=<span class="string">"koelectra-small-tf"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--pt_discriminator_path"</span>, type=str, default=<span class="string">"koelectra-small-discriminator"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--pt_generator_path"</span>, type=str, default=<span class="string">"koelectra-small-generator"</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path,</span><br><span class="line">                                 config_file=os.path.join(args.pt_discriminator_path, <span class="string">"config.json"</span>),</span><br><span class="line">                                 pytorch_dump_path=os.path.join(args.pt_discriminator_path, <span class="string">"pytorch_model.bin"</span>),</span><br><span class="line">                                 discriminator_or_generator=<span class="string">"discriminator"</span>)</span><br><span class="line"></span><br><span class="line">convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path,</span><br><span class="line">                                 config_file=os.path.join(args.pt_generator_path, <span class="string">"config.json"</span>),</span><br><span class="line">                                 pytorch_dump_path=os.path.join(args.pt_generator_path, <span class="string">"pytorch_model.bin"</span>),</span><br><span class="line">                                 discriminator_or_generator=<span class="string">"generator"</span>)</span><br></pre></td></tr></table></figure><h2 id="Upload-your-model-to-Huggingface-s3"><a href="#Upload-your-model-to-Huggingface-s3" class="headerlink" title="Upload your model to Huggingface s3"></a>Upload your model to Huggingface s3</h2><ol><li>먼저 <a href="https://huggingface.co/" rel="external nofollow noopener noreferrer" target="_blank">huggingface.co</a>로 가서 <strong>회원가입</strong>을 해야 함</li><li>아래의 명령어로 s3에 업로드 진행</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ transformers-cli login</span><br><span class="line">$ transformers-cli upload koelectra-small-discriminator</span><br><span class="line">$ transformers-cli upload koelectra-small-generator</span><br></pre></td></tr></table></figure><h2 id="Now-Let’s-Use-It"><a href="#Now-Let’s-Use-It" class="headerlink" title="Now Let’s Use It"></a>Now Let’s Use It</h2><figure class="highlight python"><figcaption><span>How to Use</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ElectraModel, ElectraTokenizer</span><br><span class="line"></span><br><span class="line">model = ElectraModel.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br></pre></td></tr></table></figure><h2 id="맺으며"><a href="#맺으며" class="headerlink" title="맺으며"></a>맺으며</h2><center><p><img src="/images/2020-05-01-transformers-porting/model_download.png" alt="생각보다 많이 사용하시네..."></p></center><p>사실 Model Porting과 관련하여 <strong>명확한 Documentation이 없어</strong> 나도 삽질을 상당히 했던 부분이다. 이번 내용이 다른 분들에게 도움이 되었으면 한다😛</p><p>또한 <strong>Huggingface s3에 모델을 업로드</strong>하는 것은 꼭 사용해보길 권한다. 이 기능이 생긴지 얼마되지 않아서 모르시는 분들이 있는데, <strong>업로드 용량의 제한도 없고</strong> 여러모로 라이브러리 사용도 편해진다. (모델을 100개 이상 올린다고 Huggingface 팀에서 뭐라고 하지 않으니 많이들 쓰셨으면ㅎㅎ)</p>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/05/01/transformers-porting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>나만의 BERT Wordpiece Vocab 만들기</title>
      <link>https://monologg.kr/2020/04/27/wordpiece-vocab/</link>
      <guid>https://monologg.kr/2020/04/27/wordpiece-vocab/</guid>
      <pubDate>Sun, 26 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;개인적으로 Pretrained Language Model 성능에 큰 영향을 주는 것 중 하나로 &lt;code&gt;Vocab quality&lt;/code&gt;라고 생각한다.&lt;/p&gt;
&lt;p&gt;이번 포스트에서는 tokenization의 방법 중 하나인 &lt;code&gt;Wordpiece&lt;/code&gt;를 이용하여 어떻게 vocab을 만드는지 알아보려 한다:)&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>개인적으로 Pretrained Language Model 성능에 큰 영향을 주는 것 중 하나로 <code>Vocab quality</code>라고 생각한다.</p><p>이번 포스트에서는 tokenization의 방법 중 하나인 <code>Wordpiece</code>를 이용하여 어떻게 vocab을 만드는지 알아보려 한다:)</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>한국어 Tokenizer의 대안으로는 크게 <code>Sentencepiece</code>, <code>Mecab</code>, <code>Wordpiece</code>가 있다. (<em>여기서의 wordpiece는 Google의 BERT에서 사용된 wordpiece로 가정한다.</em>)</p><p>BERT, ELECTRA 등은 기본적으로 <code>Wordpiece</code>를 사용하기에 공식 코드에서 기본적으로 제공되는 Tokenizer 역시 이에 호환되게 코드가 작성되었다. 즉, <code>Sentencepiece</code>나 <code>Mecab</code>을 사용하려면 <strong>별도의 Tokenizer</strong>를 직접 만들어야 하고, 이렇게 되면 <code>transformers</code> 등의 라이브러리에서 모델을 곧바로 사용하는데 불편함이 생기게 된다.</p><h2 id="Original-wordpiece-code-is-NOT-available"><a href="#Original-wordpiece-code-is-NOT-available" class="headerlink" title="Original wordpiece code is NOT available!"></a>Original wordpiece code is NOT available!</h2><p float="left" align="left">    <img width="800" src="https://user-images.githubusercontent.com/28896432/80015023-19f7e680-850c-11ea-90d3-436ca253a7a1.png">  </p><p><strong>공식 BERT에서 사용된 Wordpiece Builder는 제공되지 않고 있다</strong>. BERT 공식 Github에서 다른 대안들을 제시해줬지만, 완전히 동일한 Wordpiece Vocab이 나오지 않았다.</p><p>몇몇 오픈소스들이 Wordpiece vocab builder를 구현하였지만 <strong>input file이 매우 클 시 메모리, 속도 등의 이슈</strong>가 종종 발생한다ㅠ</p><h2 id="Huggingface-Tokenizers"><a href="#Huggingface-Tokenizers" class="headerlink" title="Huggingface Tokenizers"></a>Huggingface Tokenizers</h2><p float="left" align="center">    <img width="600" src="https://user-images.githubusercontent.com/28896432/80016455-1c5b4000-850e-11ea-8432-3c356c11f932.png">  </p><p>최종적으로, 최근 Huggingface에서 발표한 <code>Tokenizers</code> 라이브러리를 이용하여 Wordpiece Vocabulary를 만드는게 제일 좋았다.</p><p>해당 라이브러리를 사용하면 Corpus가 매우 커도 메모리 이슈가 발생하지 않으며, <code>Rust</code>로 구현이 되어있어 속도 또한 Python보다 빠르다😃</p><h2 id="Code-for-building-Wordpiece-vocab"><a href="#Code-for-building-Wordpiece-vocab" class="headerlink" title="Code for building Wordpiece vocab"></a>Code for building Wordpiece vocab</h2><p><strong>(tokenizer v0.7.0 기준으로 작성하였다. 현재도 라이브러리가 업데이트 중이어서 api가 달라질 수도…)</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--corpus_file"</span>, type=str)</span><br><span class="line">parser.add_argument(<span class="string">"--vocab_size"</span>, type=int, default=<span class="number">32000</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--limit_alphabet"</span>, type=int, default=<span class="number">6000</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">tokenizer = BertWordPieceTokenizer(</span><br><span class="line">    vocab_file=<span class="literal">None</span>,</span><br><span class="line">    clean_text=<span class="literal">True</span>,</span><br><span class="line">    handle_chinese_chars=<span class="literal">True</span>,</span><br><span class="line">    strip_accents=<span class="literal">False</span>, <span class="comment"># Must be False if cased model</span></span><br><span class="line">    lowercase=<span class="literal">False</span>,</span><br><span class="line">    wordpieces_prefix=<span class="string">"##"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.train(</span><br><span class="line">    files=[args.corpus_file],</span><br><span class="line">    limit_alphabet=args.limit_alphabet,</span><br><span class="line">    vocab_size=args.vocab_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.save(<span class="string">"./"</span>, <span class="string">"ch-&#123;&#125;-wpm-&#123;&#125;"</span>.format(args.limit_alphabet, args.vocab_size))</span><br></pre></td></tr></table></figure><ul><li><p>주의해야할 점은 <code>lowercase=False</code>로 할 시 <code>strip_accent=False</code>로 해줘야 한다는 것!</p></li><li><p><code>[UNK]</code>의 비중을 최대한 줄이기 위해 <strong>모든 character를 커버</strong>할 수 있도록 처리하였다. (<code>limit_alphabet</code>)</p></li><li><p>Corpus의 전처리가 완료되었다는 전제하에 sentencepiece와 비교했을 때 <strong>UNK Ratio가 훨씬 낮았다.</strong></p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://wikidocs.net/22592" rel="external nofollow noopener noreferrer" target="_blank">Sentencepiece vs Wordpiece</a></li><li><a href="https://github.com/google-research/bert#learning-a-new-wordpiece-vocabulary" rel="external nofollow noopener noreferrer" target="_blank">Learning a new WordPiece vocabulary</a></li><li><a href="https://github.com/kwonmha/bert-vocab-builder" rel="external nofollow noopener noreferrer" target="_blank">kwonmha’s bert-vocab-builder</a></li><li><a href="https://github.com/huggingface/tokenizers" rel="external nofollow noopener noreferrer" target="_blank">Huggingface Tokenizers</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/04/27/wordpiece-vocab/#disqus_thread</comments>
    </item>
    
    <item>
      <title>TPU를 이용하여 Electra Pretraining하기</title>
      <link>https://monologg.kr/2020/04/20/tpu-electra/</link>
      <guid>https://monologg.kr/2020/04/20/tpu-electra/</guid>
      <pubDate>Sun, 19 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;최근 &lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;ELECTRA&lt;/a&gt;의 공식 코드가 공개되면서 한국어 Corpus에 직접 Electra를 만들게 되었다.&lt;/p&gt;
&lt;p&gt;이번 글에서는 GCP에서 TPU를 어떻게 사용했는지 그 과정을 공유해보려 한다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>최근 <a href="https://openreview.net/forum?id=r1xMH1BtvB" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA</a>의 공식 코드가 공개되면서 한국어 Corpus에 직접 Electra를 만들게 되었다.</p><p>이번 글에서는 GCP에서 TPU를 어떻게 사용했는지 그 과정을 공유해보려 한다.</p><a id="more"></a><h2 id="Tensorflow-Research-Cloud-신청"><a href="#Tensorflow-Research-Cloud-신청" class="headerlink" title="Tensorflow Research Cloud 신청"></a>Tensorflow Research Cloud 신청</h2><p>Tensorflow Research Cloud (TFRC)는 <strong>1달 동안 TPU를 무료</strong>로 사용할 수 있게 해주는 프로그램이다.</p><p>해당 <a href="https://www.tensorflow.org/tfrc?hl=ko" rel="external nofollow noopener noreferrer" target="_blank">링크</a>로 가서 신청을 하게 되면 메일이 하나 오게 된다.</p><p><img src="https://user-images.githubusercontent.com/28896432/79709907-61a92300-82fe-11ea-9773-9ac63b5ebbb6.png" alt></p><p>해당 메일에서 요구하는 대로 신청서를 추가로 작성한 후 제출하면 얼마 후 아래와 같이 답장이 오게 되고, 그 때부터 GCP에서 TPU를 무료로 사용할 수 있게 된다:)</p><p><img src="https://user-images.githubusercontent.com/28896432/79709997-9ddc8380-82fe-11ea-9040-06d8ef9c1f1b.png" alt></p><h2 id="Bucket에-Data-업로드"><a href="#Bucket에-Data-업로드" class="headerlink" title="Bucket에 Data 업로드"></a>Bucket에 Data 업로드</h2><p>TPU를 쓰는 경우 모든 input file을 <strong>Cloud storage bucket을 통해야만 한다.</strong> (<a href="https://cloud.google.com/tpu/docs/troubleshooting?hl=ko#common-errors" rel="external nofollow noopener noreferrer" target="_blank">관련 FAQ</a>)</p><h3 id="Bucket-생성"><a href="#Bucket-생성" class="headerlink" title="Bucket 생성"></a>Bucket 생성</h3><ul><li><p>예제상 Bucket의 이름을 <code>test-for-electra</code>로 만들어 보겠다.</p></li><li><p>GCP 메인 페이지 좌측의 <code>[Storage]</code> - <code>[브라우저]</code> 로 이동</p></li><li><p><code>버킷 만들기</code> 클릭</p></li><li><p><strong>사용할 TPU와 동일한 Region</strong>에 Bucket 만드는 것을 권장</p><p><img src="https://user-images.githubusercontent.com/28896432/79711012-a84c4c80-8301-11ea-955c-39dc604f5c10.png" alt></p></li></ul><h3 id="File-Upload"><a href="#File-Upload" class="headerlink" title="File Upload"></a>File Upload</h3><ul><li><p>준비한 <code>pretrain_tfrecords</code>와 <code>vocab.txt</code>를 Bucket에 업로드</p><p><img src="https://user-images.githubusercontent.com/28896432/79739355-0a747400-8339-11ea-8de2-f78f8ade887f.png" alt></p></li></ul><h2 id="GCP-VM-amp-TPU-생성"><a href="#GCP-VM-amp-TPU-생성" class="headerlink" title="GCP VM &amp; TPU 생성"></a>GCP VM &amp; TPU 생성</h2><ul><li>VM과 TPU를 각각 따로 만드는 것보다, 우측 상단의 <code>cloud shell</code>을 열어 아래의 명령어를 입력하는 것을 추천한다.</li><li>저장소는 Bucket이, 연산은 TPU에서 처리하기 때문에 <strong>VM Instance는 가벼운 것을 써도 상관이 없다.</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ctpu up --zone=europe-west4<span class="_">-a</span> --tf-version=1.15 \</span><br><span class="line">          --tpu-size=v3-8 --machine-type=n1-standard-1 \</span><br><span class="line">          --disk-size-gb=20 --name=&#123;<span class="variable">$VM_NAME</span>&#125;</span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/28896432/79740137-24fb1d00-833a-11ea-9be8-e317521fa178.png" alt></p><h2 id="Electra-학습-진행"><a href="#Electra-학습-진행" class="headerlink" title="Electra 학습 진행"></a>Electra 학습 진행</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/google-research/electra</span><br><span class="line">$ <span class="built_in">cd</span> electra</span><br><span class="line">$ python3 run_pretraining.py --data-dir gs://&#123;<span class="variable">$BUCKET_NAME</span>&#125; \</span><br><span class="line">                             --model-name &#123;<span class="variable">$MODEL_NAME</span>&#125; \</span><br><span class="line">                             --hparams &#123;<span class="variable">$CONFIG_PATH</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="학습-완료-후-Instance-Bucket-삭제"><a href="#학습-완료-후-Instance-Bucket-삭제" class="headerlink" title="학습 완료 후 Instance, Bucket 삭제"></a>학습 완료 후 Instance, Bucket 삭제</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ctpu delete --zone=europe-west4<span class="_">-a</span> --name=&#123;<span class="variable">$VM_NAME</span>&#125;</span><br><span class="line">$ gsutil rm -r gs://<span class="built_in">test</span>-for-electra</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA official github</a></li><li><a href="https://github.com/pren1/A_Pipeline_Of_Pretraining_Bert_On_Google_TPU" rel="external nofollow noopener noreferrer" target="_blank">A Pipeline Of Pretraining Bert On Google TPU</a></li><li><a href="https://cloud.google.com/tpu/docs" rel="external nofollow noopener noreferrer" target="_blank">Official TPU Documentation</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/04/20/tpu-electra/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
