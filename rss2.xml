<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Monologg Blog</title>
    <link>https://monologg.kr/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Monologg Blog</description>
    <pubDate>Mon, 07 Aug 2023 16:26:50 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€</title>
      <link>https://monologg.kr/2020/05/02/koelectra-part2/</link>
      <guid>https://monologg.kr/2020/05/02/koelectra-part2/</guid>
      <pubDate>Fri, 01 May 2020 18:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;strong&gt;2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œ&lt;/strong&gt;ì„ ë§ˆì¹˜ê³ , ê·¸ ê³¼ì •ì„ ê¸€ë¡œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤.&lt;/p&gt;
&lt;p&gt;ì´ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ë“¤ì€ ë‚´ê°€ í–ˆë˜ &lt;strong&gt;ì‚½ì§ˆ(?)&lt;/strong&gt;ì„ ìµœëŒ€í•œ ëœ í•˜ê¸¸ ë°”ë¼ëŠ” ë§ˆìŒì´ë‹¤:)&lt;/p&gt;
&lt;p&gt;2ë¶€ì—ëŠ” &lt;strong&gt;Pretraining, Finetuning&lt;/strong&gt; ë“±ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Github Repo:&lt;/strong&gt; &lt;a href=&quot;https://github.com/monologg/KoELECTRA&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://github.com/monologg/KoELECTRA&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><strong>2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œ</strong>ì„ ë§ˆì¹˜ê³ , ê·¸ ê³¼ì •ì„ ê¸€ë¡œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤.</p><p>ì´ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ë“¤ì€ ë‚´ê°€ í–ˆë˜ <strong>ì‚½ì§ˆ(?)</strong>ì„ ìµœëŒ€í•œ ëœ í•˜ê¸¸ ë°”ë¼ëŠ” ë§ˆìŒì´ë‹¤:)</p><p>2ë¶€ì—ëŠ” <strong>Pretraining, Finetuning</strong> ë“±ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.</p><blockquote><p><strong>Github Repo:</strong> <a href="https://github.com/monologg/KoELECTRA" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/monologg/KoELECTRA</a></p></blockquote><a id="more"></a><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>ğŸ˜€ ë“œë””ì–´ ëª¨ë“  ì‚½ì§ˆë“¤ì„ ëë‚´ê³ , Pretrainingì„ ì‹œì‘í•˜ì˜€ë‹¤ ğŸ˜€</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><div class="justified-gallery"><p><img src="/images/2020-05-02-koelectra-part2/base_loss.png" alt="base loss"><br><img src="/images/2020-05-02-koelectra-part2/small_loss.png" alt="small loss"></p></div><p>ì•½ 300k step ê¹Œì§€ì˜ <code>base</code>ì™€ <code>small</code>ì˜ loss ì¶”ì´ì´ë‹¤. ì²« 100kê¹Œì§€ëŠ” ë§¤ìš° ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë‹¤ê°€, ê·¸ ì´í›„ì—ëŠ” ì¡°ê¸ˆì”© ì¤„ì–´ë“œëŠ” ëª¨ìŠµì„ ë³´ì¸ë‹¤.</p><h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><p>í•™ìŠµ ì¤‘ê°„ì¤‘ê°„ ì„±ëŠ¥ ì²´í¬ëŠ” <code>nsmc</code> ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  ê°„ë‹¨í•˜ê²Œ í‰ê°€í•˜ì˜€ë‹¤ (ì‚¬ì‹¤ GPUê°€ 1ê°œ ë°–ì— ì—†ì–´ <code>nsmc</code>ë§Œ í…ŒìŠ¤íŠ¸í•œ ê±´ ë¹„ë°€ğŸ˜¢)</p><table><thead><tr><th>Acc(%)</th><th>25K</th><th>75K</th><th>90K</th><th>125K</th><th>150K</th><th>250K</th><th>300K</th><th>450K</th></tr></thead><tbody><tr><td><code>Base</code></td><td>88.10</td><td>88.48</td><td>88.67</td><td>88.92</td><td>88.97</td><td>89.51</td><td>89.65</td><td>90.16</td></tr></tbody></table><p>Stepì´ ì¦ê°€í• ìˆ˜ë¡ accuracyê°€ ì˜¤ë¥´ëŠ” ê²ƒì´ ëˆˆì— ë„ê²Œ ë³´ì´ë‹ˆ ì‹ ê¸°í•˜ê¸´ í–ˆë‹¤. (ì´ê²ƒì´ Pretrainingì˜ í˜ì¸ê°€â€¦.)</p><h3 id="Training-Time"><a href="#Training-Time" class="headerlink" title="Training Time"></a>Training Time</h3><p>ë°ì´í„°ì˜ ê²½ìš° <strong>14GB</strong>ë¡œ ì´ <code>2.6B Token</code>ì´ë‹¤. BERTì™€ ELECTRAì—ì„œëŠ” <code>3.3B Token</code>ì„ ì‚¬ìš©í•œ ê²ƒì— ë¹„í•˜ë©´ ë°ì´í„°ì˜ ì–‘ì´ ì¡°ê¸ˆ ëª¨ìë€ ê²Œ ì•„ì‰½ê¸´ í•˜ì§€ë§Œ, ì´ê²ƒì´ ê°œì¸ ë‹¨ìœ„ì—ì„œ ëª¨ì„ ìˆ˜ ìˆì—ˆë˜ ìµœì„ ì˜ ë°ì´í„°ì–‘ì´ì—ˆë‹¤ğŸ˜µ</p><p><strong>TPU v3-8</strong> ê¸°ì¤€ìœ¼ë¡œ <code>Base</code> ëª¨ë¸ì€ <strong>ì•½ 7ì¼</strong>, <code>Small</code> ëª¨ë¸ì€ <strong>ì•½ 3ì¼</strong>ì´ ì†Œìš”ë˜ì—ˆë‹¤. ê·¸ë˜ì„œ ì´ ê¸°ê°„ ë™ì•ˆ <strong>Finetuning</strong> ì½”ë“œë¥¼ ì§œëŠ” ê²ƒìœ¼ë¡œ ì‹œê°„ì„ ì ˆì•½í•˜ì˜€ë‹¤.</p><h2 id="Finetuning-ì½”ë“œ-ì œì‘"><a href="#Finetuning-ì½”ë“œ-ì œì‘" class="headerlink" title="Finetuning ì½”ë“œ ì œì‘"></a>Finetuning ì½”ë“œ ì œì‘</h2><p>ì½”ë“œì˜ ê²½ìš°ëŠ” <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="external nofollow noopener noreferrer" target="_blank">Transformersì˜ Example ì½”ë“œ</a>ë¥¼ ì°¸ê³ í•˜ì—¬ ì œì‘í•˜ì˜€ë‹¤.</p><p><strong>Finetuningì˜ ê²½ìš° ì´ 7ê°œì˜ taskì— ëŒ€í•´ ì§„í–‰í•˜ì˜€ë‹¤.</strong> (ë•Œë§ˆì¹¨ ì–¼ë§ˆ ì „ ì¹´ì¹´ì˜¤ë¸Œë ˆì¸ì—ì„œ <code>KorNLI</code>ì™€ <code>KorSTS</code> ë°ì´í„°ì…‹ì„ ê³µê°œí•´ì£¼ì—ˆë‹¤ğŸ‘ 1ë…„ ì „ê³¼ ë¹„êµí–ˆì„ ë•Œ ë²¤ì¹˜ë§ˆí¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ë°ì´í„°ì…‹ì´ ë§ì•„ì§„ ê²ƒì€ ì •ë§ ì¢‹ì€ ì¼ì´ë¼ í•  ìˆ˜ ìˆë‹¤.)</p><table><thead><tr><th></th><th align="center">NSMC</th><th align="center">PAWS</th><th align="center">QuestionPair</th><th align="center">KorNLI</th><th align="center">KorSTS</th><th align="center">NaverNER</th><th align="center">KorQuad</th></tr></thead><tbody><tr><td><strong>Task</strong></td><td align="center">ê°ì •ë¶„ì„</td><td align="center">ìœ ì‚¬ë¬¸ì¥</td><td align="center">ìœ ì‚¬ë¬¸ì¥</td><td align="center">ì¶”ë¡ </td><td align="center">ìœ ì‚¬ë¬¸ì¥</td><td align="center">ê°œì²´ëª…ì¸ì‹</td><td align="center">ê¸°ê³„ë…í•´</td></tr><tr><td><strong>Metric</strong></td><td align="center">Acc</td><td align="center">Acc</td><td align="center">Acc</td><td align="center">Acc</td><td align="center">Spearman</td><td align="center">F1</td><td align="center">EM/F1</td></tr></tbody></table><p>ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì—ì„œëŠ” <code>Bert-Multilingual-Cased</code>ë¥¼ ê°€ì§€ê³  ë§ì´ ë¹„êµí•˜ì˜€ëŠ”ë°, ì´ë²ˆ ì—°êµ¬ì—ì„œëŠ” <code>XLM-Roberta-Base</code> ëª¨ë¸ë¡œ í‰ê°€ë¥¼ ì‹œë„í•˜ì˜€ë‹¤. í™•ì‹¤íˆ xlm-robertaê°€ bertë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ì¢‹ì•˜ê¸°ì—, ì ì–´ë„ <code>KoELECTRA</code>ê°€ xlm-robertaëŠ” ë›°ì–´ë„˜ì–´ì•¼ ìœ ì˜ë¯¸í•˜ì§€ ì•Šì„ê¹Œë¼ê³  ìƒê°í•´ì„œ ì˜€ë‹¤.</p><p><a href="https://deview.kr/2019/schedule/291" rel="external nofollow noopener noreferrer" target="_blank">Deviewì—ì„œ ë°œí‘œí•œ Larva</a>ì˜ ê²½ìš° Benchmark pipelineì„ ë§Œë“¤ì–´ì„œ ckptê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ê³„ì† evaluationì„ í•´ì£¼ì—ˆë‹¤ê³  í•˜ëŠ”ë°, ì•ì—ì„œë„ ë§í–ˆë“¯ì´ ë‚˜ì—ê²ŒëŠ” GPUê°€ 1ê°œ ë°–ì— ì—†ê³ , GCPì—ì„œ GPUë¥¼ ë§ì´ ë¹Œë¦´ ìˆ˜ë„ ì—†ê¸°ì— <strong>ê°€ì¥ ìµœê·¼ 5ê°œì˜ ckptë¥¼ ê°€ì§€ê³  í‰ê°€</strong>í•˜ì˜€ê³ , ê·¸ ì¤‘ì—ì„œ í‰ê· ê°’ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ê²ƒì„ ìµœì¢… ëª¨ë¸ë¡œ ì„ ì •í•˜ì˜€ë‹¤.</p><p><strong>Finetuningìš© ì½”ë“œ ë° ì‚¬ìš©ë²•</strong>ì€ <a href="https://github.com/monologg/KoELECTRA/tree/master/finetune" rel="external nofollow noopener noreferrer" target="_blank">ì—¬ê¸°</a>ì—ì„œ ì§ì ‘ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤.</p><h2 id="Convert-from-Tensorflow-to-Pytorch"><a href="#Convert-from-Tensorflow-to-Pytorch" class="headerlink" title="Convert from Tensorflow to Pytorch"></a>Convert from Tensorflow to Pytorch</h2><p>Huggingfaceì—ì„œ <code>ElectraModel</code>ì„ êµ¬í˜„í•˜ë©´ì„œ tensorflowë¥¼ pytorchë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œë„ í•¨ê»˜ ë§Œë“¤ì–´ì¤¬ë‹¤ğŸ˜</p><p>ê´€ë ¨ ë‚´ìš©ì€ [<a href="/2020/05/01/transformers-porting/" title="ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°">ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°</a>]ë¥¼ ì½ì–´ë³´ê¸¸ ë°”ë€ë‹¤. (<strong>ì—¬ê¸°ì„œë„ êµ‰ì¥íˆ ì‚½ì§ˆì„ ë§ì´ í•´ë³¸ ì…ì¥ìœ¼ë¡œì„œ ê¼­ ì½ì–´ë³´ê¸¸ ê¶Œí•œë‹¤</strong>)</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><div class="justified-gallery"><p><img src="/images/2020-05-02-koelectra-part2/base_result.png" alt="Base result"><br><img src="/images/2020-05-02-koelectra-part2/small_result.png" alt="Small result"></p></div><p>ì²˜ìŒì— ì´ í”„ë¡œì íŠ¸ë¥¼ ê³„íší•  ë•Œ ê°€ì¥ ê±±ì •ë˜ì—ˆë˜ ì ì´ <strong>â€œì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¤ë©´ ì–´ì©Œë‚˜â€</strong>ì˜€ë‹¤. (ì„±ëŠ¥ì´ ë„ˆë¬´ ì•ˆ ì¢‹ìœ¼ë©´ ì‚¬ì‹¤ 2ì£¼ë¥¼ ì œëŒ€ë¡œ ë‚ ë¦° ì…ˆì´ê¸°ì—â€¦.)</p><p><strong>ê²°ê³¼ëŠ” ì˜ˆìƒí–ˆë˜ ê²ƒë³´ë‹¤ í›¨ì”¬ ì¢‹ì•˜ë‹¤.</strong> ì• ì´ˆì— ë°ì´í„°ì˜ ì–‘ì´ë‚˜ Tokenizer ë“±ì„ ê³ ë ¤í–ˆì„ ë•Œ <code>HanBERT</code>ë¥¼ ì™„ë²½íˆ ë”°ë¼ì¡ëŠ” ê²ƒì€ ë¬´ë¦¬ë¼ê³  ìƒê°í–ˆì§€ë§Œ, <code>KoBERT</code>ë³´ë‹¤ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë§ì´ ì¢‹ì„ ì¤„ì€ ëª°ëë‹¤. <code>HanBERT</code>ì™€ë„ ì‹¤ì§ˆì ì¸ ê²°ê³¼ëŠ” ë¹„ìŠ·í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ë” ì¢‹ì€ ì¼€ì´ìŠ¤ë„ ìˆì–´ì„œ ì´ ì •ë„ë©´ ğŸ‰<strong>ëŒ€ì„±ê³µ</strong>ğŸ‰ì´ë¼ê³  ë´ë„ ë  êº¼ ê°™ë‹¤.</p><p><code>KoELECTRA-Small</code>ì˜ ì„±ëŠ¥ì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆëŠ”ë°, ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ <code>DistilKoBERT</code>ì˜ ì ˆë°˜ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. <strong>ê²½ëŸ‰í™” ëª¨ë¸ì—ì„œ ì¶©ë¶„íˆ ì‚¬ìš©í•  ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ë‹¤.</strong></p><h2 id="How-to-Use"><a href="#How-to-Use" class="headerlink" title="How to Use"></a>How to Use</h2><p>ì´ í”„ë¡œì íŠ¸ë¥¼ ì²˜ìŒ ê³„íší–ˆì„ ë•Œì˜ ê³ ë ¤ì‚¬í•­ ì¤‘ ì•„ë˜ ì‚¬í•­ë“¤ì´ ê°€ì¥ ì¤‘ìš”í–ˆì—ˆë‹¤.</p><blockquote><p>â€œ<strong>ëª¨ë“  OS</strong>ì—ì„œ ì‚¬ìš© ê°€ëŠ¥â€<br>â€œ<strong>tokenization íŒŒì¼ì„ ë§Œë“¤ í•„ìš” ì—†ì´</strong> ê³§ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥â€</p></blockquote><p>ê·¸ë¦¬ê³  ì´ì œ <code>transformers</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ìˆìœ¼ë©´ ì–´ë– í•œ í™˜ê²½ì—ì„œë„ <code>í•œêµ­ì–´ PLM</code>ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œëœ ê²ƒì´ë‹¤ğŸ¤—</p><figure class="highlight bash"><figcaption><span>Installation</span></figcaption><table><tr><td class="code"><pre><span class="line">$ pip3 install -U transformers</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>How to Use</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ElectraModel, ElectraTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># KoELECTRA-Base</span></span><br><span class="line">model = ElectraModel.from_pretrained(<span class="string">"monologg/koelectra-base-discriminator"</span>)</span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-base-discriminator"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KoELECTRA-Small</span></span><br><span class="line">model = ElectraModel.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br></pre></td></tr></table></figure><h2 id="ë§ºìœ¼ë©°"><a href="#ë§ºìœ¼ë©°" class="headerlink" title="ë§ºìœ¼ë©°"></a>ë§ºìœ¼ë©°</h2><center><p><img src="/images/2020-05-02-koelectra-part2/facebook-reaction.png" alt="ì™€ìš°!"></p></center><p>ì†”ì§íˆ ì´ë ‡ê²Œê¹Œì§€ ë°˜ì‘ì´ ì¢‹ì„ ì¤„ì€ ëª°ëë‹¤ğŸ™„ ê°œë°œìë¡œì„œ ì´ëŸ´ ë•Œê°€ ê°€ì¥ ë³´ëŒìˆì§€ ì•Šì€ê°€ ì‹¶ë‹¤.</p><p>ì²˜ìŒ ê³„íší•  ë•Œë¶€í„° <strong>â€œì´ê±° ë§Œë“¤ë©´ ë‹¤ë¥¸ ë¶„ë“¤ì—ê²Œë„ í° ë„ì›€ì´ ë˜ê² ë‹¤â€</strong>ë¼ ìƒê°í–ˆëŠ”ë°, ì‹¤ì œë¡œë„ ê·¸ëŸ° ê²ƒ ê°™ì•„ ê¸°ë¶„ì´ (êµ‰ì¥íˆ) ì¢‹ë‹¤ ğŸ˜€</p><blockquote><p>â€œë‚´ê°€ ë¶ˆí¸í•´í•˜ëŠ” ê²ƒì€ ë¶„ëª… ë‹¤ë¥¸ ëˆ„êµ°ê°€ë„ ë¶ˆí¸í•´í•œë‹¤. ê·¸ëŸ° ë¶€ë¶„ì„ í•´ê²°í•´ì£¼ëŠ” ê²ƒì´ ê°œë°œìì˜ ì—­í• ì´ë¼ê³  ìƒê°í•œë‹¤.â€</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA Official Code</a></li><li><a href="https://github.com/huggingface/transformers" rel="external nofollow noopener noreferrer" target="_blank">Huggingface Transformers</a></li><li><a href="https://deview.kr/data/deview/2019/presentation/[111]+%E1%84%8B%E1%85%A5%E1%86%B7_%E1%84%8E%E1%85%A5%E1%86%BC+%E1%84%8F%E1%85%B3%E1%86%AB+%E1%84%8B%E1%85%A5%E1%86%AB%E1%84%8B%E1%85%A5+%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF+%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%A1%E1%86%BC+%E1%84%80%E1%85%A1%E1%84%83%E1%85%A9%E1%86%BC%E1%84%80%E1%85%B5.pdf" rel="external nofollow noopener noreferrer" target="_blank">Deview 2019 Larva Presentation</a></li><li><a href="https://github.com/kakaobrain/KorNLUDatasets" rel="external nofollow noopener noreferrer" target="_blank">KorNLI, KorSTS</a></li></ul><h2 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h2><ul><li><a href="/2020/05/01/transformers-porting/" title="ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°">ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°</a></li><li><a href="/2020/05/02/koelectra-part1/" title="2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 1ë¶€">2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 1ë¶€</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/05/02/koelectra-part2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 1ë¶€</title>
      <link>https://monologg.kr/2020/05/02/koelectra-part1/</link>
      <guid>https://monologg.kr/2020/05/02/koelectra-part1/</guid>
      <pubDate>Fri, 01 May 2020 16:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;strong&gt;2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œ&lt;/strong&gt;ì„ ë§ˆì¹˜ê³ , ê·¸ ê³¼ì •ì„ ê¸€ë¡œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤.&lt;/p&gt;
&lt;p&gt;ì´ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ë“¤ì€ ë‚´ê°€ í–ˆë˜ &lt;strong&gt;ì‚½ì§ˆ(?)&lt;/strong&gt;ì„ ìµœëŒ€í•œ ëœ í•˜ê¸¸ ë°”ë¼ëŠ” ë§ˆìŒì´ë‹¤:)&lt;/p&gt;
&lt;p&gt;1ë¶€ì—ëŠ” &lt;strong&gt;ì‹¤ì œ í•™ìŠµì„ ëŒë¦¬ê¸° ì „ê¹Œì§€ì˜ ê³¼ì •&lt;/strong&gt;ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Github Repo:&lt;/strong&gt; &lt;a href=&quot;https://github.com/monologg/KoELECTRA&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://github.com/monologg/KoELECTRA&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><strong>2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œ</strong>ì„ ë§ˆì¹˜ê³ , ê·¸ ê³¼ì •ì„ ê¸€ë¡œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤.</p><p>ì´ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ë“¤ì€ ë‚´ê°€ í–ˆë˜ <strong>ì‚½ì§ˆ(?)</strong>ì„ ìµœëŒ€í•œ ëœ í•˜ê¸¸ ë°”ë¼ëŠ” ë§ˆìŒì´ë‹¤:)</p><p>1ë¶€ì—ëŠ” <strong>ì‹¤ì œ í•™ìŠµì„ ëŒë¦¬ê¸° ì „ê¹Œì§€ì˜ ê³¼ì •</strong>ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.</p><blockquote><p><strong>Github Repo:</strong> <a href="https://github.com/monologg/KoELECTRA" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/monologg/KoELECTRA</a></p></blockquote><a id="more"></a><h2 id="ë¬¸ì œ-ì˜ì‹"><a href="#ë¬¸ì œ-ì˜ì‹" class="headerlink" title="ë¬¸ì œ ì˜ì‹"></a>ë¬¸ì œ ì˜ì‹</h2><p>í•œêµ­ì— Publicí•˜ê²Œ ê³µê°œë˜ì–´ ìˆëŠ” <code>í•œêµ­ì–´ PLM(Pretrained Language Model)</code>ì—ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ìˆë‹¤.</p><ol><li>SKTì˜ <code>KoBERT</code></li><li>TwoBlock AIì˜ <code>HanBERT</code></li><li>ETRIì˜ <code>KorBERT</code></li></ol><p>3ê°€ì§€ ëª¨ë‘ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, 3ê°€ì§€ ëª¨ë‘ ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. ê°ê°ì˜ ë‹¨ì ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p><table><thead><tr><th align="left"></th><th align="left">ë‹¨ì </th></tr></thead><tbody><tr><td align="left"><strong>KoBERT</strong></td><td align="left"><strong>Vocab size (8002ê°œ)</strong>ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì‘ìŒ</td></tr><tr><td align="left"><strong>HanBERT</strong></td><td align="left">Tokenizerë¡œ ì¸í•´ <strong>Ubuntu í™˜ê²½</strong>ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥</td></tr><tr><td align="left"><strong>KorBERT</strong></td><td align="left"><strong>API ì‹ ì²­</strong> ë“±ì˜ ê³¼ì • í•„ìš”</td></tr></tbody></table><p>íŠ¹íˆ 3ê°€ì§€ ëª¨ë‘ ê³µí†µì ìœ¼ë¡œ <code>Huggingface Transformers</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ <strong>tokenization íŒŒì¼ì„ ë”°ë¡œ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ë‹¨ì </strong>ì´ ìˆë‹¤.</p><p><a href="https://github.com/monologg/DistilKoBERT" rel="external nofollow noopener noreferrer" target="_blank">KoBERT</a>ì™€ <a href="https://github.com/monologg/HanBert-Transformers" rel="external nofollow noopener noreferrer" target="_blank">HanBERT</a>ì˜ ê²½ìš° ë‚´ê°€ ì§ì ‘ tokenization íŒŒì¼ì„ ë§Œë“¤ì–´ì„œ githubì— ë°°í¬í–ˆì§€ë§Œ, ì¼ë¶€ ì‚¬ìš©ìë“¤ì´ ë¶ˆí¸í•¨ì„ í† ë¡œí•˜ê¸°ë„ í–ˆë‹¤.</p><p>ê·¸ë˜ì„œ ì´ë²ˆ ê¸°íšŒì— ìœ„ì˜ ë‹¨ì ë“¤ì„ ëª¨ë‘ í•´ê²°í•œ <code>í•œêµ­ì–´ PLM</code>ì„ ê°œë°œí•˜ê³  ì‹¶ì—ˆë‹¤.</p><blockquote><ol><li><strong>Vocab size</strong>ê°€ ì–´ëŠ ì •ë„ ì»¤ì•¼ í•¨ (30000ê°œ ì •ë„)</li><li><strong>ëª¨ë“  OS</strong>ì—ì„œ ì‚¬ìš© ê°€ëŠ¥</li><li><strong>tokenization íŒŒì¼ì„ ë§Œë“¤ í•„ìš” ì—†ì´</strong> ê³§ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥</li><li>ì–´ëŠ ì •ë„ <strong>ì„±ëŠ¥</strong>ê¹Œì§€ ë³´ì¥ë˜ì–´ì•¼ í•¨</li></ol></blockquote><p>ìœ„ì˜ 4ê°€ì§€ë¥¼ ëª¨ë‘ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•˜ì—¬ ì‹œì‘í•œ í”„ë¡œì íŠ¸ê°€ ë°”ë¡œ <code>KoELECTRA</code> ì´ë‹¤.</p><h2 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h2><p>ì‹¤ì œ í˜„ì—…ì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•´ <code>Mecab + Sentencepiece</code>ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³µì‹ BERT, ELECTRA ë“±ì€ <code>Wordpiece</code>ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, <code>transformers</code>ì—ì„œë„ ê³µì‹ì ìœ¼ë¡œ Wordpieceë§Œ ì§€ì›í•˜ê³  ìˆë‹¤.</p><p>ì¦‰, ELECTRAì—ì„œ Mecabì´ë‚˜ Sentencepieceë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ <code>tokenization</code> íŒŒì¼ì„ ë§Œë“¤ì–´ì•¼ í•˜ë©°, <code>transformers</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ apiê°€ ë°”ë€Œë©´ ë‚´ê°€ ì§ì ‘ ê·¸ì— ë§ê²Œ tokenization íŒŒì¼ì„ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. (KoBERTì˜ ê²½ìš° ì‹¤ì œë¡œë„ ê·¸ë ‡ê²Œ í•˜ê³  ìˆë‹¤ã… )</p><p><strong>ì´ëŸ¬í•œ ë¬¸ì œë“¤ ë•Œë¬¸ì— ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë¬´ì¡°ê±´ìœ¼ë¡œ <code>Wordpiece</code>ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì•ˆìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.</strong></p><h3 id="Wordpiece"><a href="#Wordpiece" class="headerlink" title="Wordpiece"></a>Wordpiece</h3><blockquote><p>â€œWordpiece vocabì„ ë§Œë“¤ ë•Œ Huggingfaceì˜ <a href="https://github.com/huggingface/tokenizers" rel="external nofollow noopener noreferrer" target="_blank"><code>Tokenizers</code></a> ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ë‹¤.â€</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--corpus_file"</span>, type=str)</span><br><span class="line">parser.add_argument(<span class="string">"--vocab_size"</span>, type=int, default=<span class="number">32000</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--limit_alphabet"</span>, type=int, default=<span class="number">6000</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">tokenizer = BertWordPieceTokenizer(</span><br><span class="line">    vocab_file=<span class="literal">None</span>,</span><br><span class="line">    clean_text=<span class="literal">True</span>,</span><br><span class="line">    handle_chinese_chars=<span class="literal">True</span>,</span><br><span class="line">    strip_accents=<span class="literal">False</span>, <span class="comment"># Must be False if cased model</span></span><br><span class="line">    lowercase=<span class="literal">False</span>,</span><br><span class="line">    wordpieces_prefix=<span class="string">"##"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.train(</span><br><span class="line">    files=[args.corpus_file],</span><br><span class="line">    limit_alphabet=args.limit_alphabet,</span><br><span class="line">    vocab_size=args.vocab_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.save(<span class="string">"./"</span>, <span class="string">"bert-wordpiece"</span>)</span><br></pre></td></tr></table></figure><blockquote><ul><li>Vocab sizeì˜ ê²½ìš° ê´€ë¡€ì ìœ¼ë¡œ ë§ì´ ì“°ì´ëŠ” <strong>ì•½ 3ë§Œê°œ</strong>ë¡œ ì„¸íŒ…í•˜ì˜€ë‹¤.</li><li><strong>ì „ì²˜ë¦¬ ì—†ì´</strong> ì›ë³¸ Corpusë§Œ ê°€ì§€ê³  vocab ë§Œë“¤ë©´ <strong>ì„±ëŠ¥ì´ êµ‰ì¥íˆ ì•ˆ ì¢‹ìŒ</strong></li><li>character coverageë¥¼ ìµœëŒ€í•œ ë†’ê²Œ ì¡ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  íŒë‹¨ (<strong>ì¦‰, corpusì—ì„œ ë“±ì¥í–ˆë˜ ëª¨ë“  characterë¥¼ vocabì— í¬í•¨ì‹œí‚´</strong>)<ul><li>ì˜ˆë¥¼ ë“¤ì–´ <code>í€­ë©”ì¼</code>ì´ë€ ë‹¨ì–´ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì.</li><li>ë§Œì¼ <code>í€­</code>ì´ vocabì— ì—†ë‹¤ë©´ <code>í€­ë©”ì¼</code> ì „ì²´ë¥¼ <code>[UNK]</code>ë¡œ ì²˜ë¦¬í•˜ê²Œ ëœë‹¤.</li><li>ë§Œì¼ <code>í€­</code>ì´ vocab ì•ˆì— ìˆìœ¼ë©´ <code>í€­ + ##ë©”ì¼</code>ë¡œ tokenizeê°€ ë  ìˆ˜ ìˆì–´ <code>##ë©”ì¼</code>ì´ë€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤.</li></ul></li></ul></blockquote><p>(ìì„¸í•œ ë‚´ìš©ì€ ì´ì „ì— í¬ìŠ¤íŒ…í•œ <a href="https://monologg.kr/2020/04/27/wordpiece-vocab/">[ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°]</a>ì„ ì°¸ê³ )</p><h2 id="ì „ì²˜ë¦¬-Preprocessing"><a href="#ì „ì²˜ë¦¬-Preprocessing" class="headerlink" title="ì „ì²˜ë¦¬ (Preprocessing)"></a>ì „ì²˜ë¦¬ (Preprocessing)</h2><blockquote><p>â€œì²«ì§¸ë„ <strong>ì „ì²˜ë¦¬</strong>! ë‘˜ì§¸ë„ <strong>ì „ì²˜ë¦¬</strong>! ì…‹ì§¸ë„ <strong>ì „ì²˜ë¦¬</strong>!â€<br>â€œPLMì˜ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì€ <code>corpus quality</code>ì´ë‹¤!â€</p></blockquote><p><strong>í¬ë¡¤ë§í•œ ë‰´ìŠ¤ì˜ ë¬¸ì¥ í•˜ë‚˜</strong>ë¥¼ ì‚´í´ë³´ì</p><blockquote><p>[ì£¼ìš”ê¸°ì‚¬] â˜ [í¬í†  ìŠ¤í† ë¦¬] ë¬´í—ˆê°€ ë„ì‹œê´‘ì‚° ì„ ì•„ì‹œë‚˜ìš”? â˜ [ë”°ëœ»í•œ ì‚¬ì§„ í•œ ì¥] ì‚¬ë‘, í•˜ë‚˜ê°€ ë˜ì–´ ê°€ëŠ” ê¸¸ &lt;ì°°ë‚˜ì˜ ê¸°ë¡, ìˆœê°„ì˜ ì§„ì‹¤ / KPPA ë°”ë¡œê°€ê¸°&gt; Copyrightsâ“’ í•œêµ­ì‚¬ì§„ê¸°ìí˜‘íšŒ(<a href="http://www.kppa.or.kr" rel="external nofollow noopener noreferrer" target="_blank">www.kppa.or.kr</a>), powered by castnet. ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€ ë³´ë‚´ê¸°</p></blockquote><p>ë¬¸ì œëŠ” ì´ëŸ° ë¬¸ì¥ì´ ë§¤ìš° ë§ì€ë°ë‹¤ê°€, ì´ê±¸ Pretrainì— ë„£ì„ ì‹œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§ˆ ê²Œ ë»”í•˜ë‹¤â€¦.</p><p>ì´ë ‡ê²Œ noiseê°€ ë§ì€ Corpusë¡œ vocabì„ ë§Œë“¤ê³  pretrainê¹Œì§€ í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì„ ë¦¬ê°€ ì—†ë‹¤.</p><p>ì•„ë˜ëŠ” ë‚´ê°€ ì ìš©í•œ ëŒ€í‘œì ì¸ ì „ì²˜ë¦¬ ê¸°ì¤€ì´ë‹¤.</p><blockquote><ul><li>í•œì, ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ì ì œê±°</li><li><strong>í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ê¸°</strong> (<a href="https://github.com/likejazz/korean-sentence-splitter" rel="external nofollow noopener noreferrer" target="_blank">kss</a>) ì‚¬ìš©</li><li>ë‰´ìŠ¤ ê´€ë ¨ ë¬¸ì¥ì€ ì œê±° (<code>ë¬´ë‹¨ì „ì¬</code>, <code>(ì„œìš¸=ë‰´ìŠ¤1)</code> ë“± í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ ì œì™¸)</li></ul></blockquote><p>ì‚¬ì‹¤ ì „ì²˜ë¦¬ì˜ ê¸°ì¤€ì— ì •ë‹µì€ ì—†ë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ <strong>ìì‹ ì˜ Taskì— ë§ê²Œ ì „ì²˜ë¦¬ ê¸°ì¤€ì„ ì„¸ìš°ëŠ” ê²ƒì´ë‹¤</strong>. ë‚´ê°€ ìƒê°í–ˆë˜ Task ë“¤ì—ëŠ” í•œìëŠ” ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•´ì„œ ì§€ìš´ ê²ƒì´ì§€, í•œìê°€ ê¼­ í•„ìš”í•œ Taskì˜ ê²½ìš°ì—ëŠ” ì§€ìš°ë©´ ì•ˆ ë  ê²ƒì´ë‹¤.</p><h2 id="TPU-ì‚¬ìš©-ê´€ë ¨-Tip"><a href="#TPU-ì‚¬ìš©-ê´€ë ¨-Tip" class="headerlink" title="TPU ì‚¬ìš© ê´€ë ¨ Tip"></a>TPU ì‚¬ìš© ê´€ë ¨ Tip</h2><p>(ìì„¸í•œ ë‚´ìš©ì€ ì´ì „ì— í¬ìŠ¤íŒ…í•œ <a href="https://monologg.kr/2020/04/20/tpu-electra/">[TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸°]</a>ì„ ì°¸ê³ )</p><h3 id="1-Tensorflow-Research-Cloud-TFRC-ë¥¼-ì“°ë©´-TPUê°€-ë¬´ë£Œ"><a href="#1-Tensorflow-Research-Cloud-TFRC-ë¥¼-ì“°ë©´-TPUê°€-ë¬´ë£Œ" class="headerlink" title="1. Tensorflow Research Cloud(TFRC)ë¥¼ ì“°ë©´ TPUê°€ ë¬´ë£Œ"></a>1. Tensorflow Research Cloud(TFRC)ë¥¼ ì“°ë©´ TPUê°€ ë¬´ë£Œ</h3><p>â†’ ì´ë¯¸ <a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ê³µì‹ ì½”ë“œ</a>ê°€ TPUë¥¼ ì™„ë²½íˆ ì§€ì›í•˜ê¸°ì—, ì§ì ‘ ELECTRAë¥¼ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ <strong>GPUë³´ë‹¤ëŠ” TPUë¥¼ ì“°ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•œë‹¤.</strong></p><h3 id="2-VM-InstanceëŠ”-ì‘ì€-ê²ƒ-n1-standard-1-ì„-ì¨ë„-ìƒê´€-ì—†ë‹¤"><a href="#2-VM-InstanceëŠ”-ì‘ì€-ê²ƒ-n1-standard-1-ì„-ì¨ë„-ìƒê´€-ì—†ë‹¤" class="headerlink" title="2. VM InstanceëŠ” ì‘ì€ ê²ƒ(n1-standard-1)ì„ ì¨ë„ ìƒê´€ ì—†ë‹¤"></a>2. VM InstanceëŠ” ì‘ì€ ê²ƒ(<code>n1-standard-1</code>)ì„ ì¨ë„ ìƒê´€ ì—†ë‹¤</h3><p>â†’ ELECTRAë¥¼ GCPì—ì„œ í•™ìŠµí•˜ë ¤ë©´ <code>TPU</code>, <code>Bucket</code>, <code>VM Instance</code> ì´ë ‡ê²Œ 3ê°œê°€ í•„ìš”í•˜ë‹¤. ê·¸ëŸ°ë° <strong>ì €ì¥ì†ŒëŠ” Bucketì´, ì—°ì‚°ì€ TPUê°€ ì²˜ë¦¬</strong>í•˜ê¸° ë•Œë¬¸ì— VM InstanceëŠ” ê°€ë²¼ìš´ ê²ƒì„ ì¨ë„ ëœë‹¤.<br>â†’ <code>n1-standard-1</code>ëŠ” ì‹œê°„ë‹¹ ì•½ $0.037, <code>n1-standard-4</code>ëŠ” ì‹œê°„ë‹¹ ì•½ $0.14ì´ë‹¤. (<strong>ë¹„ìš©ì´ ë¬´ë ¤ 2ë°° ì°¨ì´!!</strong>)</p><h3 id="3-TPUë¥¼-ì“°ëŠ”-ê²½ìš°-ëª¨ë“ -input-fileì€-Cloud-storage-bucketì„-í†µí•´ì•¼ë§Œ-í•œë‹¤"><a href="#3-TPUë¥¼-ì“°ëŠ”-ê²½ìš°-ëª¨ë“ -input-fileì€-Cloud-storage-bucketì„-í†µí•´ì•¼ë§Œ-í•œë‹¤" class="headerlink" title="3. TPUë¥¼ ì“°ëŠ” ê²½ìš° ëª¨ë“  input fileì€ Cloud storage bucketì„ í†µí•´ì•¼ë§Œ í•œë‹¤"></a>3. TPUë¥¼ ì“°ëŠ” ê²½ìš° ëª¨ë“  input fileì€ Cloud storage bucketì„ í†µí•´ì•¼ë§Œ í•œë‹¤</h3><p>â†’ ì´ê²ƒë„ ì²˜ìŒì— ëª°ëë‹¤ê°€ ê³ ìƒí–ˆë˜ ì‚½ì§ˆ ì¤‘ í•˜ë‚˜ë‹¤. <code>tf.estimator.tpu</code> ìª½ ì½”ë“œë¥¼ ì“°ëŠ” ê²½ìš° ë¡œì»¬ ë°ì´í„°ê°€ ì•„ë‹Œ <code>Bucket</code>ì„ í†µí•´ì•¼ë§Œ í•œë‹¤. (<a href="https://cloud.google.com/tpu/docs/troubleshooting?hl=ko#common-errors" rel="external nofollow noopener noreferrer" target="_blank">ê´€ë ¨ FAQ</a>)<br>â†’ ë‹¤í–‰íˆë„ Bucketì˜ ë¹„ìš©ì´ ë¹„ì‹¸ì§€ê°€ ì•Šë‹¤ (íŠ¹ì • ë¦¬ì „ì— ë§Œë“¤ì–´ ë†“ìœ¼ë©´ 1GBë‹¹ ì›” ì•½ $0.03)</p><h3 id="4-VM-Instanceì™€-TPUë¥¼-ë§Œë“¤-ë•Œ-ctpu-up-ëª…ë ¹ì–´ë¥¼-ì‚¬ìš©í•´ë¼"><a href="#4-VM-Instanceì™€-TPUë¥¼-ë§Œë“¤-ë•Œ-ctpu-up-ëª…ë ¹ì–´ë¥¼-ì‚¬ìš©í•´ë¼" class="headerlink" title="4. VM Instanceì™€ TPUë¥¼ ë§Œë“¤ ë•Œ ctpu up ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ë¼"></a>4. VM Instanceì™€ TPUë¥¼ ë§Œë“¤ ë•Œ <code>ctpu up</code> ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ë¼</h3><p>â†’ VM Instanceì™€ TPUë¥¼ ë”°ë¡œ ë”°ë¡œ ìƒì„±í•˜ë©´ ì²˜ìŒì— ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì—ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ <code>cloud shell</code>ë¡œ ëª…ë ¹ì–´ë¥¼ í•œ ë²ˆë§Œ ì¹˜ë©´ VMê³¼ TPUê°€ ë™ì‹œì— ìƒì„±ëœë‹¤ğŸ˜ƒ</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ctpu up --zone=europe-west4<span class="_">-a</span> --tf-version=1.15 \</span><br><span class="line">          --tpu-size=v3-8 --machine-type=n1-standard-2 \</span><br><span class="line">          --disk-size-gb=20 --name=&#123;<span class="variable">$VM_NAME</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="Configurationì˜-í•¨ì •"><a href="#Configurationì˜-í•¨ì •" class="headerlink" title="Configurationì˜ í•¨ì •"></a>Configurationì˜ í•¨ì •</h2><p>ì•ì—ì„œë„ ì–¸ê¸‰í–ˆë“¯ì´ ELECTRA Pretrainingì€ <a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ê³µì‹ ì½”ë“œ</a>ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ë‹¤ ì“°ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê³µì‹ ì½”ë“œë¥¼ ê°€ì ¸ë‹¤ì“°ê¸° ì „ì— <code>ë…¼ë¬¸</code>ê³¼ <code>ì½”ë“œ ë¶„ì„</code>ì„ ì–´ëŠ ì •ë„ í•˜ê³  ì§„í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤.</p><p>ê·¸ëŸ¼ì—ë„ ì¢€ í—·ê°ˆë ¸ë˜ ë¶€ë¶„ì´ ìˆì–´ ì—¬ê¸°ì„œ ì–¸ê¸‰í•˜ë ¤ í•œë‹¤.</p><h3 id="1-ê³µì‹-ë ˆí¬ì—ì„œ-ì œê³µí•˜ëŠ”-small-ëª¨ë¸ì€-ì •í™•íˆëŠ”-small-ëª¨ë¸ì´ë‹¤"><a href="#1-ê³µì‹-ë ˆí¬ì—ì„œ-ì œê³µí•˜ëŠ”-small-ëª¨ë¸ì€-ì •í™•íˆëŠ”-small-ëª¨ë¸ì´ë‹¤" class="headerlink" title="1. ê³µì‹ ë ˆí¬ì—ì„œ ì œê³µí•˜ëŠ” small ëª¨ë¸ì€ ì •í™•íˆëŠ” small++ ëª¨ë¸ì´ë‹¤"></a>1. ê³µì‹ ë ˆí¬ì—ì„œ ì œê³µí•˜ëŠ” <code>small</code> ëª¨ë¸ì€ ì •í™•íˆëŠ” <code>small++</code> ëª¨ë¸ì´ë‹¤</h3><p><img src="/images/2020-05-02-koelectra-part1/small_notice.png" alt><br><img src="/images/2020-05-02-koelectra-part1/small_gen_size.png" alt></p><p>ê³µì‹ ì½”ë“œì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ <code>small</code>ë¡œ ê³µê°œëœ ëª¨ë¸ì€ ì •í™•íˆ ë§í•˜ë©´ <code>small++</code> ëª¨ë¸ì´ë‹¤. ë‘˜ì˜ ì°¨ì´ì ì€ ì•„ë˜ì™€ ê°™ë‹¤.</p><table><thead><tr><th align="left"></th><th>max_seq_len</th><th>generator_hidden_size</th></tr></thead><tbody><tr><td align="left"><strong>small</strong></td><td>128</td><td>0.25</td></tr><tr><td align="left"><strong>small++</strong></td><td>512</td><td>1.0</td></tr></tbody></table><p>(<code>generator_hidden_size=1.0</code>ì´ë€ ê²ƒì€ <code>discriminator</code>ì™€ <code>generator</code>ì˜ hidden_sizeê°€ ê°™ë‹¤ëŠ” ê²ƒì´ë‹¤)</p><p>ì´ëŸ¬í•œ ë¶€ë¶„ì´ ë…¼ë¬¸ì— ìì„¸íˆ ë‚˜ì™€ ìˆì§€ ì•Šì•„ ì²˜ìŒì— small ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ì§„ì§œ smallë¡œ ë§Œë“¤ì—ˆë‹¤ê°€ ë‹¤ì‹œ small++ë¡œ ë§Œë“œëŠ” ìˆ˜ê³ ë¥¼ ê±°ì³¤ë‹¤â€¦. (ì´ ê¸€ì„ ì½ì€ ë¶„ë“¤ì€ ì´ ì‚½ì§ˆì„ ì•ˆ í•˜ê¸¸ ë¹ˆë‹¤.)</p><figure class="highlight json"><figcaption><span>hparams.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"tpu_name"</span>: <span class="string">"electra-small"</span>,</span><br><span class="line">  <span class="attr">"tpu_zone"</span>: <span class="string">"europe-west4-a"</span>,</span><br><span class="line">  <span class="attr">"num_train_steps"</span>: <span class="number">1000000</span>,</span><br><span class="line">  <span class="attr">"save_checkpoints_steps"</span>: <span class="number">50000</span>,</span><br><span class="line">  <span class="attr">"train_batch_size"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"learning_rate"</span>: <span class="number">5e-4</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">32200</span>,</span><br><span class="line">  <span class="attr">"max_seq_length"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"generator_hidden_size"</span>: <span class="number">1.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-max-seq-lengthë¥¼-128ë¡œ-ì¤„ì¸ë‹¤ë©´-max-position-embeddingsë„-128ë¡œ-ì¤„ì—¬ì•¼-í•œë‹¤"><a href="#2-max-seq-lengthë¥¼-128ë¡œ-ì¤„ì¸ë‹¤ë©´-max-position-embeddingsë„-128ë¡œ-ì¤„ì—¬ì•¼-í•œë‹¤" class="headerlink" title="2. max_seq_lengthë¥¼ 128ë¡œ ì¤„ì¸ë‹¤ë©´ max_position_embeddingsë„ 128ë¡œ ì¤„ì—¬ì•¼ í•œë‹¤"></a>2. max_seq_lengthë¥¼ 128ë¡œ ì¤„ì¸ë‹¤ë©´ max_position_embeddingsë„ 128ë¡œ ì¤„ì—¬ì•¼ í•œë‹¤</h3><p>ê°„í˜¹ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆëœ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ <code>max_seq_length</code>ë¥¼ ì¤„ì´ê³ ì í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.</p><p>ê·¸ëŸ´ ì‹œ <code>max_seq_length</code>ë§Œ ì¤„ì´ë©´ í•´ê²°ëœë‹¤ê³  ì˜¤í•´í•  ìˆ˜ ìˆëŠ”ë°, <code>max_position_embeddings</code>ë„ ì¤„ì—¬ì¤˜ì•¼ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ê²Œ ë³€í™˜í•  ë•Œ ë¬¸ì œê°€ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤. (transformersê°€ max_position_embeddingsìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì•Œì•„ë‚´ê¸° ë•Œë¬¸!)</p><p>ë” í° í•¨ì •ì€ ì•„ë˜ì™€ ê°™ì´ <code>model_hparam_overrides</code>ë¼ëŠ” attribute ì•ˆì— <code>max_position_embeddings</code>ë¥¼ ë„£ì–´ì¤˜ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤!</p><figure class="highlight json"><figcaption><span>hparams.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"max_seq_length"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"model_hparam_overrides"</span>: &#123;</span><br><span class="line">    <span class="attr">"max_position_embeddings"</span>: <span class="number">128</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="ë§ˆì¹˜ë©°"><a href="#ë§ˆì¹˜ë©°" class="headerlink" title="ë§ˆì¹˜ë©°"></a>ë§ˆì¹˜ë©°</h2><p>1ë¶€ì—ì„œëŠ” <strong>ì‹¤ì œ Pretrainingì„ ì‹œì‘í•˜ê¸° ì „ì˜ ì¤€ë¹„ ê³¼ì •</strong>ì„ ë‹¤ë¤˜ë‹¤.</p><a href="/2020/05/02/koelectra-part2/" title="2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€">2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€</a> ì—ì„œëŠ” ì‹¤ì œ Pretraining, Transformers í¬íŒ…, Finetuning ë“±ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤:)<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA Official Code</a></li><li><a href="https://github.com/huggingface/tokenizers" rel="external nofollow noopener noreferrer" target="_blank">Huggingface Tokenizers</a></li><li><a href="https://cloud.google.com/tpu/docs?hl=ko" rel="external nofollow noopener noreferrer" target="_blank">Cloud TPU Documentation</a></li></ul><h2 id="Related-Posts"><a href="#Related-Posts" class="headerlink" title="Related Posts"></a>Related Posts</h2><ul><li><a href="/2020/04/20/tpu-electra/" title="TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸°">TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸°</a></li><li><a href="/2020/04/27/wordpiece-vocab/" title="ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°">ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°</a></li><li><a href="/2020/05/02/koelectra-part2/" title="2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€">2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/05/02/koelectra-part1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°</title>
      <link>https://monologg.kr/2020/05/01/transformers-porting/</link>
      <guid>https://monologg.kr/2020/05/01/transformers-porting/</guid>
      <pubDate>Thu, 30 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;BERT&lt;/code&gt;, &lt;code&gt;ALBERT&lt;/code&gt;, &lt;code&gt;ELECTRA&lt;/code&gt; ë“±ì„ ì§ì ‘ Pretrainí•˜ê²Œ ë˜ë©´ ëª¨ë¸ì´ Tensorflowì˜ ckpt í˜•íƒœë¡œ ì €ì¥ì´ ëœë‹¤.&lt;/p&gt;
&lt;p&gt;ì´ë²ˆ ê¸€ì—ì„œëŠ” &lt;code&gt;tensorflow ckpt&lt;/code&gt;ë¥¼ transformersì˜ &lt;code&gt;pytorch ckpt&lt;/code&gt;ë¡œ ë³€í™˜í•˜ëŠ” ë²•ì„ ì•Œì•„ë³´ê² ë‹¤ğŸ¤—&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p><code>BERT</code>, <code>ALBERT</code>, <code>ELECTRA</code> ë“±ì„ ì§ì ‘ Pretrainí•˜ê²Œ ë˜ë©´ ëª¨ë¸ì´ Tensorflowì˜ ckpt í˜•íƒœë¡œ ì €ì¥ì´ ëœë‹¤.</p><p>ì´ë²ˆ ê¸€ì—ì„œëŠ” <code>tensorflow ckpt</code>ë¥¼ transformersì˜ <code>pytorch ckpt</code>ë¡œ ë³€í™˜í•˜ëŠ” ë²•ì„ ì•Œì•„ë³´ê² ë‹¤ğŸ¤—</p><a id="more"></a><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><ul><li>ì´ë²ˆ ê¸€ì—ì„œëŠ” <code>ELECTRA-Small</code>ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ìŠµì„ í•´ë³¸ë‹¤. (<code>BERT</code> ë“±ë„ ë°©ë²•ì€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤)</li><li><code>transformers v2.8.0</code>ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ë‹¤. ì´í›„ ë²„ì „ì—ì„œ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆë‹¤.</li><li><code>Transformers</code>ì˜ <code>ELECTRA</code>ëŠ” <code>discriminator</code>ì™€ <code>generator</code>ë¥¼ <strong>ê°ê° ë”°ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤!</strong></li></ul><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><h3 id="1-Original-Tensorflow-Checkpoint"><a href="#1-Original-Tensorflow-Checkpoint" class="headerlink" title="1. Original Tensorflow Checkpoint"></a>1. Original Tensorflow Checkpoint</h3><p>ë‹¹ì—°íˆ <strong>Tensorflowë¡œ í•™ìŠµí•œ ê²°ê³¼ë¬¼</strong>ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•œë‹¤.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">â”œâ”€â”€ koelectra-small-tf</span><br><span class="line">â”‚   â”œâ”€â”€ checkpoint</span><br><span class="line">â”‚   â”œâ”€â”€ events.out.tfevents.1586942968.koelectra-small</span><br><span class="line">â”‚   â”œâ”€â”€ graph.pbtxt</span><br><span class="line">â”‚   â”œâ”€â”€ ...</span><br><span class="line">â”‚   â”œâ”€â”€ model.ckpt-700000.data-00000-of-00001</span><br><span class="line">â”‚   â”œâ”€â”€ model.ckpt-700000.index</span><br><span class="line">â”‚   â””â”€â”€ model.ckpt-700000.meta</span><br><span class="line">â””â”€â”€ ...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>checkpoint</span></figcaption><table><tr><td class="code"><pre><span class="line">model_checkpoint_path: &quot;model.ckpt-700000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-600000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-625000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-650000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-675000&quot;</span><br><span class="line">all_model_checkpoint_paths: &quot;model.ckpt-700000&quot;</span><br></pre></td></tr></table></figure><p>ì£¼ì˜í•  ì ì€ <code>checkpoint</code> íŒŒì¼ì—ì„œ <code>model_checkpoint_path</code> ê°’ì„ <strong>â€œì›í•˜ëŠ” stepì˜ ckptâ€</strong>ë¡œ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.</p><h3 id="2-config-json"><a href="#2-config-json" class="headerlink" title="2. config.json"></a>2. config.json</h3><blockquote><p><strong>(ì£¼ì˜!) <code>transformers</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—…ë°ì´íŠ¸ë˜ë©´ì„œ APIê°€ ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìˆê³ , ì´ì— ë”°ë¼ <code>config.json</code>ì˜ attributeê°€ ì¶”ê°€/ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.</strong></p></blockquote><blockquote><p><a href="https://huggingface.co/models" rel="external nofollow noopener noreferrer" target="_blank">https://huggingface.co/models</a>ë¡œ ê°€ì„œ ëŒ€í‘œ ëª¨ë¸ì˜ <code>config.json</code>ì„ ë³´ë©´ì„œ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•œë‹¤.</p></blockquote><ul><li><code>vocab_size</code> ë³€ê²½ì—ë§Œ ì£¼ì˜í•˜ë©´ ì¶©ë¶„í•¨</li><li>ë§Œì¼ <code>max_seq_length</code>ë¥¼ ë°”ê¿¨ë‹¤ë©´ <code>max_position_embeddings</code>ë„ ë°”ê¿”ì•¼ í•¨</li></ul><figure class="highlight json"><figcaption><span>discriminator</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"architectures"</span>: [<span class="string">"ElectraForPreTraining"</span>],</span><br><span class="line">  <span class="attr">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"embedding_size"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="attr">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"hidden_size"</span>: <span class="number">256</span>,</span><br><span class="line">  <span class="attr">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="attr">"intermediate_size"</span>: <span class="number">1024</span>,</span><br><span class="line">  <span class="attr">"layer_norm_eps"</span>: <span class="number">1e-12</span>,</span><br><span class="line">  <span class="attr">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"model_type"</span>: <span class="string">"electra"</span>,</span><br><span class="line">  <span class="attr">"num_attention_heads"</span>: <span class="number">4</span>,</span><br><span class="line">  <span class="attr">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="attr">"pad_token_id"</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">32200</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><figcaption><span>generator</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"architectures"</span>: [<span class="string">"ElectraForMaskedLM"</span>],</span><br><span class="line">  <span class="attr">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"embedding_size"</span>: <span class="number">128</span>,</span><br><span class="line">  <span class="attr">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="attr">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"hidden_size"</span>: <span class="number">256</span>,</span><br><span class="line">  <span class="attr">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="attr">"intermediate_size"</span>: <span class="number">1024</span>,</span><br><span class="line">  <span class="attr">"layer_norm_eps"</span>: <span class="number">1e-12</span>,</span><br><span class="line">  <span class="attr">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"model_type"</span>: <span class="string">"electra"</span>,</span><br><span class="line">  <span class="attr">"num_attention_heads"</span>: <span class="number">4</span>,</span><br><span class="line">  <span class="attr">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="attr">"pad_token_id"</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">32200</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-tokenizer-config-json"><a href="#3-tokenizer-config-json" class="headerlink" title="3. tokenizer_config.json"></a>3. tokenizer_config.json</h3><p><code>cased</code> ëª¨ë¸ì˜ ê²½ìš° ê·¸ëƒ¥ tokenizerë¥¼ loadí•˜ë©´ ë§¤ë²ˆ <code>do_lower_case=False</code>ë¥¼ ì§ì ‘ ì¶”ê°€í•´ì¤˜ì•¼ í•œë‹¤.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ElectraTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>,</span><br><span class="line">                                             do_lower_case=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p><code>tokenizer_config.json</code>ì„ ë§Œë“¤ì–´ì£¼ë©´ ì´ëŸ¬í•œ ë²ˆê±°ë¡œì›€ì„ ì—†ì•¨ ìˆ˜ ìˆë‹¤.<br>(ë§Œì¼ <code>max_seq_length</code>ê°€ 128ì´ë©´ <code>model_max_length</code>ë„ 128ë¡œ ë°”ê¿”ì£¼ë©´ ëœë‹¤.)</p><figure class="highlight json"><figcaption><span>tokenizer_config.json</span></figcaption><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"do_lower_case"</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">"model_max_length"</span>: <span class="number">512</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-vocab-txt"><a href="#4-vocab-txt" class="headerlink" title="4. vocab.txt"></a>4. vocab.txt</h3><p>tensorflowì—ì„œ í•™ìŠµí–ˆì„ ë•Œ ì“´ <code>vocab.txt</code>ë¥¼ ê·¸ëŒ€ë¡œ ì“°ë©´ ëœë‹¤.</p><h3 id="5-ìµœì¢…ì ì¸-ë””ë ‰í† ë¦¬-í˜•íƒœ"><a href="#5-ìµœì¢…ì ì¸-ë””ë ‰í† ë¦¬-í˜•íƒœ" class="headerlink" title="5. ìµœì¢…ì ì¸ ë””ë ‰í† ë¦¬ í˜•íƒœ"></a>5. ìµœì¢…ì ì¸ ë””ë ‰í† ë¦¬ í˜•íƒœ</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">â”œâ”€â”€ koelectra-small-tf</span><br><span class="line">â”‚   â”œâ”€â”€ checkpoint</span><br><span class="line">â”‚   â”œâ”€â”€ events.out.tfevents.1586942968.koelectra-small</span><br><span class="line">â”‚   â”œâ”€â”€ graph.pbtxt</span><br><span class="line">â”‚   â”œâ”€â”€ ...</span><br><span class="line">â”‚   â”œâ”€â”€ model.ckpt-700000.data-00000-of-00001</span><br><span class="line">â”‚   â”œâ”€â”€ model.ckpt-700000.index</span><br><span class="line">â”‚   â””â”€â”€ model.ckpt-700000.meta</span><br><span class="line">â”‚</span><br><span class="line">â”œâ”€â”€ electra-small-discriminator</span><br><span class="line">â”‚   â”œâ”€â”€ config.json</span><br><span class="line">â”‚   â”œâ”€â”€ tokenizer_config.json</span><br><span class="line">â”‚   â””â”€â”€ vocab.txt</span><br><span class="line">â”‚</span><br><span class="line">â”œâ”€â”€ electra-small-generator</span><br><span class="line">â”‚   â”œâ”€â”€ config.json</span><br><span class="line">â”‚   â”œâ”€â”€ tokenizer_config.json</span><br><span class="line">â”‚   â””â”€â”€ vocab.txt</span><br><span class="line">â””â”€â”€ ...</span><br></pre></td></tr></table></figure><ul><li><code>electra-small-discriminator</code>ì™€ <code>electra-small-generator</code> í´ë”ë¥¼ ê°ê° ë§Œë“ ë‹¤.</li><li><code>config.json</code>ì€ discriminatorìš©ê³¼ generatorìš©ì„ <strong>ë”°ë¡œ</strong> ë§Œë“¤ì–´ì„œ í´ë” ì•ˆì— ë„£ëŠ”ë‹¤.</li><li><code>tokenizer_config.json</code>ê³¼ <code>vocab.txt</code>ëŠ” discriminatorì™€ generator ë‘˜ ë‹¤ <strong>ë™ì¼í•œ íŒŒì¼</strong>ì„ ë„£ìœ¼ë©´ ëœë‹¤.</li></ul><h2 id="Convert"><a href="#Convert" class="headerlink" title="Convert"></a>Convert</h2><figure class="highlight python"><figcaption><span>convert.py</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> transformers.convert_electra_original_tf_checkpoint_to_pytorch <span class="keyword">import</span> convert_tf_checkpoint_to_pytorch</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--tf_ckpt_path"</span>, type=str, default=<span class="string">"koelectra-small-tf"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--pt_discriminator_path"</span>, type=str, default=<span class="string">"koelectra-small-discriminator"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--pt_generator_path"</span>, type=str, default=<span class="string">"koelectra-small-generator"</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path,</span><br><span class="line">                                 config_file=os.path.join(args.pt_discriminator_path, <span class="string">"config.json"</span>),</span><br><span class="line">                                 pytorch_dump_path=os.path.join(args.pt_discriminator_path, <span class="string">"pytorch_model.bin"</span>),</span><br><span class="line">                                 discriminator_or_generator=<span class="string">"discriminator"</span>)</span><br><span class="line"></span><br><span class="line">convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path,</span><br><span class="line">                                 config_file=os.path.join(args.pt_generator_path, <span class="string">"config.json"</span>),</span><br><span class="line">                                 pytorch_dump_path=os.path.join(args.pt_generator_path, <span class="string">"pytorch_model.bin"</span>),</span><br><span class="line">                                 discriminator_or_generator=<span class="string">"generator"</span>)</span><br></pre></td></tr></table></figure><h2 id="Upload-your-model-to-Huggingface-s3"><a href="#Upload-your-model-to-Huggingface-s3" class="headerlink" title="Upload your model to Huggingface s3"></a>Upload your model to Huggingface s3</h2><ol><li>ë¨¼ì € <a href="https://huggingface.co/" rel="external nofollow noopener noreferrer" target="_blank">huggingface.co</a>ë¡œ ê°€ì„œ <strong>íšŒì›ê°€ì…</strong>ì„ í•´ì•¼ í•¨</li><li>ì•„ë˜ì˜ ëª…ë ¹ì–´ë¡œ s3ì— ì—…ë¡œë“œ ì§„í–‰</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ transformers-cli login</span><br><span class="line">$ transformers-cli upload koelectra-small-discriminator</span><br><span class="line">$ transformers-cli upload koelectra-small-generator</span><br></pre></td></tr></table></figure><h2 id="Now-Letâ€™s-Use-It"><a href="#Now-Letâ€™s-Use-It" class="headerlink" title="Now Letâ€™s Use It"></a>Now Letâ€™s Use It</h2><figure class="highlight python"><figcaption><span>How to Use</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> ElectraModel, ElectraTokenizer</span><br><span class="line"></span><br><span class="line">model = ElectraModel.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br><span class="line">tokenizer = ElectraTokenizer.from_pretrained(<span class="string">"monologg/koelectra-small-discriminator"</span>)</span><br></pre></td></tr></table></figure><h2 id="ë§ºìœ¼ë©°"><a href="#ë§ºìœ¼ë©°" class="headerlink" title="ë§ºìœ¼ë©°"></a>ë§ºìœ¼ë©°</h2><center><p><img src="/images/2020-05-01-transformers-porting/model_download.png" alt="ìƒê°ë³´ë‹¤ ë§ì´ ì‚¬ìš©í•˜ì‹œë„¤..."></p></center><p>ì‚¬ì‹¤ Model Portingê³¼ ê´€ë ¨í•˜ì—¬ <strong>ëª…í™•í•œ Documentationì´ ì—†ì–´</strong> ë‚˜ë„ ì‚½ì§ˆì„ ìƒë‹¹íˆ í–ˆë˜ ë¶€ë¶„ì´ë‹¤. ì´ë²ˆ ë‚´ìš©ì´ ë‹¤ë¥¸ ë¶„ë“¤ì—ê²Œ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ í•œë‹¤ğŸ˜›</p><p>ë˜í•œ <strong>Huggingface s3ì— ëª¨ë¸ì„ ì—…ë¡œë“œ</strong>í•˜ëŠ” ê²ƒì€ ê¼­ ì‚¬ìš©í•´ë³´ê¸¸ ê¶Œí•œë‹¤. ì´ ê¸°ëŠ¥ì´ ìƒê¸´ì§€ ì–¼ë§ˆë˜ì§€ ì•Šì•„ì„œ ëª¨ë¥´ì‹œëŠ” ë¶„ë“¤ì´ ìˆëŠ”ë°, <strong>ì—…ë¡œë“œ ìš©ëŸ‰ì˜ ì œí•œë„ ì—†ê³ </strong> ì—¬ëŸ¬ëª¨ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë„ í¸í•´ì§„ë‹¤. (ëª¨ë¸ì„ 100ê°œ ì´ìƒ ì˜¬ë¦°ë‹¤ê³  Huggingface íŒ€ì—ì„œ ë­ë¼ê³  í•˜ì§€ ì•Šìœ¼ë‹ˆ ë§ì´ë“¤ ì“°ì…¨ìœ¼ë©´ã…ã…)</p>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/05/01/transformers-porting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°</title>
      <link>https://monologg.kr/2020/04/27/wordpiece-vocab/</link>
      <guid>https://monologg.kr/2020/04/27/wordpiece-vocab/</guid>
      <pubDate>Sun, 26 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;ê°œì¸ì ìœ¼ë¡œ Pretrained Language Model ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ë¡œ &lt;code&gt;Vocab quality&lt;/code&gt;ë¼ê³  ìƒê°í•œë‹¤.&lt;/p&gt;
&lt;p&gt;ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” tokenizationì˜ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ &lt;code&gt;Wordpiece&lt;/code&gt;ë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ vocabì„ ë§Œë“œëŠ”ì§€ ì•Œì•„ë³´ë ¤ í•œë‹¤:)&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>ê°œì¸ì ìœ¼ë¡œ Pretrained Language Model ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ë¡œ <code>Vocab quality</code>ë¼ê³  ìƒê°í•œë‹¤.</p><p>ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” tokenizationì˜ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ <code>Wordpiece</code>ë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ vocabì„ ë§Œë“œëŠ”ì§€ ì•Œì•„ë³´ë ¤ í•œë‹¤:)</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>í•œêµ­ì–´ Tokenizerì˜ ëŒ€ì•ˆìœ¼ë¡œëŠ” í¬ê²Œ <code>Sentencepiece</code>, <code>Mecab</code>, <code>Wordpiece</code>ê°€ ìˆë‹¤. (<em>ì—¬ê¸°ì„œì˜ wordpieceëŠ” Googleì˜ BERTì—ì„œ ì‚¬ìš©ëœ wordpieceë¡œ ê°€ì •í•œë‹¤.</em>)</p><p>BERT, ELECTRA ë“±ì€ ê¸°ë³¸ì ìœ¼ë¡œ <code>Wordpiece</code>ë¥¼ ì‚¬ìš©í•˜ê¸°ì— ê³µì‹ ì½”ë“œì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µë˜ëŠ” Tokenizer ì—­ì‹œ ì´ì— í˜¸í™˜ë˜ê²Œ ì½”ë“œê°€ ì‘ì„±ë˜ì—ˆë‹¤. ì¦‰, <code>Sentencepiece</code>ë‚˜ <code>Mecab</code>ì„ ì‚¬ìš©í•˜ë ¤ë©´ <strong>ë³„ë„ì˜ Tokenizer</strong>ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•˜ê³ , ì´ë ‡ê²Œ ë˜ë©´ <code>transformers</code> ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª¨ë¸ì„ ê³§ë°”ë¡œ ì‚¬ìš©í•˜ëŠ”ë° ë¶ˆí¸í•¨ì´ ìƒê¸°ê²Œ ëœë‹¤.</p><h2 id="Original-wordpiece-code-is-NOT-available"><a href="#Original-wordpiece-code-is-NOT-available" class="headerlink" title="Original wordpiece code is NOT available!"></a>Original wordpiece code is NOT available!</h2><p float="left" align="left">    <img width="800" src="https://user-images.githubusercontent.com/28896432/80015023-19f7e680-850c-11ea-90d3-436ca253a7a1.png">  </p><p><strong>ê³µì‹ BERTì—ì„œ ì‚¬ìš©ëœ Wordpiece BuilderëŠ” ì œê³µë˜ì§€ ì•Šê³  ìˆë‹¤</strong>. BERT ê³µì‹ Githubì—ì„œ ë‹¤ë¥¸ ëŒ€ì•ˆë“¤ì„ ì œì‹œí•´ì¤¬ì§€ë§Œ, ì™„ì „íˆ ë™ì¼í•œ Wordpiece Vocabì´ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤.</p><p>ëª‡ëª‡ ì˜¤í”ˆì†ŒìŠ¤ë“¤ì´ Wordpiece vocab builderë¥¼ êµ¬í˜„í•˜ì˜€ì§€ë§Œ <strong>input fileì´ ë§¤ìš° í´ ì‹œ ë©”ëª¨ë¦¬, ì†ë„ ë“±ì˜ ì´ìŠˆ</strong>ê°€ ì¢…ì¢… ë°œìƒí•œë‹¤ã… </p><h2 id="Huggingface-Tokenizers"><a href="#Huggingface-Tokenizers" class="headerlink" title="Huggingface Tokenizers"></a>Huggingface Tokenizers</h2><p float="left" align="center">    <img width="600" src="https://user-images.githubusercontent.com/28896432/80016455-1c5b4000-850e-11ea-8432-3c356c11f932.png">  </p><p>ìµœì¢…ì ìœ¼ë¡œ, ìµœê·¼ Huggingfaceì—ì„œ ë°œí‘œí•œ <code>Tokenizers</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ Wordpiece Vocabularyë¥¼ ë§Œë“œëŠ”ê²Œ ì œì¼ ì¢‹ì•˜ë‹¤.</p><p>í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ Corpusê°€ ë§¤ìš° ì»¤ë„ ë©”ëª¨ë¦¬ ì´ìŠˆê°€ ë°œìƒí•˜ì§€ ì•Šìœ¼ë©°, <code>Rust</code>ë¡œ êµ¬í˜„ì´ ë˜ì–´ìˆì–´ ì†ë„ ë˜í•œ Pythonë³´ë‹¤ ë¹ ë¥´ë‹¤ğŸ˜ƒ</p><h2 id="Code-for-building-Wordpiece-vocab"><a href="#Code-for-building-Wordpiece-vocab" class="headerlink" title="Code for building Wordpiece vocab"></a>Code for building Wordpiece vocab</h2><p><strong>(tokenizer v0.7.0 ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ë‹¤. í˜„ì¬ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—…ë°ì´íŠ¸ ì¤‘ì´ì–´ì„œ apiê°€ ë‹¬ë¼ì§ˆ ìˆ˜ë„â€¦)</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> BertWordPieceTokenizer</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--corpus_file"</span>, type=str)</span><br><span class="line">parser.add_argument(<span class="string">"--vocab_size"</span>, type=int, default=<span class="number">32000</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--limit_alphabet"</span>, type=int, default=<span class="number">6000</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">tokenizer = BertWordPieceTokenizer(</span><br><span class="line">    vocab_file=<span class="literal">None</span>,</span><br><span class="line">    clean_text=<span class="literal">True</span>,</span><br><span class="line">    handle_chinese_chars=<span class="literal">True</span>,</span><br><span class="line">    strip_accents=<span class="literal">False</span>, <span class="comment"># Must be False if cased model</span></span><br><span class="line">    lowercase=<span class="literal">False</span>,</span><br><span class="line">    wordpieces_prefix=<span class="string">"##"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.train(</span><br><span class="line">    files=[args.corpus_file],</span><br><span class="line">    limit_alphabet=args.limit_alphabet,</span><br><span class="line">    vocab_size=args.vocab_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.save(<span class="string">"./"</span>, <span class="string">"ch-&#123;&#125;-wpm-&#123;&#125;"</span>.format(args.limit_alphabet, args.vocab_size))</span><br></pre></td></tr></table></figure><ul><li><p>ì£¼ì˜í•´ì•¼í•  ì ì€ <code>lowercase=False</code>ë¡œ í•  ì‹œ <code>strip_accent=False</code>ë¡œ í•´ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒ!</p></li><li><p><code>[UNK]</code>ì˜ ë¹„ì¤‘ì„ ìµœëŒ€í•œ ì¤„ì´ê¸° ìœ„í•´ <strong>ëª¨ë“  characterë¥¼ ì»¤ë²„</strong>í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬í•˜ì˜€ë‹¤. (<code>limit_alphabet</code>)</p></li><li><p>Corpusì˜ ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆë‹¤ëŠ” ì „ì œí•˜ì— sentencepieceì™€ ë¹„êµí–ˆì„ ë•Œ <strong>UNK Ratioê°€ í›¨ì”¬ ë‚®ì•˜ë‹¤.</strong></p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://wikidocs.net/22592" rel="external nofollow noopener noreferrer" target="_blank">Sentencepiece vs Wordpiece</a></li><li><a href="https://github.com/google-research/bert#learning-a-new-wordpiece-vocabulary" rel="external nofollow noopener noreferrer" target="_blank">Learning a new WordPiece vocabulary</a></li><li><a href="https://github.com/kwonmha/bert-vocab-builder" rel="external nofollow noopener noreferrer" target="_blank">kwonmhaâ€™s bert-vocab-builder</a></li><li><a href="https://github.com/huggingface/tokenizers" rel="external nofollow noopener noreferrer" target="_blank">Huggingface Tokenizers</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/04/27/wordpiece-vocab/#disqus_thread</comments>
    </item>
    
    <item>
      <title>TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸°</title>
      <link>https://monologg.kr/2020/04/20/tpu-electra/</link>
      <guid>https://monologg.kr/2020/04/20/tpu-electra/</guid>
      <pubDate>Sun, 19 Apr 2020 15:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;ìµœê·¼ &lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;ELECTRA&lt;/a&gt;ì˜ ê³µì‹ ì½”ë“œê°€ ê³µê°œë˜ë©´ì„œ í•œêµ­ì–´ Corpusì— ì§ì ‘ Electraë¥¼ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤.&lt;/p&gt;
&lt;p&gt;ì´ë²ˆ ê¸€ì—ì„œëŠ” GCPì—ì„œ TPUë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í–ˆëŠ”ì§€ ê·¸ ê³¼ì •ì„ ê³µìœ í•´ë³´ë ¤ í•œë‹¤.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>ìµœê·¼ <a href="https://openreview.net/forum?id=r1xMH1BtvB" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA</a>ì˜ ê³µì‹ ì½”ë“œê°€ ê³µê°œë˜ë©´ì„œ í•œêµ­ì–´ Corpusì— ì§ì ‘ Electraë¥¼ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤.</p><p>ì´ë²ˆ ê¸€ì—ì„œëŠ” GCPì—ì„œ TPUë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í–ˆëŠ”ì§€ ê·¸ ê³¼ì •ì„ ê³µìœ í•´ë³´ë ¤ í•œë‹¤.</p><a id="more"></a><h2 id="Tensorflow-Research-Cloud-ì‹ ì²­"><a href="#Tensorflow-Research-Cloud-ì‹ ì²­" class="headerlink" title="Tensorflow Research Cloud ì‹ ì²­"></a>Tensorflow Research Cloud ì‹ ì²­</h2><p>Tensorflow Research Cloud (TFRC)ëŠ” <strong>1ë‹¬ ë™ì•ˆ TPUë¥¼ ë¬´ë£Œ</strong>ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”„ë¡œê·¸ë¨ì´ë‹¤.</p><p>í•´ë‹¹ <a href="https://www.tensorflow.org/tfrc?hl=ko" rel="external nofollow noopener noreferrer" target="_blank">ë§í¬</a>ë¡œ ê°€ì„œ ì‹ ì²­ì„ í•˜ê²Œ ë˜ë©´ ë©”ì¼ì´ í•˜ë‚˜ ì˜¤ê²Œ ëœë‹¤.</p><p><img src="https://user-images.githubusercontent.com/28896432/79709907-61a92300-82fe-11ea-9773-9ac63b5ebbb6.png" alt></p><p>í•´ë‹¹ ë©”ì¼ì—ì„œ ìš”êµ¬í•˜ëŠ” ëŒ€ë¡œ ì‹ ì²­ì„œë¥¼ ì¶”ê°€ë¡œ ì‘ì„±í•œ í›„ ì œì¶œí•˜ë©´ ì–¼ë§ˆ í›„ ì•„ë˜ì™€ ê°™ì´ ë‹µì¥ì´ ì˜¤ê²Œ ë˜ê³ , ê·¸ ë•Œë¶€í„° GCPì—ì„œ TPUë¥¼ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤:)</p><p><img src="https://user-images.githubusercontent.com/28896432/79709997-9ddc8380-82fe-11ea-9040-06d8ef9c1f1b.png" alt></p><h2 id="Bucketì—-Data-ì—…ë¡œë“œ"><a href="#Bucketì—-Data-ì—…ë¡œë“œ" class="headerlink" title="Bucketì— Data ì—…ë¡œë“œ"></a>Bucketì— Data ì—…ë¡œë“œ</h2><p>TPUë¥¼ ì“°ëŠ” ê²½ìš° ëª¨ë“  input fileì„ <strong>Cloud storage bucketì„ í†µí•´ì•¼ë§Œ í•œë‹¤.</strong> (<a href="https://cloud.google.com/tpu/docs/troubleshooting?hl=ko#common-errors" rel="external nofollow noopener noreferrer" target="_blank">ê´€ë ¨ FAQ</a>)</p><h3 id="Bucket-ìƒì„±"><a href="#Bucket-ìƒì„±" class="headerlink" title="Bucket ìƒì„±"></a>Bucket ìƒì„±</h3><ul><li><p>ì˜ˆì œìƒ Bucketì˜ ì´ë¦„ì„ <code>test-for-electra</code>ë¡œ ë§Œë“¤ì–´ ë³´ê² ë‹¤.</p></li><li><p>GCP ë©”ì¸ í˜ì´ì§€ ì¢Œì¸¡ì˜ <code>[Storage]</code> - <code>[ë¸Œë¼ìš°ì €]</code> ë¡œ ì´ë™</p></li><li><p><code>ë²„í‚· ë§Œë“¤ê¸°</code> í´ë¦­</p></li><li><p><strong>ì‚¬ìš©í•  TPUì™€ ë™ì¼í•œ Region</strong>ì— Bucket ë§Œë“œëŠ” ê²ƒì„ ê¶Œì¥</p><p><img src="https://user-images.githubusercontent.com/28896432/79711012-a84c4c80-8301-11ea-955c-39dc604f5c10.png" alt></p></li></ul><h3 id="File-Upload"><a href="#File-Upload" class="headerlink" title="File Upload"></a>File Upload</h3><ul><li><p>ì¤€ë¹„í•œ <code>pretrain_tfrecords</code>ì™€ <code>vocab.txt</code>ë¥¼ Bucketì— ì—…ë¡œë“œ</p><p><img src="https://user-images.githubusercontent.com/28896432/79739355-0a747400-8339-11ea-8de2-f78f8ade887f.png" alt></p></li></ul><h2 id="GCP-VM-amp-TPU-ìƒì„±"><a href="#GCP-VM-amp-TPU-ìƒì„±" class="headerlink" title="GCP VM &amp; TPU ìƒì„±"></a>GCP VM &amp; TPU ìƒì„±</h2><ul><li>VMê³¼ TPUë¥¼ ê°ê° ë”°ë¡œ ë§Œë“œëŠ” ê²ƒë³´ë‹¤, ìš°ì¸¡ ìƒë‹¨ì˜ <code>cloud shell</code>ì„ ì—´ì–´ ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤.</li><li>ì €ì¥ì†ŒëŠ” Bucketì´, ì—°ì‚°ì€ TPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— <strong>VM InstanceëŠ” ê°€ë²¼ìš´ ê²ƒì„ ì¨ë„ ìƒê´€ì´ ì—†ë‹¤.</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ctpu up --zone=europe-west4<span class="_">-a</span> --tf-version=1.15 \</span><br><span class="line">          --tpu-size=v3-8 --machine-type=n1-standard-1 \</span><br><span class="line">          --disk-size-gb=20 --name=&#123;<span class="variable">$VM_NAME</span>&#125;</span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/28896432/79740137-24fb1d00-833a-11ea-9be8-e317521fa178.png" alt></p><h2 id="Electra-í•™ìŠµ-ì§„í–‰"><a href="#Electra-í•™ìŠµ-ì§„í–‰" class="headerlink" title="Electra í•™ìŠµ ì§„í–‰"></a>Electra í•™ìŠµ ì§„í–‰</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/google-research/electra</span><br><span class="line">$ <span class="built_in">cd</span> electra</span><br><span class="line">$ python3 run_pretraining.py --data-dir gs://&#123;<span class="variable">$BUCKET_NAME</span>&#125; \</span><br><span class="line">                             --model-name &#123;<span class="variable">$MODEL_NAME</span>&#125; \</span><br><span class="line">                             --hparams &#123;<span class="variable">$CONFIG_PATH</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="í•™ìŠµ-ì™„ë£Œ-í›„-Instance-Bucket-ì‚­ì œ"><a href="#í•™ìŠµ-ì™„ë£Œ-í›„-Instance-Bucket-ì‚­ì œ" class="headerlink" title="í•™ìŠµ ì™„ë£Œ í›„ Instance, Bucket ì‚­ì œ"></a>í•™ìŠµ ì™„ë£Œ í›„ Instance, Bucket ì‚­ì œ</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ctpu delete --zone=europe-west4<span class="_">-a</span> --name=&#123;<span class="variable">$VM_NAME</span>&#125;</span><br><span class="line">$ gsutil rm -r gs://<span class="built_in">test</span>-for-electra</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/google-research/electra" rel="external nofollow noopener noreferrer" target="_blank">ELECTRA official github</a></li><li><a href="https://github.com/pren1/A_Pipeline_Of_Pretraining_Bert_On_Google_TPU" rel="external nofollow noopener noreferrer" target="_blank">A Pipeline Of Pretraining Bert On Google TPU</a></li><li><a href="https://cloud.google.com/tpu/docs" rel="external nofollow noopener noreferrer" target="_blank">Official TPU Documentation</a></li></ul>]]></content:encoded>
      
      <comments>https://monologg.kr/2020/04/20/tpu-electra/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
