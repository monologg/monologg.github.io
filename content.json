{"pages":[],"posts":[{"title":"TPU를 이용하여 Electra Pretraining하기","text":"최근 ELECTRA의 공식 코드가 공개되면서 한국어 Corpus에 직접 Electra를 만들게 되었습니다. 이번 글에서는 GCP에서 TPU를 어떻게 사용했는지 그 과정을 공유해보려 합니다. Tensorflow Research Cloud 신청Tensorflow Research Cloud (TFRC)는 1달 동안 TPU를 무료로 사용할 수 있게 해주는 프로그램입니다. 해당 링크로 가서 신청을 하게 되면 메일이 하나 오게 됩니다. 해당 메일에서 요구하는 대로 신청서를 추가로 작성한 후 제출하면 얼마 후 아래와 같이 답장이 오게 되고, 그 때부터 GCP에서 TPU를 무료로 사용할 수 있게 됩니다:) Bucket에 Data 업로드TPU를 쓰는 경우 모든 input file을 Cloud storage bucket을 통해야만 합니다. (관련 FAQ) Bucket 생성 Bucket의 이름을 test-for-electra로 만들어 보겠습니다. GCP 메인 페이지 좌측의 [Storage] - [브라우저] 로 이동 버킷 만들기 클릭 사용할 TPU와 동일한 Region에 Bucket 만드는 것을 권장 File Upload 준비한 pretrain_tfrecords와 vocab.txt를 Bucket에 업로드 GCP VM &amp; TPU 생성 VM과 TPU를 각각 따로 만드는 것보다, 우측 상단의 cloud shell을 열어 아래의 명령어를 입력하는 것을 추천합니다. 저장소는 Bucket이, 연산은 TPU에서 처리하기 때문에 VM Instance는 가벼운 것을 써도 상관이 없습니다. $ ctpu up --zone=europe-west4-a --tf-version=1.15 \\ --tpu-size=v3-8 --machine-type=n1-standard-2 \\ --disk-size-gb=20 --name={$VM_NAME} Electra 학습 진행$ git clone https://github.com/google-research/electra$ cd electra$ python3 run_pretraining.py --data-dir gs://{$BUCKET_NAME} \\ --model-name {$MODEL_NAME} \\ --hparams {$CONFIG_PATH} 학습 완료 후 Instance, Bucket 삭제$ ctpu delete --zone=europe-west4-a --name={$VM_NAME}$ gsutil rm -r gs://test-for-electra Reference electra A Pipeline Of Pretraining Bert On Google TPU Official TPU Documentation","link":"/2020/04/20/tpu-electra/"},{"title":"나만의 BERT Wordpiece Vocab 만들기","text":"개인적으로 Pretrained Language Model 성능에 큰 영향을 주는 것 중 하나로 Vocab quality라고 생각합니다. 이번 포스트에서는 tokenization의 방법 중 하나인 Wordpiece를 이용하여 어떻게 vocab을 만드는지 알아보겠습니다:) Introduction한국어 Tokenizer의 대안으로는 크게 Sentencepiece, Mecab, Wordpiece가 있습니다. (여기서의 wordpiece는 Google의 BERT에서 사용된 wordpiece로 가정하겠습니다.) BERT, ELECTRA 등은 기본적으로 Wordpiece를 사용하기에 공식 코드에서 기본적으로 제공되는 Tokenizer 역시 이에 호환되게 코드가 작성되었습니다. 즉, Sentencepiece나 Mecab을 사용하려면 별도의 Tokenizer를 직접 만들어야 하고, 이렇게 되면 transformers 등의 라이브러리에서 모델을 곧바로 사용하는데 불편함이 생깁니다. Original wordpiece code is NOT available! 공식 BERT에서 사용된 Wordpiece Builder는 제공되지 않고 있습니다. BERT 공식 Github에서 다른 대안들을 제시해줬지만, 완전히 동일한 Wordpiece Vocab이 나오지 않습니다. 몇몇 오픈소스들이 Wordpiece vocab builder 구현을 구현하였지만 input_file이 매우 클 시 메모리, 속도 등의 이슈가 종종 발생하였습니다. Huggingface Tokenizers 최종적으로, 최근 Huggingface에서 발표한 Tokenizers 라이브러리를 이용하여 Wordpiece Vocabulary를 만드는게 제일 좋았습니다. 해당 라이브러리를 사용하면 Corpus가 매우 커도 메모리 이슈가 발생하지 않으며, Rust로 구현이 되어있어 속도 또한 Python보다 빠릅니다. Code for building Wordpiece vocab(tokenizer v0.7.0 기준으로 작성하였습니다. 현재도 라이브러리가 업데이트 중이어서 api가 달라질 수 있습니다.) import osimport argparsefrom tokenizers import BertWordPieceTokenizerparser = argparse.ArgumentParser()parser.add_argument(\"--corpus_file\", type=str)parser.add_argument(\"--vocab_size\", type=int, default=32000)parser.add_argument(\"--limit_alphabet\", type=int, default=6000)args = parser.parse_args()tokenizer = BertWordPieceTokenizer( vocab_file=None, clean_text=True, handle_chinese_chars=True, strip_accents=False, # Must be False if cased model lowercase=False, wordpieces_prefix=\"##\")tokenizer.train( files=[args.corpus_file], limit_alphabet=args.limit_alphabet, vocab_size=args.vocab_size)tokenizer.save(\"./\", \"ch-{}-wpm-{}\".format(args.limit_alphabet, args.vocab_size)) 주의해야할 점은 lowercase=False로 할 시 strip_accent=False로 해줘야 한다는 것입니다. [UNK]의 비중을 최대한 줄이기 위해 모든 character를 커버할 수 있도록 처리하였습니다. (limit_alphabet) Corpus의 전처리가 완료되었다는 전제하에 sentencepiece와 비교했을 때 UNK Ratio가 훨씬 낮았습니다. Reference Sentencepiece vs Wordpiece Learning a new WordPiece vocabulary kwonmha’s bert-vocab-builder Huggingface Tokenizers","link":"/2020/04/27/wordpiece-vocab/"}],"tags":[{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"tpu","slug":"tpu","link":"/tags/tpu/"},{"name":"electra","slug":"electra","link":"/tags/electra/"},{"name":"wordpiece","slug":"wordpiece","link":"/tags/wordpiece/"},{"name":"tokenization","slug":"tokenization","link":"/tags/tokenization/"}],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"TPU","slug":"NLP/TPU","link":"/categories/NLP/TPU/"},{"name":"Wordpiece","slug":"NLP/Wordpiece","link":"/categories/NLP/Wordpiece/"}]}