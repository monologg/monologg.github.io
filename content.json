{"pages":[{"title":"ë°•ì¥ì› (monologg)","text":"Github | LinkedIn | Scholar ì•ˆë…•í•˜ì„¸ìš” NLP Engineerë¡œ í™œë™í•˜ê³  ìˆëŠ” ë°•ì¥ì›ì…ë‹ˆë‹¤:) ì›ë˜ ì œ ì „ê³µì€ ê²½ì˜í•™ê³¼ì…ë‹ˆë‹¤. ìš°ì—°í•œ ê³„ê¸°ë¡œ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì ‘í•˜ê²Œ ë˜ì–´ í˜„ì¬ëŠ” NLP Engineerë¡œ ì‚´ì•„ê°€ê³  ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ NLPì— ë” ë§ì€ ê¸°ì—¬ë¥¼ í•˜ê¸° ìœ„í•´ ì˜¤ëŠ˜ë„ ë‹¬ë¦½ë‹ˆë‹¤!! â€œë³€í™”ë¥¼ ë‘ë ¤ì›Œí•˜ëŠ” ì‚¬ëŒì´ ê°€ì¥ ë¶ˆí–‰í•œ ì‚¬ëŒì´ë‹¤.â€ - Mignon McLaughlin Career &amp; EducationBHSN - NLP Engineer (2022.12 ~ ) KB AI Research - NLP Engineer (2020.12 ~ 2022.10) Mathpresso - NLP Engineer (2020.06 ~ 2020.11) Samsung Research - NLP Engineer (2019.02 ~ 2020.01) Yonsei University - Business &amp; Computer Science (2011.03 ~ 2019.02) Projects KoBigBird - í•œêµ­ì–´ Corpusë¡œ í•™ìŠµí•œ BigBird ëª¨ë¸ KoELECTRA - í•œêµ­ì–´ Corpusë¡œ í•™ìŠµí•œ ELECTRA ëª¨ë¸ DistilKoBERT - SKT KoBERTì— Distillation ì ìš© Publications KLUE: Korean Language Understanding EvaluationS. Park*, J. Moon*, S. Kim*, W. Cho*, J. Han, J. Park, C. Song, J. Kim, Y. Song, T. Oh, J. Lee, J. Oh, S. Lyu, Y. Jeong, I. Lee, S. Seo, D. Lee, H. Kim, M. Lee, S. Jang, S. Do, S. Kim, K. Lim, J. Lee, K. Park, J. Shin, S. Kim, L. Park, A. Oh, J. Ha, and K. Cho(NeurIPS 2021 Datasets and Benchmarks Track) Domain-agnostic Question-Answering with Adversarial TrainingS. Lee*, D Kim*, and J. Park*(EMNLP-IJCNLP 2019 MRQA Workshop Shared Task) Awards 2nd Place @ OpenResource Hackathon Seoul 2019 - 2019.12Instagram Imageì™€ Textë¥¼ ì´ìš©í•œ Multimodal Hashtag Prediction Silver &amp; Bronze Award @ Naver NLP Challenge - 2018.12Silver award on NER Task, Bronze award on SRL Task","link":"/about/index.html"}],"posts":[{"title":"2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 1ë¶€","text":"2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œì„ ë§ˆì¹˜ê³ , ê·¸ ê³¼ì •ì„ ê¸€ë¡œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤. ì´ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ë“¤ì€ ë‚´ê°€ í–ˆë˜ ì‚½ì§ˆ(?)ì„ ìµœëŒ€í•œ ëœ í•˜ê¸¸ ë°”ë¼ëŠ” ë§ˆìŒì´ë‹¤:) 1ë¶€ì—ëŠ” ì‹¤ì œ í•™ìŠµì„ ëŒë¦¬ê¸° ì „ê¹Œì§€ì˜ ê³¼ì •ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. Github Repo: https://github.com/monologg/KoELECTRA ë¬¸ì œ ì˜ì‹í•œêµ­ì— Publicí•˜ê²Œ ê³µê°œë˜ì–´ ìˆëŠ” í•œêµ­ì–´ PLM(Pretrained Language Model)ì—ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ìˆë‹¤. SKTì˜ KoBERT TwoBlock AIì˜ HanBERT ETRIì˜ KorBERT 3ê°€ì§€ ëª¨ë‘ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, 3ê°€ì§€ ëª¨ë‘ ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. ê°ê°ì˜ ë‹¨ì ì„ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. ë‹¨ì  KoBERT Vocab size (8002ê°œ)ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì‘ìŒ HanBERT Tokenizerë¡œ ì¸í•´ Ubuntu í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥ KorBERT API ì‹ ì²­ ë“±ì˜ ê³¼ì • í•„ìš” íŠ¹íˆ 3ê°€ì§€ ëª¨ë‘ ê³µí†µì ìœ¼ë¡œ Huggingface Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ tokenization íŒŒì¼ì„ ë”°ë¡œ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ë‹¨ì ì´ ìˆë‹¤. KoBERTì™€ HanBERTì˜ ê²½ìš° ë‚´ê°€ ì§ì ‘ tokenization íŒŒì¼ì„ ë§Œë“¤ì–´ì„œ githubì— ë°°í¬í–ˆì§€ë§Œ, ì¼ë¶€ ì‚¬ìš©ìë“¤ì´ ë¶ˆí¸í•¨ì„ í† ë¡œí•˜ê¸°ë„ í–ˆë‹¤. ê·¸ë˜ì„œ ì´ë²ˆ ê¸°íšŒì— ìœ„ì˜ ë‹¨ì ë“¤ì„ ëª¨ë‘ í•´ê²°í•œ í•œêµ­ì–´ PLMì„ ê°œë°œí•˜ê³  ì‹¶ì—ˆë‹¤. Vocab sizeê°€ ì–´ëŠ ì •ë„ ì»¤ì•¼ í•¨ (30000ê°œ ì •ë„) ëª¨ë“  OSì—ì„œ ì‚¬ìš© ê°€ëŠ¥ tokenization íŒŒì¼ì„ ë§Œë“¤ í•„ìš” ì—†ì´ ê³§ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥ ì–´ëŠ ì •ë„ ì„±ëŠ¥ê¹Œì§€ ë³´ì¥ë˜ì–´ì•¼ í•¨ ìœ„ì˜ 4ê°€ì§€ë¥¼ ëª¨ë‘ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•˜ì—¬ ì‹œì‘í•œ í”„ë¡œì íŠ¸ê°€ ë°”ë¡œ KoELECTRA ì´ë‹¤. Tokenizerì‹¤ì œ í˜„ì—…ì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•´ Mecab + Sentencepieceë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³µì‹ BERT, ELECTRA ë“±ì€ Wordpieceë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, transformersì—ì„œë„ ê³µì‹ì ìœ¼ë¡œ Wordpieceë§Œ ì§€ì›í•˜ê³  ìˆë‹¤. ì¦‰, ELECTRAì—ì„œ Mecabì´ë‚˜ Sentencepieceë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ tokenization íŒŒì¼ì„ ë§Œë“¤ì–´ì•¼ í•˜ë©°, transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ apiê°€ ë°”ë€Œë©´ ë‚´ê°€ ì§ì ‘ ê·¸ì— ë§ê²Œ tokenization íŒŒì¼ì„ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. (KoBERTì˜ ê²½ìš° ì‹¤ì œë¡œë„ ê·¸ë ‡ê²Œ í•˜ê³  ìˆë‹¤ã… ) ì´ëŸ¬í•œ ë¬¸ì œë“¤ ë•Œë¬¸ì— ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë¬´ì¡°ê±´ìœ¼ë¡œ Wordpieceë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì•ˆìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤. Wordpiece â€œWordpiece vocabì„ ë§Œë“¤ ë•Œ Huggingfaceì˜ Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ë‹¤.â€ import argparsefrom tokenizers import BertWordPieceTokenizerparser = argparse.ArgumentParser()parser.add_argument(\"--corpus_file\", type=str)parser.add_argument(\"--vocab_size\", type=int, default=32000)parser.add_argument(\"--limit_alphabet\", type=int, default=6000)args = parser.parse_args()tokenizer = BertWordPieceTokenizer( vocab_file=None, clean_text=True, handle_chinese_chars=True, strip_accents=False, # Must be False if cased model lowercase=False, wordpieces_prefix=\"##\")tokenizer.train( files=[args.corpus_file], limit_alphabet=args.limit_alphabet, vocab_size=args.vocab_size)tokenizer.save(\"./\", \"bert-wordpiece\") Vocab sizeì˜ ê²½ìš° ê´€ë¡€ì ìœ¼ë¡œ ë§ì´ ì“°ì´ëŠ” ì•½ 3ë§Œê°œë¡œ ì„¸íŒ…í•˜ì˜€ë‹¤. ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ Corpusë§Œ ê°€ì§€ê³  vocab ë§Œë“¤ë©´ ì„±ëŠ¥ì´ êµ‰ì¥íˆ ì•ˆ ì¢‹ìŒ character coverageë¥¼ ìµœëŒ€í•œ ë†’ê²Œ ì¡ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  íŒë‹¨ (ì¦‰, corpusì—ì„œ ë“±ì¥í–ˆë˜ ëª¨ë“  characterë¥¼ vocabì— í¬í•¨ì‹œí‚´) ì˜ˆë¥¼ ë“¤ì–´ í€­ë©”ì¼ì´ë€ ë‹¨ì–´ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì. ë§Œì¼ í€­ì´ vocabì— ì—†ë‹¤ë©´ í€­ë©”ì¼ ì „ì²´ë¥¼ [UNK]ë¡œ ì²˜ë¦¬í•˜ê²Œ ëœë‹¤. ë§Œì¼ í€­ì´ vocab ì•ˆì— ìˆìœ¼ë©´ í€­ + ##ë©”ì¼ë¡œ tokenizeê°€ ë  ìˆ˜ ìˆì–´ ##ë©”ì¼ì´ë€ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ ì´ì „ì— í¬ìŠ¤íŒ…í•œ [ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°]ì„ ì°¸ê³ ) ì „ì²˜ë¦¬ (Preprocessing) â€œì²«ì§¸ë„ ì „ì²˜ë¦¬! ë‘˜ì§¸ë„ ì „ì²˜ë¦¬! ì…‹ì§¸ë„ ì „ì²˜ë¦¬!â€â€œPLMì˜ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ì£¼ëŠ” ê²ƒì€ corpus qualityì´ë‹¤!â€ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ì˜ ë¬¸ì¥ í•˜ë‚˜ë¥¼ ì‚´í´ë³´ì [ì£¼ìš”ê¸°ì‚¬] â˜ [í¬í†  ìŠ¤í† ë¦¬] ë¬´í—ˆê°€ ë„ì‹œê´‘ì‚° ì„ ì•„ì‹œë‚˜ìš”? â˜ [ë”°ëœ»í•œ ì‚¬ì§„ í•œ ì¥] ì‚¬ë‘, í•˜ë‚˜ê°€ ë˜ì–´ ê°€ëŠ” ê¸¸ &lt;ì°°ë‚˜ì˜ ê¸°ë¡, ìˆœê°„ì˜ ì§„ì‹¤ / KPPA ë°”ë¡œê°€ê¸°&gt; Copyrightsâ“’ í•œêµ­ì‚¬ì§„ê¸°ìí˜‘íšŒ(www.kppa.or.kr), powered by castnet. ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€ ë³´ë‚´ê¸° ë¬¸ì œëŠ” ì´ëŸ° ë¬¸ì¥ì´ ë§¤ìš° ë§ì€ë°ë‹¤ê°€, ì´ê±¸ Pretrainì— ë„£ì„ ì‹œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§ˆ ê²Œ ë»”í•˜ë‹¤â€¦. ì´ë ‡ê²Œ noiseê°€ ë§ì€ Corpusë¡œ vocabì„ ë§Œë“¤ê³  pretrainê¹Œì§€ í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì„ ë¦¬ê°€ ì—†ë‹¤. ì•„ë˜ëŠ” ë‚´ê°€ ì ìš©í•œ ëŒ€í‘œì ì¸ ì „ì²˜ë¦¬ ê¸°ì¤€ì´ë‹¤. í•œì, ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ì ì œê±° í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ê¸° (kss) ì‚¬ìš© ë‰´ìŠ¤ ê´€ë ¨ ë¬¸ì¥ì€ ì œê±° (ë¬´ë‹¨ì „ì¬, (ì„œìš¸=ë‰´ìŠ¤1) ë“± í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ ì œì™¸) ì‚¬ì‹¤ ì „ì²˜ë¦¬ì˜ ê¸°ì¤€ì— ì •ë‹µì€ ì—†ë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ìì‹ ì˜ Taskì— ë§ê²Œ ì „ì²˜ë¦¬ ê¸°ì¤€ì„ ì„¸ìš°ëŠ” ê²ƒì´ë‹¤. ë‚´ê°€ ìƒê°í–ˆë˜ Task ë“¤ì—ëŠ” í•œìëŠ” ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•´ì„œ ì§€ìš´ ê²ƒì´ì§€, í•œìê°€ ê¼­ í•„ìš”í•œ Taskì˜ ê²½ìš°ì—ëŠ” ì§€ìš°ë©´ ì•ˆ ë  ê²ƒì´ë‹¤. TPU ì‚¬ìš© ê´€ë ¨ Tip(ìì„¸í•œ ë‚´ìš©ì€ ì´ì „ì— í¬ìŠ¤íŒ…í•œ [TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸°]ì„ ì°¸ê³ ) 1. Tensorflow Research Cloud(TFRC)ë¥¼ ì“°ë©´ TPUê°€ ë¬´ë£Œâ†’ ì´ë¯¸ ê³µì‹ ì½”ë“œê°€ TPUë¥¼ ì™„ë²½íˆ ì§€ì›í•˜ê¸°ì—, ì§ì ‘ ELECTRAë¥¼ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ GPUë³´ë‹¤ëŠ” TPUë¥¼ ì“°ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•œë‹¤. 2. VM InstanceëŠ” ì‘ì€ ê²ƒ(n1-standard-1)ì„ ì¨ë„ ìƒê´€ ì—†ë‹¤â†’ ELECTRAë¥¼ GCPì—ì„œ í•™ìŠµí•˜ë ¤ë©´ TPU, Bucket, VM Instance ì´ë ‡ê²Œ 3ê°œê°€ í•„ìš”í•˜ë‹¤. ê·¸ëŸ°ë° ì €ì¥ì†ŒëŠ” Bucketì´, ì—°ì‚°ì€ TPUê°€ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— VM InstanceëŠ” ê°€ë²¼ìš´ ê²ƒì„ ì¨ë„ ëœë‹¤.â†’ n1-standard-1ëŠ” ì‹œê°„ë‹¹ ì•½ $0.037, n1-standard-4ëŠ” ì‹œê°„ë‹¹ ì•½ $0.14ì´ë‹¤. (ë¹„ìš©ì´ ë¬´ë ¤ 2ë°° ì°¨ì´!!) 3. TPUë¥¼ ì“°ëŠ” ê²½ìš° ëª¨ë“  input fileì€ Cloud storage bucketì„ í†µí•´ì•¼ë§Œ í•œë‹¤â†’ ì´ê²ƒë„ ì²˜ìŒì— ëª°ëë‹¤ê°€ ê³ ìƒí–ˆë˜ ì‚½ì§ˆ ì¤‘ í•˜ë‚˜ë‹¤. tf.estimator.tpu ìª½ ì½”ë“œë¥¼ ì“°ëŠ” ê²½ìš° ë¡œì»¬ ë°ì´í„°ê°€ ì•„ë‹Œ Bucketì„ í†µí•´ì•¼ë§Œ í•œë‹¤. (ê´€ë ¨ FAQ)â†’ ë‹¤í–‰íˆë„ Bucketì˜ ë¹„ìš©ì´ ë¹„ì‹¸ì§€ê°€ ì•Šë‹¤ (íŠ¹ì • ë¦¬ì „ì— ë§Œë“¤ì–´ ë†“ìœ¼ë©´ 1GBë‹¹ ì›” ì•½ $0.03) 4. VM Instanceì™€ TPUë¥¼ ë§Œë“¤ ë•Œ ctpu up ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ë¼â†’ VM Instanceì™€ TPUë¥¼ ë”°ë¡œ ë”°ë¡œ ìƒì„±í•˜ë©´ ì²˜ìŒì— ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì—ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ cloud shellë¡œ ëª…ë ¹ì–´ë¥¼ í•œ ë²ˆë§Œ ì¹˜ë©´ VMê³¼ TPUê°€ ë™ì‹œì— ìƒì„±ëœë‹¤ğŸ˜ƒ $ ctpu up --zone=europe-west4-a --tf-version=1.15 \\ --tpu-size=v3-8 --machine-type=n1-standard-2 \\ --disk-size-gb=20 --name={$VM_NAME} Configurationì˜ í•¨ì •ì•ì—ì„œë„ ì–¸ê¸‰í–ˆë“¯ì´ ELECTRA Pretrainingì€ ê³µì‹ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ë‹¤ ì“°ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê³µì‹ ì½”ë“œë¥¼ ê°€ì ¸ë‹¤ì“°ê¸° ì „ì— ë…¼ë¬¸ê³¼ ì½”ë“œ ë¶„ì„ì„ ì–´ëŠ ì •ë„ í•˜ê³  ì§„í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ê·¸ëŸ¼ì—ë„ ì¢€ í—·ê°ˆë ¸ë˜ ë¶€ë¶„ì´ ìˆì–´ ì—¬ê¸°ì„œ ì–¸ê¸‰í•˜ë ¤ í•œë‹¤. 1. ê³µì‹ ë ˆí¬ì—ì„œ ì œê³µí•˜ëŠ” small ëª¨ë¸ì€ ì •í™•íˆëŠ” small++ ëª¨ë¸ì´ë‹¤ ê³µì‹ ì½”ë“œì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ smallë¡œ ê³µê°œëœ ëª¨ë¸ì€ ì •í™•íˆ ë§í•˜ë©´ small++ ëª¨ë¸ì´ë‹¤. ë‘˜ì˜ ì°¨ì´ì ì€ ì•„ë˜ì™€ ê°™ë‹¤. max_seq_len generator_hidden_size small 128 0.25 small++ 512 1.0 (generator_hidden_size=1.0ì´ë€ ê²ƒì€ discriminatorì™€ generatorì˜ hidden_sizeê°€ ê°™ë‹¤ëŠ” ê²ƒì´ë‹¤) ì´ëŸ¬í•œ ë¶€ë¶„ì´ ë…¼ë¬¸ì— ìì„¸íˆ ë‚˜ì™€ ìˆì§€ ì•Šì•„ ì²˜ìŒì— small ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ì§„ì§œ smallë¡œ ë§Œë“¤ì—ˆë‹¤ê°€ ë‹¤ì‹œ small++ë¡œ ë§Œë“œëŠ” ìˆ˜ê³ ë¥¼ ê±°ì³¤ë‹¤â€¦. (ì´ ê¸€ì„ ì½ì€ ë¶„ë“¤ì€ ì´ ì‚½ì§ˆì„ ì•ˆ í•˜ê¸¸ ë¹ˆë‹¤.) hparams.json{ \"tpu_name\": \"electra-small\", \"tpu_zone\": \"europe-west4-a\", \"num_train_steps\": 1000000, \"save_checkpoints_steps\": 50000, \"train_batch_size\": 128, \"learning_rate\": 5e-4, \"vocab_size\": 32200, \"max_seq_length\": 512, \"generator_hidden_size\": 1.0} 2. max_seq_lengthë¥¼ 128ë¡œ ì¤„ì¸ë‹¤ë©´ max_position_embeddingsë„ 128ë¡œ ì¤„ì—¬ì•¼ í•œë‹¤ê°„í˜¹ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆëœ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ max_seq_lengthë¥¼ ì¤„ì´ê³ ì í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ê·¸ëŸ´ ì‹œ max_seq_lengthë§Œ ì¤„ì´ë©´ í•´ê²°ëœë‹¤ê³  ì˜¤í•´í•  ìˆ˜ ìˆëŠ”ë°, max_position_embeddingsë„ ì¤„ì—¬ì¤˜ì•¼ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ê²Œ ë³€í™˜í•  ë•Œ ë¬¸ì œê°€ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤. (transformersê°€ max_position_embeddingsìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì•Œì•„ë‚´ê¸° ë•Œë¬¸!) ë” í° í•¨ì •ì€ ì•„ë˜ì™€ ê°™ì´ model_hparam_overridesë¼ëŠ” attribute ì•ˆì— max_position_embeddingsë¥¼ ë„£ì–´ì¤˜ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤! hparams.json{ \"max_seq_length\": 128, \"model_hparam_overrides\": { \"max_position_embeddings\": 128 }} ë§ˆì¹˜ë©°1ë¶€ì—ì„œëŠ” ì‹¤ì œ Pretrainingì„ ì‹œì‘í•˜ê¸° ì „ì˜ ì¤€ë¹„ ê³¼ì •ì„ ë‹¤ë¤˜ë‹¤. 2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€ ì—ì„œëŠ” ì‹¤ì œ Pretraining, Transformers í¬íŒ…, Finetuning ë“±ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤:) Reference ELECTRA Official Code Huggingface Tokenizers Cloud TPU Documentation Related Posts TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸° ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸° 2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€","link":"/2020/05/02/koelectra-part1/"},{"title":"TPUë¥¼ ì´ìš©í•˜ì—¬ Electra Pretrainingí•˜ê¸°","text":"ìµœê·¼ ELECTRAì˜ ê³µì‹ ì½”ë“œê°€ ê³µê°œë˜ë©´ì„œ í•œêµ­ì–´ Corpusì— ì§ì ‘ Electraë¥¼ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” GCPì—ì„œ TPUë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í–ˆëŠ”ì§€ ê·¸ ê³¼ì •ì„ ê³µìœ í•´ë³´ë ¤ í•œë‹¤. Tensorflow Research Cloud ì‹ ì²­Tensorflow Research Cloud (TFRC)ëŠ” 1ë‹¬ ë™ì•ˆ TPUë¥¼ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”„ë¡œê·¸ë¨ì´ë‹¤. í•´ë‹¹ ë§í¬ë¡œ ê°€ì„œ ì‹ ì²­ì„ í•˜ê²Œ ë˜ë©´ ë©”ì¼ì´ í•˜ë‚˜ ì˜¤ê²Œ ëœë‹¤. í•´ë‹¹ ë©”ì¼ì—ì„œ ìš”êµ¬í•˜ëŠ” ëŒ€ë¡œ ì‹ ì²­ì„œë¥¼ ì¶”ê°€ë¡œ ì‘ì„±í•œ í›„ ì œì¶œí•˜ë©´ ì–¼ë§ˆ í›„ ì•„ë˜ì™€ ê°™ì´ ë‹µì¥ì´ ì˜¤ê²Œ ë˜ê³ , ê·¸ ë•Œë¶€í„° GCPì—ì„œ TPUë¥¼ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤:) Bucketì— Data ì—…ë¡œë“œTPUë¥¼ ì“°ëŠ” ê²½ìš° ëª¨ë“  input fileì„ Cloud storage bucketì„ í†µí•´ì•¼ë§Œ í•œë‹¤. (ê´€ë ¨ FAQ) Bucket ìƒì„± ì˜ˆì œìƒ Bucketì˜ ì´ë¦„ì„ test-for-electraë¡œ ë§Œë“¤ì–´ ë³´ê² ë‹¤. GCP ë©”ì¸ í˜ì´ì§€ ì¢Œì¸¡ì˜ [Storage] - [ë¸Œë¼ìš°ì €] ë¡œ ì´ë™ ë²„í‚· ë§Œë“¤ê¸° í´ë¦­ ì‚¬ìš©í•  TPUì™€ ë™ì¼í•œ Regionì— Bucket ë§Œë“œëŠ” ê²ƒì„ ê¶Œì¥ File Upload ì¤€ë¹„í•œ pretrain_tfrecordsì™€ vocab.txtë¥¼ Bucketì— ì—…ë¡œë“œ GCP VM &amp; TPU ìƒì„± VMê³¼ TPUë¥¼ ê°ê° ë”°ë¡œ ë§Œë“œëŠ” ê²ƒë³´ë‹¤, ìš°ì¸¡ ìƒë‹¨ì˜ cloud shellì„ ì—´ì–´ ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. ì €ì¥ì†ŒëŠ” Bucketì´, ì—°ì‚°ì€ TPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— VM InstanceëŠ” ê°€ë²¼ìš´ ê²ƒì„ ì¨ë„ ìƒê´€ì´ ì—†ë‹¤. $ ctpu up --zone=europe-west4-a --tf-version=1.15 \\ --tpu-size=v3-8 --machine-type=n1-standard-1 \\ --disk-size-gb=20 --name={$VM_NAME} Electra í•™ìŠµ ì§„í–‰$ git clone https://github.com/google-research/electra$ cd electra$ python3 run_pretraining.py --data-dir gs://{$BUCKET_NAME} \\ --model-name {$MODEL_NAME} \\ --hparams {$CONFIG_PATH} í•™ìŠµ ì™„ë£Œ í›„ Instance, Bucket ì‚­ì œ$ ctpu delete --zone=europe-west4-a --name={$VM_NAME}$ gsutil rm -r gs://test-for-electra Reference ELECTRA official github A Pipeline Of Pretraining Bert On Google TPU Official TPU Documentation","link":"/2020/04/20/tpu-electra/"},{"title":"2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 2ë¶€","text":"2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œì„ ë§ˆì¹˜ê³ , ê·¸ ê³¼ì •ì„ ê¸€ë¡œ ë‚¨ê¸°ë ¤ê³  í•œë‹¤. ì´ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ë“¤ì€ ë‚´ê°€ í–ˆë˜ ì‚½ì§ˆ(?)ì„ ìµœëŒ€í•œ ëœ í•˜ê¸¸ ë°”ë¼ëŠ” ë§ˆìŒì´ë‹¤:) 2ë¶€ì—ëŠ” Pretraining, Finetuning ë“±ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. Github Repo: https://github.com/monologg/KoELECTRA TrainingğŸ˜€ ë“œë””ì–´ ëª¨ë“  ì‚½ì§ˆë“¤ì„ ëë‚´ê³ , Pretrainingì„ ì‹œì‘í•˜ì˜€ë‹¤ ğŸ˜€ Loss ì•½ 300k step ê¹Œì§€ì˜ baseì™€ smallì˜ loss ì¶”ì´ì´ë‹¤. ì²« 100kê¹Œì§€ëŠ” ë§¤ìš° ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë‹¤ê°€, ê·¸ ì´í›„ì—ëŠ” ì¡°ê¸ˆì”© ì¤„ì–´ë“œëŠ” ëª¨ìŠµì„ ë³´ì¸ë‹¤. Benchmarkí•™ìŠµ ì¤‘ê°„ì¤‘ê°„ ì„±ëŠ¥ ì²´í¬ëŠ” nsmc ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  ê°„ë‹¨í•˜ê²Œ í‰ê°€í•˜ì˜€ë‹¤ (ì‚¬ì‹¤ GPUê°€ 1ê°œ ë°–ì— ì—†ì–´ nsmcë§Œ í…ŒìŠ¤íŠ¸í•œ ê±´ ë¹„ë°€ğŸ˜¢) Acc(%) 25K 75K 90K 125K 150K 250K 300K 450K Base 88.10 88.48 88.67 88.92 88.97 89.51 89.65 90.16 Stepì´ ì¦ê°€í• ìˆ˜ë¡ accuracyê°€ ì˜¤ë¥´ëŠ” ê²ƒì´ ëˆˆì— ë„ê²Œ ë³´ì´ë‹ˆ ì‹ ê¸°í•˜ê¸´ í–ˆë‹¤. (ì´ê²ƒì´ Pretrainingì˜ í˜ì¸ê°€â€¦.) Training Timeë°ì´í„°ì˜ ê²½ìš° 14GBë¡œ ì´ 2.6B Tokenì´ë‹¤. BERTì™€ ELECTRAì—ì„œëŠ” 3.3B Tokenì„ ì‚¬ìš©í•œ ê²ƒì— ë¹„í•˜ë©´ ë°ì´í„°ì˜ ì–‘ì´ ì¡°ê¸ˆ ëª¨ìë€ ê²Œ ì•„ì‰½ê¸´ í•˜ì§€ë§Œ, ì´ê²ƒì´ ê°œì¸ ë‹¨ìœ„ì—ì„œ ëª¨ì„ ìˆ˜ ìˆì—ˆë˜ ìµœì„ ì˜ ë°ì´í„°ì–‘ì´ì—ˆë‹¤ğŸ˜µ TPU v3-8 ê¸°ì¤€ìœ¼ë¡œ Base ëª¨ë¸ì€ ì•½ 7ì¼, Small ëª¨ë¸ì€ ì•½ 3ì¼ì´ ì†Œìš”ë˜ì—ˆë‹¤. ê·¸ë˜ì„œ ì´ ê¸°ê°„ ë™ì•ˆ Finetuning ì½”ë“œë¥¼ ì§œëŠ” ê²ƒìœ¼ë¡œ ì‹œê°„ì„ ì ˆì•½í•˜ì˜€ë‹¤. Finetuning ì½”ë“œ ì œì‘ì½”ë“œì˜ ê²½ìš°ëŠ” Transformersì˜ Example ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì œì‘í•˜ì˜€ë‹¤. Finetuningì˜ ê²½ìš° ì´ 7ê°œì˜ taskì— ëŒ€í•´ ì§„í–‰í•˜ì˜€ë‹¤. (ë•Œë§ˆì¹¨ ì–¼ë§ˆ ì „ ì¹´ì¹´ì˜¤ë¸Œë ˆì¸ì—ì„œ KorNLIì™€ KorSTS ë°ì´í„°ì…‹ì„ ê³µê°œí•´ì£¼ì—ˆë‹¤ğŸ‘ 1ë…„ ì „ê³¼ ë¹„êµí–ˆì„ ë•Œ ë²¤ì¹˜ë§ˆí¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ë°ì´í„°ì…‹ì´ ë§ì•„ì§„ ê²ƒì€ ì •ë§ ì¢‹ì€ ì¼ì´ë¼ í•  ìˆ˜ ìˆë‹¤.) NSMC PAWS QuestionPair KorNLI KorSTS NaverNER KorQuad Task ê°ì •ë¶„ì„ ìœ ì‚¬ë¬¸ì¥ ìœ ì‚¬ë¬¸ì¥ ì¶”ë¡  ìœ ì‚¬ë¬¸ì¥ ê°œì²´ëª…ì¸ì‹ ê¸°ê³„ë…í•´ Metric Acc Acc Acc Acc Spearman F1 EM/F1 ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì—ì„œëŠ” Bert-Multilingual-Casedë¥¼ ê°€ì§€ê³  ë§ì´ ë¹„êµí•˜ì˜€ëŠ”ë°, ì´ë²ˆ ì—°êµ¬ì—ì„œëŠ” XLM-Roberta-Base ëª¨ë¸ë¡œ í‰ê°€ë¥¼ ì‹œë„í•˜ì˜€ë‹¤. í™•ì‹¤íˆ xlm-robertaê°€ bertë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ì¢‹ì•˜ê¸°ì—, ì ì–´ë„ KoELECTRAê°€ xlm-robertaëŠ” ë›°ì–´ë„˜ì–´ì•¼ ìœ ì˜ë¯¸í•˜ì§€ ì•Šì„ê¹Œë¼ê³  ìƒê°í•´ì„œ ì˜€ë‹¤. Deviewì—ì„œ ë°œí‘œí•œ Larvaì˜ ê²½ìš° Benchmark pipelineì„ ë§Œë“¤ì–´ì„œ ckptê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ê³„ì† evaluationì„ í•´ì£¼ì—ˆë‹¤ê³  í•˜ëŠ”ë°, ì•ì—ì„œë„ ë§í–ˆë“¯ì´ ë‚˜ì—ê²ŒëŠ” GPUê°€ 1ê°œ ë°–ì— ì—†ê³ , GCPì—ì„œ GPUë¥¼ ë§ì´ ë¹Œë¦´ ìˆ˜ë„ ì—†ê¸°ì— ê°€ì¥ ìµœê·¼ 5ê°œì˜ ckptë¥¼ ê°€ì§€ê³  í‰ê°€í•˜ì˜€ê³ , ê·¸ ì¤‘ì—ì„œ í‰ê· ê°’ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ê²ƒì„ ìµœì¢… ëª¨ë¸ë¡œ ì„ ì •í•˜ì˜€ë‹¤. Finetuningìš© ì½”ë“œ ë° ì‚¬ìš©ë²•ì€ ì—¬ê¸°ì—ì„œ ì§ì ‘ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤. Convert from Tensorflow to PytorchHuggingfaceì—ì„œ ElectraModelì„ êµ¬í˜„í•˜ë©´ì„œ tensorflowë¥¼ pytorchë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œë„ í•¨ê»˜ ë§Œë“¤ì–´ì¤¬ë‹¤ğŸ˜ ê´€ë ¨ ë‚´ìš©ì€ [ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°]ë¥¼ ì½ì–´ë³´ê¸¸ ë°”ë€ë‹¤. (ì—¬ê¸°ì„œë„ êµ‰ì¥íˆ ì‚½ì§ˆì„ ë§ì´ í•´ë³¸ ì…ì¥ìœ¼ë¡œì„œ ê¼­ ì½ì–´ë³´ê¸¸ ê¶Œí•œë‹¤) Result ì²˜ìŒì— ì´ í”„ë¡œì íŠ¸ë¥¼ ê³„íší•  ë•Œ ê°€ì¥ ê±±ì •ë˜ì—ˆë˜ ì ì´ â€œì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¤ë©´ ì–´ì©Œë‚˜â€ì˜€ë‹¤. (ì„±ëŠ¥ì´ ë„ˆë¬´ ì•ˆ ì¢‹ìœ¼ë©´ ì‚¬ì‹¤ 2ì£¼ë¥¼ ì œëŒ€ë¡œ ë‚ ë¦° ì…ˆì´ê¸°ì—â€¦.) ê²°ê³¼ëŠ” ì˜ˆìƒí–ˆë˜ ê²ƒë³´ë‹¤ í›¨ì”¬ ì¢‹ì•˜ë‹¤. ì• ì´ˆì— ë°ì´í„°ì˜ ì–‘ì´ë‚˜ Tokenizer ë“±ì„ ê³ ë ¤í–ˆì„ ë•Œ HanBERTë¥¼ ì™„ë²½íˆ ë”°ë¼ì¡ëŠ” ê²ƒì€ ë¬´ë¦¬ë¼ê³  ìƒê°í–ˆì§€ë§Œ, KoBERTë³´ë‹¤ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë§ì´ ì¢‹ì„ ì¤„ì€ ëª°ëë‹¤. HanBERTì™€ë„ ì‹¤ì§ˆì ì¸ ê²°ê³¼ëŠ” ë¹„ìŠ·í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ë” ì¢‹ì€ ì¼€ì´ìŠ¤ë„ ìˆì–´ì„œ ì´ ì •ë„ë©´ ğŸ‰ëŒ€ì„±ê³µğŸ‰ì´ë¼ê³  ë´ë„ ë  êº¼ ê°™ë‹¤. KoELECTRA-Smallì˜ ì„±ëŠ¥ì´ ê°€ì¥ ì¸ìƒì ì´ì—ˆëŠ”ë°, ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ DistilKoBERTì˜ ì ˆë°˜ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ê²½ëŸ‰í™” ëª¨ë¸ì—ì„œ ì¶©ë¶„íˆ ì‚¬ìš©í•  ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ë‹¤. How to Useì´ í”„ë¡œì íŠ¸ë¥¼ ì²˜ìŒ ê³„íší–ˆì„ ë•Œì˜ ê³ ë ¤ì‚¬í•­ ì¤‘ ì•„ë˜ ì‚¬í•­ë“¤ì´ ê°€ì¥ ì¤‘ìš”í–ˆì—ˆë‹¤. â€œëª¨ë“  OSì—ì„œ ì‚¬ìš© ê°€ëŠ¥â€â€œtokenization íŒŒì¼ì„ ë§Œë“¤ í•„ìš” ì—†ì´ ê³§ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥â€ ê·¸ë¦¬ê³  ì´ì œ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ìˆìœ¼ë©´ ì–´ë– í•œ í™˜ê²½ì—ì„œë„ í•œêµ­ì–´ PLMì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œëœ ê²ƒì´ë‹¤ğŸ¤— Installation$ pip3 install -U transformers How to Usefrom transformers import ElectraModel, ElectraTokenizer# KoELECTRA-Basemodel = ElectraModel.from_pretrained(\"monologg/koelectra-base-discriminator\")tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")# KoELECTRA-Smallmodel = ElectraModel.from_pretrained(\"monologg/koelectra-small-discriminator\")tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-discriminator\") ë§ºìœ¼ë©° ì†”ì§íˆ ì´ë ‡ê²Œê¹Œì§€ ë°˜ì‘ì´ ì¢‹ì„ ì¤„ì€ ëª°ëë‹¤ğŸ™„ ê°œë°œìë¡œì„œ ì´ëŸ´ ë•Œê°€ ê°€ì¥ ë³´ëŒìˆì§€ ì•Šì€ê°€ ì‹¶ë‹¤. ì²˜ìŒ ê³„íší•  ë•Œë¶€í„° â€œì´ê±° ë§Œë“¤ë©´ ë‹¤ë¥¸ ë¶„ë“¤ì—ê²Œë„ í° ë„ì›€ì´ ë˜ê² ë‹¤â€ë¼ ìƒê°í–ˆëŠ”ë°, ì‹¤ì œë¡œë„ ê·¸ëŸ° ê²ƒ ê°™ì•„ ê¸°ë¶„ì´ (êµ‰ì¥íˆ) ì¢‹ë‹¤ ğŸ˜€ â€œë‚´ê°€ ë¶ˆí¸í•´í•˜ëŠ” ê²ƒì€ ë¶„ëª… ë‹¤ë¥¸ ëˆ„êµ°ê°€ë„ ë¶ˆí¸í•´í•œë‹¤. ê·¸ëŸ° ë¶€ë¶„ì„ í•´ê²°í•´ì£¼ëŠ” ê²ƒì´ ê°œë°œìì˜ ì—­í• ì´ë¼ê³  ìƒê°í•œë‹¤.â€ Reference ELECTRA Official Code Huggingface Transformers Deview 2019 Larva Presentation KorNLI, KorSTS Related Posts ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸° 2ì£¼ ê°„ì˜ KoELECTRA ê°œë°œê¸° - 1ë¶€","link":"/2020/05/02/koelectra-part2/"},{"title":"ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°","text":"ê°œì¸ì ìœ¼ë¡œ Pretrained Language Model ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ë¡œ Vocab qualityë¼ê³  ìƒê°í•œë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” tokenizationì˜ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ Wordpieceë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ vocabì„ ë§Œë“œëŠ”ì§€ ì•Œì•„ë³´ë ¤ í•œë‹¤:) Introductioní•œêµ­ì–´ Tokenizerì˜ ëŒ€ì•ˆìœ¼ë¡œëŠ” í¬ê²Œ Sentencepiece, Mecab, Wordpieceê°€ ìˆë‹¤. (ì—¬ê¸°ì„œì˜ wordpieceëŠ” Googleì˜ BERTì—ì„œ ì‚¬ìš©ëœ wordpieceë¡œ ê°€ì •í•œë‹¤.) BERT, ELECTRA ë“±ì€ ê¸°ë³¸ì ìœ¼ë¡œ Wordpieceë¥¼ ì‚¬ìš©í•˜ê¸°ì— ê³µì‹ ì½”ë“œì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µë˜ëŠ” Tokenizer ì—­ì‹œ ì´ì— í˜¸í™˜ë˜ê²Œ ì½”ë“œê°€ ì‘ì„±ë˜ì—ˆë‹¤. ì¦‰, Sentencepieceë‚˜ Mecabì„ ì‚¬ìš©í•˜ë ¤ë©´ ë³„ë„ì˜ Tokenizerë¥¼ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•˜ê³ , ì´ë ‡ê²Œ ë˜ë©´ transformers ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª¨ë¸ì„ ê³§ë°”ë¡œ ì‚¬ìš©í•˜ëŠ”ë° ë¶ˆí¸í•¨ì´ ìƒê¸°ê²Œ ëœë‹¤. Original wordpiece code is NOT available! ê³µì‹ BERTì—ì„œ ì‚¬ìš©ëœ Wordpiece BuilderëŠ” ì œê³µë˜ì§€ ì•Šê³  ìˆë‹¤. BERT ê³µì‹ Githubì—ì„œ ë‹¤ë¥¸ ëŒ€ì•ˆë“¤ì„ ì œì‹œí•´ì¤¬ì§€ë§Œ, ì™„ì „íˆ ë™ì¼í•œ Wordpiece Vocabì´ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤. ëª‡ëª‡ ì˜¤í”ˆì†ŒìŠ¤ë“¤ì´ Wordpiece vocab builderë¥¼ êµ¬í˜„í•˜ì˜€ì§€ë§Œ input fileì´ ë§¤ìš° í´ ì‹œ ë©”ëª¨ë¦¬, ì†ë„ ë“±ì˜ ì´ìŠˆê°€ ì¢…ì¢… ë°œìƒí•œë‹¤ã…  Huggingface Tokenizers ìµœì¢…ì ìœ¼ë¡œ, ìµœê·¼ Huggingfaceì—ì„œ ë°œí‘œí•œ Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ Wordpiece Vocabularyë¥¼ ë§Œë“œëŠ”ê²Œ ì œì¼ ì¢‹ì•˜ë‹¤. í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ Corpusê°€ ë§¤ìš° ì»¤ë„ ë©”ëª¨ë¦¬ ì´ìŠˆê°€ ë°œìƒí•˜ì§€ ì•Šìœ¼ë©°, Rustë¡œ êµ¬í˜„ì´ ë˜ì–´ìˆì–´ ì†ë„ ë˜í•œ Pythonë³´ë‹¤ ë¹ ë¥´ë‹¤ğŸ˜ƒ Code for building Wordpiece vocab(tokenizer v0.7.0 ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ë‹¤. í˜„ì¬ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—…ë°ì´íŠ¸ ì¤‘ì´ì–´ì„œ apiê°€ ë‹¬ë¼ì§ˆ ìˆ˜ë„â€¦) import argparsefrom tokenizers import BertWordPieceTokenizerparser = argparse.ArgumentParser()parser.add_argument(\"--corpus_file\", type=str)parser.add_argument(\"--vocab_size\", type=int, default=32000)parser.add_argument(\"--limit_alphabet\", type=int, default=6000)args = parser.parse_args()tokenizer = BertWordPieceTokenizer( vocab_file=None, clean_text=True, handle_chinese_chars=True, strip_accents=False, # Must be False if cased model lowercase=False, wordpieces_prefix=\"##\")tokenizer.train( files=[args.corpus_file], limit_alphabet=args.limit_alphabet, vocab_size=args.vocab_size)tokenizer.save(\"./\", \"ch-{}-wpm-{}\".format(args.limit_alphabet, args.vocab_size)) ì£¼ì˜í•´ì•¼í•  ì ì€ lowercase=Falseë¡œ í•  ì‹œ strip_accent=Falseë¡œ í•´ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒ! [UNK]ì˜ ë¹„ì¤‘ì„ ìµœëŒ€í•œ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë“  characterë¥¼ ì»¤ë²„í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬í•˜ì˜€ë‹¤. (limit_alphabet) Corpusì˜ ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆë‹¤ëŠ” ì „ì œí•˜ì— sentencepieceì™€ ë¹„êµí–ˆì„ ë•Œ UNK Ratioê°€ í›¨ì”¬ ë‚®ì•˜ë‹¤. Reference Sentencepiece vs Wordpiece Learning a new WordPiece vocabulary kwonmhaâ€™s bert-vocab-builder Huggingface Tokenizers","link":"/2020/04/27/wordpiece-vocab/"},{"title":"ë‚´ê°€ ë§Œë“  ELECTRAë¥¼ Huggingface Transformersë¡œ Portingí•˜ê¸°","text":"BERT, ALBERT, ELECTRA ë“±ì„ ì§ì ‘ Pretrainí•˜ê²Œ ë˜ë©´ ëª¨ë¸ì´ Tensorflowì˜ ckpt í˜•íƒœë¡œ ì €ì¥ì´ ëœë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” tensorflow ckptë¥¼ transformersì˜ pytorch ckptë¡œ ë³€í™˜í•˜ëŠ” ë²•ì„ ì•Œì•„ë³´ê² ë‹¤ğŸ¤— Intro ì´ë²ˆ ê¸€ì—ì„œëŠ” ELECTRA-Smallì„ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ìŠµì„ í•´ë³¸ë‹¤. (BERT ë“±ë„ ë°©ë²•ì€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤) transformers v2.8.0ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ë‹¤. ì´í›„ ë²„ì „ì—ì„œ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆë‹¤. Transformersì˜ ELECTRAëŠ” discriminatorì™€ generatorë¥¼ ê°ê° ë”°ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•œë‹¤! Prerequisite1. Original Tensorflow Checkpointë‹¹ì—°íˆ Tensorflowë¡œ í•™ìŠµí•œ ê²°ê³¼ë¬¼ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•œë‹¤. .â”œâ”€â”€ koelectra-small-tfâ”‚ â”œâ”€â”€ checkpointâ”‚ â”œâ”€â”€ events.out.tfevents.1586942968.koelectra-smallâ”‚ â”œâ”€â”€ graph.pbtxtâ”‚ â”œâ”€â”€ ...â”‚ â”œâ”€â”€ model.ckpt-700000.data-00000-of-00001â”‚ â”œâ”€â”€ model.ckpt-700000.indexâ”‚ â””â”€â”€ model.ckpt-700000.metaâ””â”€â”€ ... checkpointmodel_checkpoint_path: &quot;model.ckpt-700000&quot;all_model_checkpoint_paths: &quot;model.ckpt-600000&quot;all_model_checkpoint_paths: &quot;model.ckpt-625000&quot;all_model_checkpoint_paths: &quot;model.ckpt-650000&quot;all_model_checkpoint_paths: &quot;model.ckpt-675000&quot;all_model_checkpoint_paths: &quot;model.ckpt-700000&quot; ì£¼ì˜í•  ì ì€ checkpoint íŒŒì¼ì—ì„œ model_checkpoint_path ê°’ì„ â€œì›í•˜ëŠ” stepì˜ ckptâ€ë¡œ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. 2. config.json (ì£¼ì˜!) transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—…ë°ì´íŠ¸ë˜ë©´ì„œ APIê°€ ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìˆê³ , ì´ì— ë”°ë¼ config.jsonì˜ attributeê°€ ì¶”ê°€/ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. https://huggingface.co/modelsë¡œ ê°€ì„œ ëŒ€í‘œ ëª¨ë¸ì˜ config.jsonì„ ë³´ë©´ì„œ ì§ì ‘ ë§Œë“¤ì–´ì•¼ í•œë‹¤. vocab_size ë³€ê²½ì—ë§Œ ì£¼ì˜í•˜ë©´ ì¶©ë¶„í•¨ ë§Œì¼ max_seq_lengthë¥¼ ë°”ê¿¨ë‹¤ë©´ max_position_embeddingsë„ ë°”ê¿”ì•¼ í•¨ discriminator{ \"architectures\": [\"ElectraForPreTraining\"], \"attention_probs_dropout_prob\": 0.1, \"embedding_size\": 128, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 256, \"initializer_range\": 0.02, \"intermediate_size\": 1024, \"layer_norm_eps\": 1e-12, \"max_position_embeddings\": 512, \"model_type\": \"electra\", \"num_attention_heads\": 4, \"num_hidden_layers\": 12, \"pad_token_id\": 0, \"type_vocab_size\": 2, \"vocab_size\": 32200} generator{ \"architectures\": [\"ElectraForMaskedLM\"], \"attention_probs_dropout_prob\": 0.1, \"embedding_size\": 128, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 256, \"initializer_range\": 0.02, \"intermediate_size\": 1024, \"layer_norm_eps\": 1e-12, \"max_position_embeddings\": 512, \"model_type\": \"electra\", \"num_attention_heads\": 4, \"num_hidden_layers\": 12, \"pad_token_id\": 0, \"type_vocab_size\": 2, \"vocab_size\": 32200} 3. tokenizer_config.jsoncased ëª¨ë¸ì˜ ê²½ìš° ê·¸ëƒ¥ tokenizerë¥¼ loadí•˜ë©´ ë§¤ë²ˆ do_lower_case=Falseë¥¼ ì§ì ‘ ì¶”ê°€í•´ì¤˜ì•¼ í•œë‹¤. from transformers import ElectraTokenizertokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-discriminator\", do_lower_case=False) tokenizer_config.jsonì„ ë§Œë“¤ì–´ì£¼ë©´ ì´ëŸ¬í•œ ë²ˆê±°ë¡œì›€ì„ ì—†ì•¨ ìˆ˜ ìˆë‹¤.(ë§Œì¼ max_seq_lengthê°€ 128ì´ë©´ model_max_lengthë„ 128ë¡œ ë°”ê¿”ì£¼ë©´ ëœë‹¤.) tokenizer_config.json{ \"do_lower_case\": false, \"model_max_length\": 512} 4. vocab.txttensorflowì—ì„œ í•™ìŠµí–ˆì„ ë•Œ ì“´ vocab.txtë¥¼ ê·¸ëŒ€ë¡œ ì“°ë©´ ëœë‹¤. 5. ìµœì¢…ì ì¸ ë””ë ‰í† ë¦¬ í˜•íƒœ.â”œâ”€â”€ koelectra-small-tfâ”‚ â”œâ”€â”€ checkpointâ”‚ â”œâ”€â”€ events.out.tfevents.1586942968.koelectra-smallâ”‚ â”œâ”€â”€ graph.pbtxtâ”‚ â”œâ”€â”€ ...â”‚ â”œâ”€â”€ model.ckpt-700000.data-00000-of-00001â”‚ â”œâ”€â”€ model.ckpt-700000.indexâ”‚ â””â”€â”€ model.ckpt-700000.metaâ”‚â”œâ”€â”€ electra-small-discriminatorâ”‚ â”œâ”€â”€ config.jsonâ”‚ â”œâ”€â”€ tokenizer_config.jsonâ”‚ â””â”€â”€ vocab.txtâ”‚â”œâ”€â”€ electra-small-generatorâ”‚ â”œâ”€â”€ config.jsonâ”‚ â”œâ”€â”€ tokenizer_config.jsonâ”‚ â””â”€â”€ vocab.txtâ””â”€â”€ ... electra-small-discriminatorì™€ electra-small-generator í´ë”ë¥¼ ê°ê° ë§Œë“ ë‹¤. config.jsonì€ discriminatorìš©ê³¼ generatorìš©ì„ ë”°ë¡œ ë§Œë“¤ì–´ì„œ í´ë” ì•ˆì— ë„£ëŠ”ë‹¤. tokenizer_config.jsonê³¼ vocab.txtëŠ” discriminatorì™€ generator ë‘˜ ë‹¤ ë™ì¼í•œ íŒŒì¼ì„ ë„£ìœ¼ë©´ ëœë‹¤. Convertconvert.pyimport osimport argparsefrom transformers.convert_electra_original_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorchparser = argparse.ArgumentParser()parser.add_argument(\"--tf_ckpt_path\", type=str, default=\"koelectra-small-tf\")parser.add_argument(\"--pt_discriminator_path\", type=str, default=\"koelectra-small-discriminator\")parser.add_argument(\"--pt_generator_path\", type=str, default=\"koelectra-small-generator\")args = parser.parse_args()convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path, config_file=os.path.join(args.pt_discriminator_path, \"config.json\"), pytorch_dump_path=os.path.join(args.pt_discriminator_path, \"pytorch_model.bin\"), discriminator_or_generator=\"discriminator\")convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path, config_file=os.path.join(args.pt_generator_path, \"config.json\"), pytorch_dump_path=os.path.join(args.pt_generator_path, \"pytorch_model.bin\"), discriminator_or_generator=\"generator\") Upload your model to Huggingface s3 ë¨¼ì € huggingface.coë¡œ ê°€ì„œ íšŒì›ê°€ì…ì„ í•´ì•¼ í•¨ ì•„ë˜ì˜ ëª…ë ¹ì–´ë¡œ s3ì— ì—…ë¡œë“œ ì§„í–‰ $ transformers-cli login$ transformers-cli upload koelectra-small-discriminator$ transformers-cli upload koelectra-small-generator Now Letâ€™s Use ItHow to Usefrom transformers import ElectraModel, ElectraTokenizermodel = ElectraModel.from_pretrained(\"monologg/koelectra-small-discriminator\")tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-discriminator\") ë§ºìœ¼ë©° ì‚¬ì‹¤ Model Portingê³¼ ê´€ë ¨í•˜ì—¬ ëª…í™•í•œ Documentationì´ ì—†ì–´ ë‚˜ë„ ì‚½ì§ˆì„ ìƒë‹¹íˆ í–ˆë˜ ë¶€ë¶„ì´ë‹¤. ì´ë²ˆ ë‚´ìš©ì´ ë‹¤ë¥¸ ë¶„ë“¤ì—ê²Œ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ í•œë‹¤ğŸ˜› ë˜í•œ Huggingface s3ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ëŠ” ê²ƒì€ ê¼­ ì‚¬ìš©í•´ë³´ê¸¸ ê¶Œí•œë‹¤. ì´ ê¸°ëŠ¥ì´ ìƒê¸´ì§€ ì–¼ë§ˆë˜ì§€ ì•Šì•„ì„œ ëª¨ë¥´ì‹œëŠ” ë¶„ë“¤ì´ ìˆëŠ”ë°, ì—…ë¡œë“œ ìš©ëŸ‰ì˜ ì œí•œë„ ì—†ê³  ì—¬ëŸ¬ëª¨ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë„ í¸í•´ì§„ë‹¤. (ëª¨ë¸ì„ 100ê°œ ì´ìƒ ì˜¬ë¦°ë‹¤ê³  Huggingface íŒ€ì—ì„œ ë­ë¼ê³  í•˜ì§€ ì•Šìœ¼ë‹ˆ ë§ì´ë“¤ ì“°ì…¨ìœ¼ë©´ã…ã…)","link":"/2020/05/01/transformers-porting/"}],"tags":[{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"electra","slug":"electra","link":"/tags/electra/"},{"name":"preprocess","slug":"preprocess","link":"/tags/preprocess/"},{"name":"tpu","slug":"tpu","link":"/tags/tpu/"},{"name":"pretraining","slug":"pretraining","link":"/tags/pretraining/"},{"name":"finetuning","slug":"finetuning","link":"/tags/finetuning/"},{"name":"wordpiece","slug":"wordpiece","link":"/tags/wordpiece/"},{"name":"tokenization","slug":"tokenization","link":"/tags/tokenization/"},{"name":"transformers","slug":"transformers","link":"/tags/transformers/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"}],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Wordpiece","slug":"NLP/Wordpiece","link":"/categories/NLP/Wordpiece/"},{"name":"ELECTRA","slug":"NLP/ELECTRA","link":"/categories/NLP/ELECTRA/"},{"name":"Transformers","slug":"NLP/Transformers","link":"/categories/NLP/Transformers/"},{"name":"TPU","slug":"NLP/TPU","link":"/categories/NLP/TPU/"}]}