{"pages":[{"title":"박장원 (monologg)","text":"Github | LinkedIn | Scholar 안녕하세요 NLP Engineer로 활동하고 있는 박장원입니다:) 원래 제 전공은 경영학과입니다. 우연한 계기로 자연어 처리를 접하게 되어 현재는 NLP Engineer로 살아가고 있습니다. 한국어 NLP에 더 많은 기여를 하기 위해 오늘도 달립니다!! “변화를 두려워하는 사람이 가장 불행한 사람이다.” - Mignon McLaughlin Career &amp; EducationBHSN - NLP Engineer (2022.12 ~ ) KB AI Research - NLP Engineer (2020.12 ~ 2022.10) Mathpresso - NLP Engineer (2020.06 ~ 2020.11) Samsung Research - NLP Engineer (2019.02 ~ 2020.01) Yonsei University - Business &amp; Computer Science (2011.03 ~ 2019.02) Projects KoBigBird - 한국어 Corpus로 학습한 BigBird 모델 KoELECTRA - 한국어 Corpus로 학습한 ELECTRA 모델 DistilKoBERT - SKT KoBERT에 Distillation 적용 Publications KLUE: Korean Language Understanding EvaluationS. Park*, J. Moon*, S. Kim*, W. Cho*, J. Han, J. Park, C. Song, J. Kim, Y. Song, T. Oh, J. Lee, J. Oh, S. Lyu, Y. Jeong, I. Lee, S. Seo, D. Lee, H. Kim, M. Lee, S. Jang, S. Do, S. Kim, K. Lim, J. Lee, K. Park, J. Shin, S. Kim, L. Park, A. Oh, J. Ha, and K. Cho(NeurIPS 2021 Datasets and Benchmarks Track) Domain-agnostic Question-Answering with Adversarial TrainingS. Lee*, D Kim*, and J. Park*(EMNLP-IJCNLP 2019 MRQA Workshop Shared Task) Awards 2nd Place @ OpenResource Hackathon Seoul 2019 - 2019.12Instagram Image와 Text를 이용한 Multimodal Hashtag Prediction Silver &amp; Bronze Award @ Naver NLP Challenge - 2018.12Silver award on NER Task, Bronze award on SRL Task","link":"/about/index.html"}],"posts":[{"title":"2주 간의 KoELECTRA 개발기 - 1부","text":"2주 간의 KoELECTRA 개발을 마치고, 그 과정을 글로 남기려고 한다. 이 글을 읽으신 분들은 내가 했던 삽질(?)을 최대한 덜 하길 바라는 마음이다:) 1부에는 실제 학습을 돌리기 전까지의 과정을 다룰 예정이다. Github Repo: https://github.com/monologg/KoELECTRA 문제 의식한국에 Public하게 공개되어 있는 한국어 PLM(Pretrained Language Model)에는 크게 3가지가 있다. SKT의 KoBERT TwoBlock AI의 HanBERT ETRI의 KorBERT 3가지 모두 좋은 성능을 보이지만, 3가지 모두 단점이 존재한다. 각각의 단점을 정리하면 아래와 같다. 단점 KoBERT Vocab size (8002개)가 상대적으로 작음 HanBERT Tokenizer로 인해 Ubuntu 환경에서만 사용 가능 KorBERT API 신청 등의 과정 필요 특히 3가지 모두 공통적으로 Huggingface Transformers 라이브러리에서 사용하려면 tokenization 파일을 따로 만들어야 하는 단점이 있다. KoBERT와 HanBERT의 경우 내가 직접 tokenization 파일을 만들어서 github에 배포했지만, 일부 사용자들이 불편함을 토로하기도 했다. 그래서 이번 기회에 위의 단점들을 모두 해결한 한국어 PLM을 개발하고 싶었다. Vocab size가 어느 정도 커야 함 (30000개 정도) 모든 OS에서 사용 가능 tokenization 파일을 만들 필요 없이 곧바로 사용 가능 어느 정도 성능까지 보장되어야 함 위의 4가지를 모두 만족시키기 위하여 시작한 프로젝트가 바로 KoELECTRA 이다. Tokenizer실제 현업에서도 좋은 성능을 위해 Mecab + Sentencepiece를 많이 사용하는 것으로 알고 있다. 그러나 공식 BERT, ELECTRA 등은 Wordpiece를 사용하고 있으며, transformers에서도 공식적으로 Wordpiece만 지원하고 있다. 즉, ELECTRA에서 Mecab이나 Sentencepiece를 사용하려면 추가적으로 tokenization 파일을 만들어야 하며, transformers 라이브러리의 api가 바뀌면 내가 직접 그에 맞게 tokenization 파일을 바꿔줘야 한다는 것이다. (KoBERT의 경우 실제로도 그렇게 하고 있다ㅠ) 이러한 문제들 때문에 이번 프로젝트에서는 무조건으로 Wordpiece를 사용하는 방안으로 진행하였다. Wordpiece “Wordpiece vocab을 만들 때 Huggingface의 Tokenizers 라이브러리를 쓰는 것이 가장 좋다.” import argparsefrom tokenizers import BertWordPieceTokenizerparser = argparse.ArgumentParser()parser.add_argument(\"--corpus_file\", type=str)parser.add_argument(\"--vocab_size\", type=int, default=32000)parser.add_argument(\"--limit_alphabet\", type=int, default=6000)args = parser.parse_args()tokenizer = BertWordPieceTokenizer( vocab_file=None, clean_text=True, handle_chinese_chars=True, strip_accents=False, # Must be False if cased model lowercase=False, wordpieces_prefix=\"##\")tokenizer.train( files=[args.corpus_file], limit_alphabet=args.limit_alphabet, vocab_size=args.vocab_size)tokenizer.save(\"./\", \"bert-wordpiece\") Vocab size의 경우 관례적으로 많이 쓰이는 약 3만개로 세팅하였다. 전처리 없이 원본 Corpus만 가지고 vocab 만들면 성능이 굉장히 안 좋음 character coverage를 최대한 높게 잡는 것이 좋다고 판단 (즉, corpus에서 등장했던 모든 character를 vocab에 포함시킴) 예를 들어 퀭메일이란 단어가 있다고 가정하자. 만일 퀭이 vocab에 없다면 퀭메일 전체를 [UNK]로 처리하게 된다. 만일 퀭이 vocab 안에 있으면 퀭 + ##메일로 tokenize가 될 수 있어 ##메일이란 단어의 의미를 가져갈 수 있다. (자세한 내용은 이전에 포스팅한 [나만의 BERT Wordpiece Vocab 만들기]을 참고) 전처리 (Preprocessing) “첫째도 전처리! 둘째도 전처리! 셋째도 전처리!”“PLM의 성능에 가장 큰 영향을 주는 것은 corpus quality이다!” 크롤링한 뉴스의 문장 하나를 살펴보자 [주요기사] ☞ [포토 스토리] 무허가 도시광산 을 아시나요? ☞ [따뜻한 사진 한 장] 사랑, 하나가 되어 가는 길 &lt;찰나의 기록, 순간의 진실 / KPPA 바로가기&gt; Copyrightsⓒ 한국사진기자협회(www.kppa.or.kr), powered by castnet. 무단 전재 및 재배포 금지 보내기 문제는 이런 문장이 매우 많은데다가, 이걸 Pretrain에 넣을 시 성능이 나빠질 게 뻔하다…. 이렇게 noise가 많은 Corpus로 vocab을 만들고 pretrain까지 하면 성능이 좋을 리가 없다. 아래는 내가 적용한 대표적인 전처리 기준이다. 한자, 일부 특수문자 제거 한국어 문장 분리기 (kss) 사용 뉴스 관련 문장은 제거 (무단전재, (서울=뉴스1) 등 포함되면 무조건 제외) 사실 전처리의 기준에 정답은 없다. 가장 중요한 것은 자신의 Task에 맞게 전처리 기준을 세우는 것이다. 내가 생각했던 Task 들에는 한자는 중요하지 않다고 판단해서 지운 것이지, 한자가 꼭 필요한 Task의 경우에는 지우면 안 될 것이다. TPU 사용 관련 Tip(자세한 내용은 이전에 포스팅한 [TPU를 이용하여 Electra Pretraining하기]을 참고) 1. Tensorflow Research Cloud(TFRC)를 쓰면 TPU가 무료→ 이미 공식 코드가 TPU를 완벽히 지원하기에, 직접 ELECTRA를 만들고 싶다면 GPU보다는 TPU를 쓰는 것을 강력히 권장한다. 2. VM Instance는 작은 것(n1-standard-1)을 써도 상관 없다→ ELECTRA를 GCP에서 학습하려면 TPU, Bucket, VM Instance 이렇게 3개가 필요하다. 그런데 저장소는 Bucket이, 연산은 TPU가 처리하기 때문에 VM Instance는 가벼운 것을 써도 된다.→ n1-standard-1는 시간당 약 $0.037, n1-standard-4는 시간당 약 $0.14이다. (비용이 무려 2배 차이!!) 3. TPU를 쓰는 경우 모든 input file은 Cloud storage bucket을 통해야만 한다→ 이것도 처음에 몰랐다가 고생했던 삽질 중 하나다. tf.estimator.tpu 쪽 코드를 쓰는 경우 로컬 데이터가 아닌 Bucket을 통해야만 한다. (관련 FAQ)→ 다행히도 Bucket의 비용이 비싸지가 않다 (특정 리전에 만들어 놓으면 1GB당 월 약 $0.03) 4. VM Instance와 TPU를 만들 때 ctpu up 명령어를 사용해라→ VM Instance와 TPU를 따로 따로 생성하면 처음에 정상적으로 작동하지 않는 경우가 있었다. 아래와 같이 cloud shell로 명령어를 한 번만 치면 VM과 TPU가 동시에 생성된다😃 $ ctpu up --zone=europe-west4-a --tf-version=1.15 \\ --tpu-size=v3-8 --machine-type=n1-standard-2 \\ --disk-size-gb=20 --name={$VM_NAME} Configuration의 함정앞에서도 언급했듯이 ELECTRA Pretraining은 공식 코드를 그대로 가져다 쓰는 것이 좋다. 공식 코드를 가져다쓰기 전에 논문과 코드 분석을 어느 정도 하고 진행하는 것을 권장한다. 그럼에도 좀 헷갈렸던 부분이 있어 여기서 언급하려 한다. 1. 공식 레포에서 제공하는 small 모델은 정확히는 small++ 모델이다 공식 코드에서도 알 수 있듯이 small로 공개된 모델은 정확히 말하면 small++ 모델이다. 둘의 차이점은 아래와 같다. max_seq_len generator_hidden_size small 128 0.25 small++ 512 1.0 (generator_hidden_size=1.0이란 것은 discriminator와 generator의 hidden_size가 같다는 것이다) 이러한 부분이 논문에 자세히 나와 있지 않아 처음에 small 모델을 만들 때 진짜 small로 만들었다가 다시 small++로 만드는 수고를 거쳤다…. (이 글을 읽은 분들은 이 삽질을 안 하길 빈다.) hparams.json{ \"tpu_name\": \"electra-small\", \"tpu_zone\": \"europe-west4-a\", \"num_train_steps\": 1000000, \"save_checkpoints_steps\": 50000, \"train_batch_size\": 128, \"learning_rate\": 5e-4, \"vocab_size\": 32200, \"max_seq_length\": 512, \"generator_hidden_size\": 1.0} 2. max_seq_length를 128로 줄인다면 max_position_embeddings도 128로 줄여야 한다간혹 커스터마이즈된 모델을 만들 때 max_seq_length를 줄이고자 하는 경우가 있다. 그럴 시 max_seq_length만 줄이면 해결된다고 오해할 수 있는데, max_position_embeddings도 줄여줘야 transformers 라이브러리에 맞게 변환할 때 문제가 생기지 않는다. (transformers가 max_position_embeddings으로 최대 길이를 알아내기 때문!) 더 큰 함정은 아래와 같이 model_hparam_overrides라는 attribute 안에 max_position_embeddings를 넣어줘야 하는 것이다! hparams.json{ \"max_seq_length\": 128, \"model_hparam_overrides\": { \"max_position_embeddings\": 128 }} 마치며1부에서는 실제 Pretraining을 시작하기 전의 준비 과정을 다뤘다. 2주 간의 KoELECTRA 개발기 - 2부 에서는 실제 Pretraining, Transformers 포팅, Finetuning 등을 다룰 예정이다:) Reference ELECTRA Official Code Huggingface Tokenizers Cloud TPU Documentation Related Posts TPU를 이용하여 Electra Pretraining하기 나만의 BERT Wordpiece Vocab 만들기 2주 간의 KoELECTRA 개발기 - 2부","link":"/2020/05/02/koelectra-part1/"},{"title":"TPU를 이용하여 Electra Pretraining하기","text":"최근 ELECTRA의 공식 코드가 공개되면서 한국어 Corpus에 직접 Electra를 만들게 되었다. 이번 글에서는 GCP에서 TPU를 어떻게 사용했는지 그 과정을 공유해보려 한다. Tensorflow Research Cloud 신청Tensorflow Research Cloud (TFRC)는 1달 동안 TPU를 무료로 사용할 수 있게 해주는 프로그램이다. 해당 링크로 가서 신청을 하게 되면 메일이 하나 오게 된다. 해당 메일에서 요구하는 대로 신청서를 추가로 작성한 후 제출하면 얼마 후 아래와 같이 답장이 오게 되고, 그 때부터 GCP에서 TPU를 무료로 사용할 수 있게 된다:) Bucket에 Data 업로드TPU를 쓰는 경우 모든 input file을 Cloud storage bucket을 통해야만 한다. (관련 FAQ) Bucket 생성 예제상 Bucket의 이름을 test-for-electra로 만들어 보겠다. GCP 메인 페이지 좌측의 [Storage] - [브라우저] 로 이동 버킷 만들기 클릭 사용할 TPU와 동일한 Region에 Bucket 만드는 것을 권장 File Upload 준비한 pretrain_tfrecords와 vocab.txt를 Bucket에 업로드 GCP VM &amp; TPU 생성 VM과 TPU를 각각 따로 만드는 것보다, 우측 상단의 cloud shell을 열어 아래의 명령어를 입력하는 것을 추천한다. 저장소는 Bucket이, 연산은 TPU에서 처리하기 때문에 VM Instance는 가벼운 것을 써도 상관이 없다. $ ctpu up --zone=europe-west4-a --tf-version=1.15 \\ --tpu-size=v3-8 --machine-type=n1-standard-1 \\ --disk-size-gb=20 --name={$VM_NAME} Electra 학습 진행$ git clone https://github.com/google-research/electra$ cd electra$ python3 run_pretraining.py --data-dir gs://{$BUCKET_NAME} \\ --model-name {$MODEL_NAME} \\ --hparams {$CONFIG_PATH} 학습 완료 후 Instance, Bucket 삭제$ ctpu delete --zone=europe-west4-a --name={$VM_NAME}$ gsutil rm -r gs://test-for-electra Reference ELECTRA official github A Pipeline Of Pretraining Bert On Google TPU Official TPU Documentation","link":"/2020/04/20/tpu-electra/"},{"title":"2주 간의 KoELECTRA 개발기 - 2부","text":"2주 간의 KoELECTRA 개발을 마치고, 그 과정을 글로 남기려고 한다. 이 글을 읽으신 분들은 내가 했던 삽질(?)을 최대한 덜 하길 바라는 마음이다:) 2부에는 Pretraining, Finetuning 등을 다룰 예정이다. Github Repo: https://github.com/monologg/KoELECTRA Training😀 드디어 모든 삽질들을 끝내고, Pretraining을 시작하였다 😀 Loss 약 300k step 까지의 base와 small의 loss 추이이다. 첫 100k까지는 매우 빠르게 줄어들다가, 그 이후에는 조금씩 줄어드는 모습을 보인다. Benchmark학습 중간중간 성능 체크는 nsmc 데이터셋을 가지고 간단하게 평가하였다 (사실 GPU가 1개 밖에 없어 nsmc만 테스트한 건 비밀😢) Acc(%) 25K 75K 90K 125K 150K 250K 300K 450K Base 88.10 88.48 88.67 88.92 88.97 89.51 89.65 90.16 Step이 증가할수록 accuracy가 오르는 것이 눈에 띄게 보이니 신기하긴 했다. (이것이 Pretraining의 힘인가….) Training Time데이터의 경우 14GB로 총 2.6B Token이다. BERT와 ELECTRA에서는 3.3B Token을 사용한 것에 비하면 데이터의 양이 조금 모자란 게 아쉽긴 하지만, 이것이 개인 단위에서 모을 수 있었던 최선의 데이터양이었다😵 TPU v3-8 기준으로 Base 모델은 약 7일, Small 모델은 약 3일이 소요되었다. 그래서 이 기간 동안 Finetuning 코드를 짜는 것으로 시간을 절약하였다. Finetuning 코드 제작코드의 경우는 Transformers의 Example 코드를 참고하여 제작하였다. Finetuning의 경우 총 7개의 task에 대해 진행하였다. (때마침 얼마 전 카카오브레인에서 KorNLI와 KorSTS 데이터셋을 공개해주었다👍 1년 전과 비교했을 때 벤치마크를 평가할 수 있는 한국어 데이터셋이 많아진 것은 정말 좋은 일이라 할 수 있다.) NSMC PAWS QuestionPair KorNLI KorSTS NaverNER KorQuad Task 감정분석 유사문장 유사문장 추론 유사문장 개체명인식 기계독해 Metric Acc Acc Acc Acc Spearman F1 EM/F1 기존의 연구들에서는 Bert-Multilingual-Cased를 가지고 많이 비교하였는데, 이번 연구에서는 XLM-Roberta-Base 모델로 평가를 시도하였다. 확실히 xlm-roberta가 bert보다는 성능이 좋았기에, 적어도 KoELECTRA가 xlm-roberta는 뛰어넘어야 유의미하지 않을까라고 생각해서 였다. Deview에서 발표한 Larva의 경우 Benchmark pipeline을 만들어서 ckpt가 들어올 때마다 계속 evaluation을 해주었다고 하는데, 앞에서도 말했듯이 나에게는 GPU가 1개 밖에 없고, GCP에서 GPU를 많이 빌릴 수도 없기에 가장 최근 5개의 ckpt를 가지고 평가하였고, 그 중에서 평균값이 가장 좋았던 것을 최종 모델로 선정하였다. Finetuning용 코드 및 사용법은 여기에서 직접 확인해볼 수 있다. Convert from Tensorflow to PytorchHuggingface에서 ElectraModel을 구현하면서 tensorflow를 pytorch로 변환하는 코드도 함께 만들어줬다😍 관련 내용은 [내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기]를 읽어보길 바란다. (여기서도 굉장히 삽질을 많이 해본 입장으로서 꼭 읽어보길 권한다) Result 처음에 이 프로젝트를 계획할 때 가장 걱정되었던 점이 “성능이 안 나오면 어쩌나”였다. (성능이 너무 안 좋으면 사실 2주를 제대로 날린 셈이기에….) 결과는 예상했던 것보다 훨씬 좋았다. 애초에 데이터의 양이나 Tokenizer 등을 고려했을 때 HanBERT를 완벽히 따라잡는 것은 무리라고 생각했지만, KoBERT보다 전반적으로 성능이 많이 좋을 줄은 몰랐다. HanBERT와도 실질적인 결과는 비슷하거나 오히려 더 좋은 케이스도 있어서 이 정도면 🎉대성공🎉이라고 봐도 될 꺼 같다. KoELECTRA-Small의 성능이 가장 인상적이었는데, 모델의 사이즈가 DistilKoBERT의 절반임에도 불구하고 우수한 성능을 보였다. 경량화 모델에서 충분히 사용할 만한 가치가 있을 것 같다. How to Use이 프로젝트를 처음 계획했을 때의 고려사항 중 아래 사항들이 가장 중요했었다. “모든 OS에서 사용 가능”“tokenization 파일을 만들 필요 없이 곧바로 사용 가능” 그리고 이제 transformers 라이브러리만 있으면 어떠한 환경에서도 한국어 PLM을 사용할 수 있게된 것이다🤗 Installation$ pip3 install -U transformers How to Usefrom transformers import ElectraModel, ElectraTokenizer# KoELECTRA-Basemodel = ElectraModel.from_pretrained(\"monologg/koelectra-base-discriminator\")tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")# KoELECTRA-Smallmodel = ElectraModel.from_pretrained(\"monologg/koelectra-small-discriminator\")tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-discriminator\") 맺으며 솔직히 이렇게까지 반응이 좋을 줄은 몰랐다🙄 개발자로서 이럴 때가 가장 보람있지 않은가 싶다. 처음 계획할 때부터 “이거 만들면 다른 분들에게도 큰 도움이 되겠다”라 생각했는데, 실제로도 그런 것 같아 기분이 (굉장히) 좋다 😀 “내가 불편해하는 것은 분명 다른 누군가도 불편해한다. 그런 부분을 해결해주는 것이 개발자의 역할이라고 생각한다.” Reference ELECTRA Official Code Huggingface Transformers Deview 2019 Larva Presentation KorNLI, KorSTS Related Posts 내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기 2주 간의 KoELECTRA 개발기 - 1부","link":"/2020/05/02/koelectra-part2/"},{"title":"나만의 BERT Wordpiece Vocab 만들기","text":"개인적으로 Pretrained Language Model 성능에 큰 영향을 주는 것 중 하나로 Vocab quality라고 생각한다. 이번 포스트에서는 tokenization의 방법 중 하나인 Wordpiece를 이용하여 어떻게 vocab을 만드는지 알아보려 한다:) Introduction한국어 Tokenizer의 대안으로는 크게 Sentencepiece, Mecab, Wordpiece가 있다. (여기서의 wordpiece는 Google의 BERT에서 사용된 wordpiece로 가정한다.) BERT, ELECTRA 등은 기본적으로 Wordpiece를 사용하기에 공식 코드에서 기본적으로 제공되는 Tokenizer 역시 이에 호환되게 코드가 작성되었다. 즉, Sentencepiece나 Mecab을 사용하려면 별도의 Tokenizer를 직접 만들어야 하고, 이렇게 되면 transformers 등의 라이브러리에서 모델을 곧바로 사용하는데 불편함이 생기게 된다. Original wordpiece code is NOT available! 공식 BERT에서 사용된 Wordpiece Builder는 제공되지 않고 있다. BERT 공식 Github에서 다른 대안들을 제시해줬지만, 완전히 동일한 Wordpiece Vocab이 나오지 않았다. 몇몇 오픈소스들이 Wordpiece vocab builder를 구현하였지만 input file이 매우 클 시 메모리, 속도 등의 이슈가 종종 발생한다ㅠ Huggingface Tokenizers 최종적으로, 최근 Huggingface에서 발표한 Tokenizers 라이브러리를 이용하여 Wordpiece Vocabulary를 만드는게 제일 좋았다. 해당 라이브러리를 사용하면 Corpus가 매우 커도 메모리 이슈가 발생하지 않으며, Rust로 구현이 되어있어 속도 또한 Python보다 빠르다😃 Code for building Wordpiece vocab(tokenizer v0.7.0 기준으로 작성하였다. 현재도 라이브러리가 업데이트 중이어서 api가 달라질 수도…) import argparsefrom tokenizers import BertWordPieceTokenizerparser = argparse.ArgumentParser()parser.add_argument(\"--corpus_file\", type=str)parser.add_argument(\"--vocab_size\", type=int, default=32000)parser.add_argument(\"--limit_alphabet\", type=int, default=6000)args = parser.parse_args()tokenizer = BertWordPieceTokenizer( vocab_file=None, clean_text=True, handle_chinese_chars=True, strip_accents=False, # Must be False if cased model lowercase=False, wordpieces_prefix=\"##\")tokenizer.train( files=[args.corpus_file], limit_alphabet=args.limit_alphabet, vocab_size=args.vocab_size)tokenizer.save(\"./\", \"ch-{}-wpm-{}\".format(args.limit_alphabet, args.vocab_size)) 주의해야할 점은 lowercase=False로 할 시 strip_accent=False로 해줘야 한다는 것! [UNK]의 비중을 최대한 줄이기 위해 모든 character를 커버할 수 있도록 처리하였다. (limit_alphabet) Corpus의 전처리가 완료되었다는 전제하에 sentencepiece와 비교했을 때 UNK Ratio가 훨씬 낮았다. Reference Sentencepiece vs Wordpiece Learning a new WordPiece vocabulary kwonmha’s bert-vocab-builder Huggingface Tokenizers","link":"/2020/04/27/wordpiece-vocab/"},{"title":"내가 만든 ELECTRA를 Huggingface Transformers로 Porting하기","text":"BERT, ALBERT, ELECTRA 등을 직접 Pretrain하게 되면 모델이 Tensorflow의 ckpt 형태로 저장이 된다. 이번 글에서는 tensorflow ckpt를 transformers의 pytorch ckpt로 변환하는 법을 알아보겠다🤗 Intro 이번 글에서는 ELECTRA-Small을 기준으로 실습을 해본다. (BERT 등도 방법은 크게 다르지 않다) transformers v2.8.0을 기준으로 작성하였다. 이후 버전에서 호환되지 않는 경우가 있을 수 있다. Transformers의 ELECTRA는 discriminator와 generator를 각각 따로 만들어줘야 한다! Prerequisite1. Original Tensorflow Checkpoint당연히 Tensorflow로 학습한 결과물을 가지고 있어야 한다. .├── koelectra-small-tf│ ├── checkpoint│ ├── events.out.tfevents.1586942968.koelectra-small│ ├── graph.pbtxt│ ├── ...│ ├── model.ckpt-700000.data-00000-of-00001│ ├── model.ckpt-700000.index│ └── model.ckpt-700000.meta└── ... checkpointmodel_checkpoint_path: &quot;model.ckpt-700000&quot;all_model_checkpoint_paths: &quot;model.ckpt-600000&quot;all_model_checkpoint_paths: &quot;model.ckpt-625000&quot;all_model_checkpoint_paths: &quot;model.ckpt-650000&quot;all_model_checkpoint_paths: &quot;model.ckpt-675000&quot;all_model_checkpoint_paths: &quot;model.ckpt-700000&quot; 주의할 점은 checkpoint 파일에서 model_checkpoint_path 값을 “원하는 step의 ckpt”로 바꿔줘야 한다는 것이다. 2. config.json (주의!) transformers 라이브러리가 업데이트되면서 API가 변경되는 경우가 있고, 이에 따라 config.json의 attribute가 추가/변경되는 경우가 있다. https://huggingface.co/models로 가서 대표 모델의 config.json을 보면서 직접 만들어야 한다. vocab_size 변경에만 주의하면 충분함 만일 max_seq_length를 바꿨다면 max_position_embeddings도 바꿔야 함 discriminator{ \"architectures\": [\"ElectraForPreTraining\"], \"attention_probs_dropout_prob\": 0.1, \"embedding_size\": 128, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 256, \"initializer_range\": 0.02, \"intermediate_size\": 1024, \"layer_norm_eps\": 1e-12, \"max_position_embeddings\": 512, \"model_type\": \"electra\", \"num_attention_heads\": 4, \"num_hidden_layers\": 12, \"pad_token_id\": 0, \"type_vocab_size\": 2, \"vocab_size\": 32200} generator{ \"architectures\": [\"ElectraForMaskedLM\"], \"attention_probs_dropout_prob\": 0.1, \"embedding_size\": 128, \"hidden_act\": \"gelu\", \"hidden_dropout_prob\": 0.1, \"hidden_size\": 256, \"initializer_range\": 0.02, \"intermediate_size\": 1024, \"layer_norm_eps\": 1e-12, \"max_position_embeddings\": 512, \"model_type\": \"electra\", \"num_attention_heads\": 4, \"num_hidden_layers\": 12, \"pad_token_id\": 0, \"type_vocab_size\": 2, \"vocab_size\": 32200} 3. tokenizer_config.jsoncased 모델의 경우 그냥 tokenizer를 load하면 매번 do_lower_case=False를 직접 추가해줘야 한다. from transformers import ElectraTokenizertokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-discriminator\", do_lower_case=False) tokenizer_config.json을 만들어주면 이러한 번거로움을 없앨 수 있다.(만일 max_seq_length가 128이면 model_max_length도 128로 바꿔주면 된다.) tokenizer_config.json{ \"do_lower_case\": false, \"model_max_length\": 512} 4. vocab.txttensorflow에서 학습했을 때 쓴 vocab.txt를 그대로 쓰면 된다. 5. 최종적인 디렉토리 형태.├── koelectra-small-tf│ ├── checkpoint│ ├── events.out.tfevents.1586942968.koelectra-small│ ├── graph.pbtxt│ ├── ...│ ├── model.ckpt-700000.data-00000-of-00001│ ├── model.ckpt-700000.index│ └── model.ckpt-700000.meta│├── electra-small-discriminator│ ├── config.json│ ├── tokenizer_config.json│ └── vocab.txt│├── electra-small-generator│ ├── config.json│ ├── tokenizer_config.json│ └── vocab.txt└── ... electra-small-discriminator와 electra-small-generator 폴더를 각각 만든다. config.json은 discriminator용과 generator용을 따로 만들어서 폴더 안에 넣는다. tokenizer_config.json과 vocab.txt는 discriminator와 generator 둘 다 동일한 파일을 넣으면 된다. Convertconvert.pyimport osimport argparsefrom transformers.convert_electra_original_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorchparser = argparse.ArgumentParser()parser.add_argument(\"--tf_ckpt_path\", type=str, default=\"koelectra-small-tf\")parser.add_argument(\"--pt_discriminator_path\", type=str, default=\"koelectra-small-discriminator\")parser.add_argument(\"--pt_generator_path\", type=str, default=\"koelectra-small-generator\")args = parser.parse_args()convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path, config_file=os.path.join(args.pt_discriminator_path, \"config.json\"), pytorch_dump_path=os.path.join(args.pt_discriminator_path, \"pytorch_model.bin\"), discriminator_or_generator=\"discriminator\")convert_tf_checkpoint_to_pytorch(tf_checkpoint_path=args.tf_ckpt_path, config_file=os.path.join(args.pt_generator_path, \"config.json\"), pytorch_dump_path=os.path.join(args.pt_generator_path, \"pytorch_model.bin\"), discriminator_or_generator=\"generator\") Upload your model to Huggingface s3 먼저 huggingface.co로 가서 회원가입을 해야 함 아래의 명령어로 s3에 업로드 진행 $ transformers-cli login$ transformers-cli upload koelectra-small-discriminator$ transformers-cli upload koelectra-small-generator Now Let’s Use ItHow to Usefrom transformers import ElectraModel, ElectraTokenizermodel = ElectraModel.from_pretrained(\"monologg/koelectra-small-discriminator\")tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-discriminator\") 맺으며 사실 Model Porting과 관련하여 명확한 Documentation이 없어 나도 삽질을 상당히 했던 부분이다. 이번 내용이 다른 분들에게 도움이 되었으면 한다😛 또한 Huggingface s3에 모델을 업로드하는 것은 꼭 사용해보길 권한다. 이 기능이 생긴지 얼마되지 않아서 모르시는 분들이 있는데, 업로드 용량의 제한도 없고 여러모로 라이브러리 사용도 편해진다. (모델을 100개 이상 올린다고 Huggingface 팀에서 뭐라고 하지 않으니 많이들 쓰셨으면ㅎㅎ)","link":"/2020/05/01/transformers-porting/"}],"tags":[{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"electra","slug":"electra","link":"/tags/electra/"},{"name":"preprocess","slug":"preprocess","link":"/tags/preprocess/"},{"name":"tpu","slug":"tpu","link":"/tags/tpu/"},{"name":"pretraining","slug":"pretraining","link":"/tags/pretraining/"},{"name":"finetuning","slug":"finetuning","link":"/tags/finetuning/"},{"name":"wordpiece","slug":"wordpiece","link":"/tags/wordpiece/"},{"name":"tokenization","slug":"tokenization","link":"/tags/tokenization/"},{"name":"transformers","slug":"transformers","link":"/tags/transformers/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"}],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Wordpiece","slug":"NLP/Wordpiece","link":"/categories/NLP/Wordpiece/"},{"name":"ELECTRA","slug":"NLP/ELECTRA","link":"/categories/NLP/ELECTRA/"},{"name":"Transformers","slug":"NLP/Transformers","link":"/categories/NLP/Transformers/"},{"name":"TPU","slug":"NLP/TPU","link":"/categories/NLP/TPU/"}]}